---
layout:     post
title:      "è®­ç»ƒæµç¨‹æµ‹è¯•"
subtitle:   ""
date:       2025-06-02
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - python
---

ä½ è¿™ä¸ªéœ€æ±‚å¯ä»¥å¾ˆå¥½åœ°ä½¿ç”¨ **é€‚é…å™¨æ¨¡å¼** æ¥æž„å»ºä¸€ä¸ª**çµæ´»çš„æ¨¡åž‹è®­ç»ƒæ¡†æž¶**ï¼Œç”¨äºŽé€‚é…å„ç§ä¸åŒç±»åž‹çš„æ¨¡åž‹ï¼ˆå¦‚çº¿æ€§å›žå½’ã€ç¥žç»ç½‘ç»œã€LLMã€å¤§æ•°æ®åˆ†æžç­‰ï¼‰ã€‚

ä¸‹é¢æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ª**å®Œæ•´çš„ç»“æž„åŒ–è®­ç»ƒæµç¨‹æ¡†æž¶**ï¼ŒåŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š

------

## âœ… æ€»ä½“æµç¨‹ï¼ˆæ¨¡å—åˆ’åˆ†ï¼‰

```text
User Request
   â†“
1. å‚æ•°è¾“å…¥ï¼ˆæ¨¡åž‹å‚æ•° + å› å­å‚æ•° + è®­ç»ƒæ•°æ®åˆ—è¡¨ï¼‰
   â†“
2. æ•°æ®æ¸…æ´—ï¼ˆç¼ºå¤±å€¼ã€å­—æ®µæ˜ å°„ã€ç±»åž‹è¯†åˆ«ï¼‰
   â†“
3. æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– / æ ‡å‡†åŒ–ï¼‰
   â†“
4. æ¨¡åž‹é€‚é…ï¼ˆé€‰æ‹©ä¸åŒæ¨¡åž‹ç±»ï¼‰
   â†“
5. æ¨¡åž‹è®­ç»ƒï¼ˆç»Ÿä¸€çš„ train æŽ¥å£ï¼‰
   â†“
6. æ•°æ®åŽå¤„ç†ï¼ˆå¦‚åå½’ä¸€åŒ–ï¼‰
   â†“
7. æ¨¡åž‹ä¿å­˜ï¼ˆæœ¬åœ°æˆ–ä¸Šä¼  S3ï¼‰
```

------

## âœ… é¡¹ç›®ç»“æž„å»ºè®®

```
ml_pipeline/
â”œâ”€â”€ adapters/                # æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ base.py              # BaseModelAdapter
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ dnn.py
â”‚   â””â”€â”€ llm.py
â”œâ”€â”€ data_cleaning/
â”‚   â”œâ”€â”€ cleaner.py           # æ•°æ®æ¸…æ´—å·¥å…·
â”‚   â””â”€â”€ field_mapper.py      # å­—æ®µæ˜ å°„/ç±»åž‹è¯†åˆ«
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ preprocessor.py      # å½’ä¸€åŒ–ç­‰é¢„å¤„ç†é€»è¾‘
â”‚   â””â”€â”€ postprocessor.py     # åå½’ä¸€åŒ–
â”œâ”€â”€ trainer.py               # è®­ç»ƒæµç¨‹ä¸»å…¥å£
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ io.py                # æ•°æ®æ–‡ä»¶è¯»å–ï¼ˆopenpyxl + csvï¼‰
â”‚   â””â”€â”€ s3_uploader.py       # æ¨¡åž‹æ–‡ä»¶ä¸Šä¼ 
â””â”€â”€ config/
    â””â”€â”€ model_registry.py    # æ³¨å†Œæ”¯æŒçš„æ¨¡åž‹ç±»åž‹
```

------

## âœ… æ ¸å¿ƒç±»ä¸Žæµç¨‹å®žçŽ°ï¼ˆå…³é”®ä»£ç ï¼‰

### 1. `BaseModelAdapter`ï¼ˆæ‰€æœ‰æ¨¡åž‹çš„ç»Ÿä¸€æŽ¥å£ï¼‰

```python
# adapters/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict):
        self.model_params = model_params

    @abstractmethod
    def preprocess(self, df):
        pass

    @abstractmethod
    def train(self, train_data, test_data):
        pass

    @abstractmethod
    def postprocess(self, results):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass
```

------

### 2. `LinearRegressionAdapter` ç¤ºä¾‹

```python
# adapters/linear_regression.py
from sklearn.linear_model import LinearRegression
from .base import BaseModelAdapter

class LinearRegressionAdapter(BaseModelAdapter):
    def __init__(self, model_params):
        super().__init__(model_params)
        self.model = LinearRegression(**model_params)

    def preprocess(self, df):
        return df  # çº¿æ€§æ¨¡åž‹é€šå¸¸ä¸è¦æ±‚é¢å¤–å¤„ç†

    def train(self, train_data, test_data):
        X_train, y_train = train_data
        self.model.fit(X_train, y_train)
        return {"score": self.model.score(*test_data)}

    def postprocess(self, results):
        return results

    def save_model(self, path):
        import joblib
        joblib.dump(self.model, path)
```

------

### 3. è®­ç»ƒæµç¨‹ä¸»å…¥å£

```python
# trainer.py
from adapters import model_registry
from data_cleaning.cleaner import clean_dataset
from preprocessing.preprocessor import preprocess_dataset
from preprocessing.postprocessor import postprocess_result
from utils.io import load_datasets
from utils.s3_uploader import upload_model_file

def train_pipeline(factor_params, model_params, model_type, dataset_files, publish=False):
    # 1. åŠ è½½æ•°æ®é›†
    df = load_datasets(dataset_files)

    # 2. æ•°æ®æ¸…æ´—
    df_clean = clean_dataset(df, factor_params)

    # 3. æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰
    df_preprocessed, scaler = preprocess_dataset(df_clean, factor_params)

    # 4. åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    X = df_preprocessed.drop(columns=["target"])
    y = df_preprocessed["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # 5. èŽ·å–æ¨¡åž‹é€‚é…å™¨
    adapter_class = model_registry.get(model_type)
    if not adapter_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")
    model = adapter_class(model_params)

    # 6. è®­ç»ƒ
    train_result = model.train((X_train, y_train), (X_test, y_test))

    # 7. åŽå¤„ç†
    final_result = model.postprocess(train_result)

    # 8. æ¨¡åž‹ä¿å­˜
    save_path = f"./output_models/{model_type}_{time.time()}.pkl"
    model.save_model(save_path)

    if publish:
        upload_model_file(save_path)

    return final_result
```

------

### 4. æ³¨å†Œæ¨¡åž‹é€‚é…å™¨

```python
# config/model_registry.py
from adapters.linear_regression import LinearRegressionAdapter
from adapters.dnn import DNNAdapter
from adapters.llm import LLMAdapter

model_registry = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
    "llm": LLMAdapter,
}
```

------

### 5. æ¸…æ´— + é¢„å¤„ç†ï¼ˆç¤ºä¾‹ï¼‰

```python
# data_cleaning/cleaner.py
def clean_dataset(df, factor_params):
    # åˆ é™¤å› å­ä¸ºç©ºçš„è¡Œ
    for factor in factor_params:
        df = df[df[factor["name"]].notnull()]
    return df

# preprocessing/preprocessor.py
from sklearn.preprocessing import MinMaxScaler

def preprocess_dataset(df, factor_params):
    numeric_cols = [f["name"] for f in factor_params if f.get("normalize") is True]
    scaler = MinMaxScaler()
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    return df, scaler
```

------

## âœ… ç¤ºä¾‹è°ƒç”¨

```python
result = train_pipeline(
    factor_params=[{"name": "feature1", "normalize": True}, {"name": "feature2", "normalize": False}],
    model_params={"fit_intercept": True},
    model_type="linear_regression",
    dataset_files=["/path/data1.xlsx", "/path/data2.csv"],
    publish=True
)
print(result)
```

------

## âœ… æ€»ç»“

- âœ” ä½¿ç”¨é€‚é…å™¨æ¨¡å¼å°è£…ä¸åŒæ¨¡åž‹è®­ç»ƒé€»è¾‘ï¼Œä¾¿äºŽæ‰©å±•æ–°æ¨¡åž‹ã€‚
- âœ” æ•°æ®æ¸…æ´—ä¸Žé¢„å¤„ç†æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºŽå¤ç”¨å’Œæµ‹è¯•ã€‚
- âœ” æ”¯æŒæ¨¡åž‹è®­ç»ƒåŽå‘å¸ƒï¼ˆå¦‚ä¸Šä¼ åˆ° S3ï¼‰ã€‚
- âœ” é«˜åº¦è§£è€¦ã€ç»“æž„æ¸…æ™°ï¼Œæ–¹ä¾¿å¤šäººåä½œæˆ–æœåŠ¡åŒ–éƒ¨ç½²ã€‚

å¦‚æžœä½ å¸Œæœ›æˆ‘ç»§ç»­è¡¥å…¨æŸä¸ªæ¨¡å—ï¼ˆå¦‚ DNNAdapterã€åå½’ä¸€åŒ–ã€ä»»åŠ¡é˜Ÿåˆ—è°ƒç”¨è®­ç»ƒï¼‰ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šç»§ç»­ç»†åŒ–ã€‚

å¾ˆå¥½ï¼Œè¿™è¯´æ˜Žä½ éœ€è¦æž„å»ºä¸€ä¸ª**çµæ´»å¯é…ç½®çš„æ¨¡åž‹è®­ç»ƒæµç¨‹**ï¼Œå…¶ä¸­æŸäº›æ­¥éª¤ï¼ˆå¦‚æ•°æ®é¢„å¤„ç†ã€å½’ä¸€åŒ–ã€åŽå¤„ç†ã€å‘å¸ƒï¼‰**å¯é€‰æ‰§è¡Œï¼Œå–å†³äºŽæ¨¡åž‹ç±»åž‹æˆ–æ¨¡åž‹é€‚é…å™¨çš„éœ€è¦**ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¢žå¼ºé€‚é…å™¨ç»“æž„ï¼Œä½¿æ¯ä¸ªæ¨¡åž‹**å£°æ˜Žéœ€è¦å“ªäº›æµç¨‹æ­¥éª¤**ï¼Œå¹¶åŠ¨æ€æŽ§åˆ¶è®­ç»ƒæµç¨‹ä¸­å„ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œã€‚

------

## âœ… æ•´ä½“è®¾è®¡ç›®æ ‡

æˆ‘ä»¬å¸Œæœ›å®žçŽ°å¦‚ä¸‹å½¢å¼çš„è®­ç»ƒæŽ§åˆ¶æµç¨‹ï¼š

```text
1. æ¨¡åž‹é€‚é…å™¨å£°æ˜Žï¼šæˆ‘æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼Ÿæ˜¯å¦éœ€è¦åŽå¤„ç†ï¼Ÿ
2. è®­ç»ƒä¸»æµç¨‹è¯»å–å£°æ˜Žå†…å®¹ï¼Œä»…æ‰§è¡Œéœ€è¦çš„æ­¥éª¤
```

------

## âœ… ä¿®æ”¹ç‚¹ä¸€ï¼šæ¨¡åž‹é€‚é…å™¨æŽ¥å£æ‰©å±•

æˆ‘ä»¬ä¸º `BaseModelAdapter` å¢žåŠ ç‰¹æ€§å£°æ˜Žï¼š

```python
# adapters/base.py
class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict):
        self.model_params = model_params

    # ========= æ–°å¢žï¼šæ¨¡åž‹æ‰€éœ€è®­ç»ƒæµç¨‹ç‰¹æ€§å£°æ˜Ž =========
    def need_preprocess(self) -> bool:
        return False

    def need_postprocess(self) -> bool:
        return False

    def need_normalization(self) -> bool:
        return False

    # ========= åŽŸå§‹æŽ¥å£ =========
    @abstractmethod
    def preprocess(self, df):
        pass

    @abstractmethod
    def train(self, train_data, test_data):
        pass

    @abstractmethod
    def postprocess(self, results):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass
```

------

## âœ… ä¿®æ”¹ç‚¹äºŒï¼šé€‚é…å™¨æŒ‰éœ€å®žçŽ°å£°æ˜Žæ–¹æ³•

ä¾‹å¦‚ï¼šçº¿æ€§å›žå½’éœ€è¦å½’ä¸€åŒ–ï¼Œä¸éœ€è¦åŽå¤„ç†

```python
# adapters/linear_regression.py
class LinearRegressionAdapter(BaseModelAdapter):
    def need_preprocess(self):
        return True

    def need_normalization(self):
        return True

    def need_postprocess(self):
        return False

    # ... å…¶ä»–æŽ¥å£åŒå‰ ...
```

------

## âœ… ä¿®æ”¹ç‚¹ä¸‰ï¼šè®­ç»ƒä¸»æµç¨‹æ”¹é€ ï¼ˆæŒ‰éœ€æ‰§è¡Œï¼‰

```python
# trainer.py
def train_pipeline(factor_params, model_params, model_type, dataset_files, publish=False):
    df = load_datasets(dataset_files)
    df_clean = clean_dataset(df, factor_params)

    adapter_class = model_registry.get(model_type)
    if not adapter_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")
    model = adapter_class(model_params)

    # ========= æ˜¯å¦éœ€è¦é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰ =========
    if model.need_preprocess():
        if model.need_normalization():
            df_clean, scaler = preprocess_dataset(df_clean, factor_params)
        else:
            df_clean, scaler = df_clean, None
    else:
        scaler = None

    # ========= åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† =========
    from sklearn.model_selection import train_test_split
    X = df_clean.drop(columns=["target"])
    y = df_clean["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # ========= æ¨¡åž‹è®­ç»ƒ =========
    train_result = model.train((X_train, y_train), (X_test, y_test))

    # ========= å¯é€‰åŽå¤„ç† =========
    if model.need_postprocess():
        final_result = model.postprocess(train_result)
    else:
        final_result = train_result

    # ========= æ¨¡åž‹ä¿å­˜ & å¯é€‰å‘å¸ƒ =========
    save_path = f"./output_models/{model_type}_{int(time.time())}.pkl"
    model.save_model(save_path)

    if publish:
        upload_model_file(save_path)

    return final_result
```

------

## âœ… æ€»ç»“ï¼šä½ èŽ·å¾—äº†ä»€ä¹ˆ

| åŠŸèƒ½               | å®žçŽ°æ–¹å¼                                       |
| ------------------ | ---------------------------------------------- |
| æ¯æ­¥æ˜¯å¦æ‰§è¡Œ       | æ¨¡åž‹é€‚é…å™¨æä¾› `.need_xxx()` æŽ¥å£è¿›è¡ŒæŽ§åˆ¶      |
| æ‰©å±•æ”¯æŒçš„æ¨¡åž‹ç±»åž‹ | åªéœ€å®žçŽ°æ–°æ¨¡åž‹é€‚é…å™¨ç±»ï¼Œå£°æ˜Žéœ€æ±‚å¹¶å®žçŽ°æŽ¥å£     |
| çµæ´»æ€§             | æµç¨‹æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥æŒ‰æ¨¡åž‹è¦æ±‚å†³å®šæ˜¯å¦æ‰§è¡Œ       |
| è§£è€¦æ€§             | æ¯ä¸ªæ¨¡å—ï¼ˆæ¸…æ´—ã€é¢„å¤„ç†ã€è®­ç»ƒã€åŽå¤„ç†ï¼‰éƒ½å¯å¤ç”¨ |

------

å¦‚æžœä½ å¸Œæœ›ä¸‹ä¸€æ­¥æˆ‘å¸®ä½ ï¼š

- æ·»åŠ æŸä¸ªå…·ä½“æ¨¡åž‹é€‚é…å™¨ï¼ˆå¦‚ DNNã€LLMï¼‰
- æ”¯æŒæŸäº›æ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æ—¥å¿—ï¼ˆç”¨äºŽå‰ç«¯æ˜¾ç¤ºï¼‰
- é€šè¿‡é˜Ÿåˆ—æˆ–ä»»åŠ¡ç®¡ç†å™¨ï¼ˆå¦‚ Celeryï¼‰æ‰§è¡Œè®­ç»ƒä»»åŠ¡

æ¬¢è¿Žç»§ç»­å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šç›´æŽ¥ç»™å‡ºå¯¹åº”ä»£ç ã€‚

æ˜Žç™½äº†ï¼Œä½ çš„éœ€æ±‚æ˜¯ï¼š

> åœ¨æ¨¡åž‹è®­ç»ƒæµç¨‹ä¸­ï¼Œæ¯ä¸ªæ­¥éª¤ï¼ˆå¦‚å½’ä¸€åŒ–ã€é¢„å¤„ç†ã€åŽå¤„ç†ç­‰ï¼‰æ˜¯å¦æ‰§è¡Œç”±**ä¸¤æ–¹å†³å®š**ï¼š

1. **æ¨¡åž‹åˆ¶ä½œè€…ï¼ˆé€‚é…å™¨å†…éƒ¨å®šä¹‰ï¼‰**ï¼šè®¾ç½®é»˜è®¤æµç¨‹è¡Œä¸ºï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ã€‚
2. **å‰ç«¯ç”¨æˆ·é…ç½®**ï¼šç”¨æˆ·å¯ä»¥è¦†ç›–éƒ¨åˆ†è¡Œä¸ºï¼Œå¦‚æžœé€‚é…å™¨å…è®¸ã€‚

å¹¶ä¸”ï¼š

- **æ¨¡åž‹é€‚é…å™¨ä¼˜å…ˆçº§é«˜**ï¼šå¦‚æžœé€‚é…å™¨**å¼ºåˆ¶æŸæ­¥éª¤æ‰§è¡Œ/è·³è¿‡**ï¼Œåˆ™ç”¨æˆ·æ— æ³•æŽ§åˆ¶ã€‚
- **ç”¨æˆ·æŽ§åˆ¶ä¼˜å…ˆä½¿ç”¨**ï¼šå¦‚æžœé€‚é…å™¨æ²¡æœ‰æ˜Žç¡®å¼ºåˆ¶è¦æ±‚æ‰§è¡Œæˆ–è·³è¿‡ï¼Œåˆ™ç”¨æˆ·æŽ§åˆ¶ç”Ÿæ•ˆã€‚

------

## âœ… æ•´ä½“æŽ§åˆ¶é€»è¾‘ï¼ˆæµç¨‹èŠ‚ç‚¹æ‰§è¡Œåˆ¤æ–­ï¼‰

æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªé€šç”¨åˆ¤æ–­å‡½æ•°ï¼š

```python
def should_execute_step(
    step_name: str,
    user_control: dict,
    adapter_control: dict
) -> bool:
    """
    å†³å®šæ˜¯å¦æ‰§è¡ŒæŸä¸€æµç¨‹èŠ‚ç‚¹ï¼š
    - adapter_control ä¼˜å…ˆçº§æ›´é«˜ï¼ˆå¿…é¡»æ‰§è¡Œæˆ–å¿…é¡»è·³è¿‡ï¼‰
    - å¦åˆ™çœ‹ç”¨æˆ·æ˜¯å¦ä¼ å…¥æŽ§åˆ¶å€¼
    - å¦åˆ™é»˜è®¤æ‰§è¡Œä¸º False
    """
    if step_name in adapter_control:
        return adapter_control[step_name]
    return user_control.get(step_name, False)
```

------

## âœ… æ¨¡åž‹é€‚é…å™¨å®šä¹‰æŽ¥å£ï¼ˆå¸¦é»˜è®¤è¡Œä¸ºï¼‰

æ¯ä¸ªæ¨¡åž‹å¯ä»¥é€šè¿‡é€‚é…å™¨å®šä¹‰æµç¨‹è¡Œä¸ºï¼Œä¾‹å¦‚ï¼š

```python
class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict):
        self.model_params = model_params

    @property
    def pipeline_control(self) -> dict:
        """
        å®šä¹‰æ¨¡åž‹é»˜è®¤è¡Œä¸ºï¼š
        {
            "preprocess": True,   # è¡¨ç¤ºæ¨¡åž‹éœ€è¦é¢„å¤„ç†
            "normalize": False,   # è¡¨ç¤ºæ¨¡åž‹ä¸éœ€è¦å½’ä¸€åŒ–
            "postprocess": None,  # è¡¨ç¤ºæ˜¯å¦éœ€è¦åŽå¤„ç†ç”±ç”¨æˆ·å†³å®š
        }
        None è¡¨ç¤ºæœªæŒ‡å®šï¼Œå…è®¸ç”¨æˆ·å†³å®šã€‚
        """
        return {}

    @abstractmethod
    def preprocess(self, df):
        pass

    @abstractmethod
    def train(self, train_data, test_data):
        pass

    @abstractmethod
    def postprocess(self, results):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass
```

ç¤ºä¾‹ï¼šçº¿æ€§å›žå½’é€‚é…å™¨å¼ºåˆ¶éœ€è¦å½’ä¸€åŒ–ã€ä¸åšåŽå¤„ç†ï¼š

```python
class LinearRegressionAdapter(BaseModelAdapter):
    @property
    def pipeline_control(self):
        return {
            "preprocess": True,
            "normalize": True,
            "postprocess": False
        }

    # å®žçŽ°ç›¸å…³æŽ¥å£...
```

------

## âœ… ä¸»è®­ç»ƒæµç¨‹è°ƒæ•´ï¼ˆå¼•å…¥åŒé‡æŽ§åˆ¶ï¼‰

```python
def train_pipeline(
    factor_params: dict,
    model_params: dict,
    model_type: str,
    dataset_files: List[Union[str, IO]],
    user_pipeline_control: dict,
    publish: bool = False
):
    df = load_datasets(dataset_files)
    df_clean = clean_dataset(df, factor_params)

    adapter_class = model_registry.get(model_type)
    if not adapter_class:
        raise ValueError(f"æ¨¡åž‹ç±»åž‹ä¸æ”¯æŒ: {model_type}")

    model = adapter_class(model_params)
    adapter_control = model.pipeline_control

    # åˆ¤æ–­æ˜¯å¦æ‰§è¡Œå„ä¸ªæ­¥éª¤
    do_preprocess = should_execute_step("preprocess", user_pipeline_control, adapter_control)
    do_normalize = should_execute_step("normalize", user_pipeline_control, adapter_control)
    do_postprocess = should_execute_step("postprocess", user_pipeline_control, adapter_control)

    # ========= é¢„å¤„ç† =========
    scaler = None
    if do_preprocess:
        if do_normalize:
            df_clean, scaler = preprocess_dataset(df_clean, factor_params)
        else:
            df_clean, scaler = df_clean, None

    # ========= åˆ’åˆ†æ•°æ®é›† =========
    from sklearn.model_selection import train_test_split
    X = df_clean.drop(columns=["target"])
    y = df_clean["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # ========= è®­ç»ƒ =========
    train_result = model.train((X_train, y_train), (X_test, y_test))

    # ========= åŽå¤„ç† =========
    if do_postprocess:
        final_result = model.postprocess(train_result)
    else:
        final_result = train_result

    # ========= ä¿å­˜ & å‘å¸ƒ =========
    save_path = f"./output_models/{model_type}_{int(time.time())}.pkl"
    model.save_model(save_path)

    if publish:
        upload_model_file(save_path)

    return final_result
```

------

## âœ… ä½¿ç”¨ç¤ºä¾‹

```python
# ç”¨æˆ·ä¼ å…¥çš„æŽ§åˆ¶å‚æ•°
user_pipeline_control = {
    "preprocess": True,
    "normalize": False,
    "postprocess": True
}

train_pipeline(
    factor_params={...},
    model_params={...},
    model_type="linear_regression",
    dataset_files=["/path/to/data1.csv", "/path/to/data2.csv"],
    user_pipeline_control=user_pipeline_control,
    publish=True
)
```

å³ä½¿ç”¨æˆ·è®¾ç½® `normalize=False`ï¼Œä½†å¦‚æžœ `adapter.pipeline_control["normalize"] = True`ï¼Œä»ä¼šæ‰§è¡Œå½’ä¸€åŒ–ã€‚

------

## âœ… æ€»ç»“

| åŠŸèƒ½                 | å®žçŽ°æ–¹å¼                                         |
| -------------------- | ------------------------------------------------ |
| æ¯ä¸ªèŠ‚ç‚¹æŽ§åˆ¶æ˜¯å¦æ‰§è¡Œ | é€‚é…å™¨ `pipeline_control` å’Œç”¨æˆ·ä¼ å‚å…±åŒå†³å®š     |
| ä¼˜å…ˆçº§æŽ§åˆ¶           | é€‚é…å™¨ä¼˜å…ˆï¼›ç”¨æˆ·ä¼ å‚ä¸ºæ¬¡é€‰ï¼›å¦åˆ™é»˜è®¤ False       |
| æ–°å¢žæ¨¡åž‹é€‚é…ç®€å•     | åªéœ€å£°æ˜Žé»˜è®¤æµç¨‹è¡Œä¸º + å®žçŽ°è®­ç»ƒæŽ¥å£å³å¯          |
| å¯ç»´æŠ¤æ€§é«˜           | æ¯ä¸ªæ¨¡å—èŒè´£æ¸…æ™°ï¼ˆæ•°æ®å¤„ç†ã€è®­ç»ƒé€»è¾‘ã€ä¿å­˜é€»è¾‘ï¼‰ |

------

å¦‚æžœä½ éœ€è¦æˆ‘ç»§ç»­æä¾›ï¼š

- æµç¨‹ç»“æž„å°è£…ä¸ºæ¨¡å—åŒ…
- ä»»åŠ¡æŒä¹…åŒ–ä¸Ž Celery é˜Ÿåˆ—é›†æˆ
- å°†è®­ç»ƒæµç¨‹æ—¥å¿—å›žä¼ å‰ç«¯

æ¬¢è¿Žç»§ç»­å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ç»§ç»­è¡¥å……å®Œæ•´ç¤ºä¾‹ã€‚

å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å°†â€œæ¨¡åž‹è®­ç»ƒæµç¨‹â€å°è£…ä¸ºæ¨¡å—åŒ…ï¼Œå¹¶æ”¯æŒå°†è®­ç»ƒæ—¥å¿—å®žæ—¶ï¼ˆæµå¼ï¼‰å›žä¼ å‰ç«¯çš„å®Œæ•´è®¾è®¡æ€è·¯ä¸Žä»£ç å®žçŽ°æ–¹æ¡ˆï¼š

------

## âœ… 1. æ¨¡å—ç»“æž„è®¾è®¡

æˆ‘ä»¬å°†æ•´ä¸ªè®­ç»ƒæµç¨‹å°è£…ä¸ºä¸€ä¸ªæ¨¡å—åŒ…ï¼Œä¾‹å¦‚ `ai_pipeline`ï¼š

```
ai_pipeline/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py                  # é€‚é…å™¨åŸºç±»ä¸Žé€šç”¨æŽ§åˆ¶é€»è¾‘
â”œâ”€â”€ adapters/                # å„ç§æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data_handler.py          # æ•°æ®åŠ è½½ã€æ¸…æ´—ã€é¢„å¤„ç†ç­‰
â”œâ”€â”€ pipeline.py              # ä¸»æµç¨‹æŽ§åˆ¶
â”œâ”€â”€ logger.py                # æ—¥å¿—å·¥å…·ï¼ˆæµå¼è¾“å‡ºï¼‰
â””â”€â”€ registry.py              # æ¨¡åž‹æ³¨å†Œå™¨
```

------

## âœ… 2. æµå¼æ—¥å¿— logger å®žçŽ°ï¼ˆ`logger.py`ï¼‰

```python
import sys
import time
from typing import Callable


class StreamLogger:
    def __init__(self, log_func: Callable[[str], None] = print):
        self.log_func = log_func

    def log(self, msg: str):
        timestamp = time.strftime("[%Y-%m-%d %H:%M:%S]")
        self.log_func(f"{timestamp} {msg}")

# ç”¨äºŽé˜Ÿåˆ—æˆ– WebSocket åœºæ™¯çš„å›žè°ƒæ³¨å…¥
logger = StreamLogger()
```

------

## âœ… 3. æ•°æ®å¤„ç†é€»è¾‘ï¼ˆ`data_handler.py`ï¼‰

ç•¥ï¼ˆä½ å·²ç»æœ‰äº†è¯»å–å¤šä¸ªæ–‡ä»¶ã€å­—æ®µåŒ¹é…ã€å½’ä¸€åŒ–ã€åå½’ä¸€åŒ–ç­‰é€»è¾‘ï¼‰ï¼Œåªè¦å°è£…æˆå‡½æ•°å¹¶æŽ¥å— logger å®žä¾‹å³å¯ã€‚

ä¾‹å¦‚ï¼š

```python
def clean_and_preprocess(datasets, factor_config, normalize=False, logger=None):
    logger = logger or print
    logger("å¼€å§‹åŠ è½½ä¸Žåˆå¹¶æ•°æ®é›†")
    # åˆå¹¶ã€æ¸…æ´—ã€å½’ä¸€åŒ–ç­‰æ“ä½œ...
```

------

## âœ… 4. é€‚é…å™¨å®šä¹‰ï¼ˆ`base.py`ï¼‰

```python
from abc import ABC, abstractmethod


class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict, logger=None):
        self.model_params = model_params
        self.logger = logger or print

    @property
    def pipeline_control(self) -> dict:
        return {}

    @abstractmethod
    def train(self, X_train, y_train, X_test, y_test):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass

    def postprocess(self, results):
        return results
```

------

## âœ… 5. è®­ç»ƒä¸»æµç¨‹ï¼ˆ`pipeline.py`ï¼‰

```python
from .registry import model_registry
from .data_handler import clean_and_preprocess
from .logger import logger
from .base import BaseModelAdapter
from .utils import should_execute_step
from sklearn.model_selection import train_test_split


def train_pipeline(
    factor_params: dict,
    model_params: dict,
    model_type: str,
    dataset_files: list,
    user_pipeline_control: dict,
    logger_instance=None
):
    log = logger_instance or logger.log
    log(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: æ¨¡åž‹ç±»åž‹={model_type}")

    # 1. åŠ è½½æ¨¡åž‹é€‚é…å™¨
    model_class = model_registry.get(model_type)
    if not model_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")

    model = model_class(model_params, logger=log)
    adapter_control = model.pipeline_control

    # 2. æ•°æ®å¤„ç†
    normalize = should_execute_step("normalize", user_pipeline_control, adapter_control)
    df, scaler = clean_and_preprocess(dataset_files, factor_params, normalize, logger=log)

    X = df.drop(columns=["target"])
    y = df["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # 3. è®­ç»ƒ
    log("å¼€å§‹æ¨¡åž‹è®­ç»ƒ...")
    result = model.train(X_train, y_train, X_test, y_test)

    # 4. åŽå¤„ç†ï¼ˆå¯é€‰ï¼‰
    if should_execute_step("postprocess", user_pipeline_control, adapter_control):
        result = model.postprocess(result)

    # 5. ä¿å­˜æ¨¡åž‹
    model_path = f"./outputs/model_{model_type}_{int(time.time())}.pkl"
    model.save_model(model_path)
    log(f"æ¨¡åž‹ä¿å­˜åˆ°: {model_path}")

    return result
```

------

## âœ… 6. WebSocket å›žä¼ ï¼ˆé€‚ç”¨äºŽ Web å‰ç«¯ï¼‰

### Django + Channels ç¤ºä¾‹ï¼ˆconsumerï¼‰

```python
# consumers.py
from channels.generic.websocket import WebsocketConsumer
import threading
from ai_pipeline import train_pipeline
from ai_pipeline.logger import StreamLogger

class TrainingConsumer(WebsocketConsumer):
    def connect(self):
        self.accept()

    def disconnect(self, code):
        pass

    def receive(self, text_data=None, bytes_data=None):
        import json
        payload = json.loads(text_data)
        
        def send_log(msg):
            self.send(json.dumps({"type": "log", "message": msg}))

        logger = StreamLogger(log_func=send_log)

        # åŽå°çº¿ç¨‹è·‘ä»»åŠ¡
        threading.Thread(target=train_pipeline, kwargs={
            "factor_params": payload["factorParams"],
            "model_params": payload["modelParams"],
            "model_type": payload["modelType"],
            "dataset_files": payload["datasetFiles"],
            "user_pipeline_control": payload.get("control", {}),
            "logger_instance": logger.log
        }).start()
```

------

## âœ… æ€»ç»“ï¼šå…³é”®ç‰¹æ€§ä¸€è§ˆ

| åŠŸèƒ½ç‚¹                         | å®žçŽ°æ–¹å¼                         |
| ------------------------------ | -------------------------------- |
| è®­ç»ƒæ¨¡å—è§£è€¦å°è£…               | `ai_pipeline` åŒ…                 |
| æ”¯æŒå¤šç§æ¨¡åž‹é€‚é…               | é€‚é…å™¨æ¨¡å¼ + æ³¨å†Œå™¨              |
| æŽ§åˆ¶æµç¨‹èŠ‚ç‚¹æ‰§è¡Œ               | `should_execute_step()` æ–¹æ³•     |
| åŒé‡æŽ§åˆ¶ï¼ˆæ¨¡åž‹ä¼˜å…ˆï¼Œç”¨æˆ·å¯æŽ§ï¼‰ | `pipeline_control + user config` |
| æ—¥å¿—å¯å›žä¼                      | è‡ªå®šä¹‰ logger + WebSocket        |
| å¯é›†æˆ Celery                  | logger ä¸Žä»»åŠ¡çº¿ç¨‹è§£è€¦            |

------

å¦‚éœ€æˆ‘ï¼š

- å¢žåŠ è®­ç»ƒç»“æžœæŒ‡æ ‡æ—¥å¿—ï¼ˆRÂ²ã€MAEç­‰ï¼‰
- å°è£…æˆ pip install åŒ…ç»“æž„
- æä¾›å‰ç«¯æ¶ˆæ¯å¤„ç†ç¤ºä¾‹ï¼ˆå¦‚ Vue + Element Plusï¼‰

éƒ½å¯ä»¥ç»§ç»­æ‰©å±•ã€‚æ˜¯å¦éœ€è¦æˆ‘æ‰“åŒ…æˆä¸€ä¸ªå¯ç›´æŽ¥è¿è¡Œçš„é¡¹ç›®ç»“æž„ç¤ºä¾‹ï¼Ÿ

```python
# ai_pipeline/__init__.py
# ç©ºæ–‡ä»¶ï¼Œä½¿ ai_pipeline æˆä¸ºæ¨¡å—åŒ…

# ai_pipeline/logger.py
import time
from typing import Callable

class StreamLogger:
    def __init__(self, log_func: Callable[[str], None] = print):
        self.log_func = log_func

    def log(self, msg: str):
        timestamp = time.strftime("[%Y-%m-%d %H:%M:%S]")
        self.log_func(f"{timestamp} {msg}")

logger = StreamLogger()

# ai_pipeline/utils.py
def should_execute_step(step: str, user_control: dict, adapter_control: dict):
    if step in adapter_control:
        return adapter_control[step]
    return user_control.get(step, False)

# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict, logger=None):
        self.model_params = model_params
        self.logger = logger or print

    @property
    def pipeline_control(self) -> dict:
        return {}

    @abstractmethod
    def train(self, X_train, y_train, X_test, y_test):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass

    def postprocess(self, results):
        return results

# ai_pipeline/adapters/linear_regression.py
import joblib
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from ai_pipeline.base import BaseModelAdapter

class LinearRegressionAdapter(BaseModelAdapter):
    def __init__(self, model_params: dict, logger=None):
        super().__init__(model_params, logger)
        self.model = LinearRegression(**model_params)

    @property
    def pipeline_control(self):
        return {"normalize": True, "postprocess": True}

    def train(self, X_train, y_train, X_test, y_test):
        self.logger("è®­ç»ƒçº¿æ€§å›žå½’æ¨¡åž‹...")
        self.model.fit(X_train, y_train)
        y_pred = self.model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        self.logger(f"æ¨¡åž‹è¯„ä¼° - MSE: {mse:.4f}, R2: {r2:.4f}")
        return {"mse": mse, "r2": r2}

    def save_model(self, path: str):
        joblib.dump(self.model, path)
        self.logger(f"æ¨¡åž‹ä¿å­˜åˆ° {path}")

# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter

model_registry = {
    "linear_regression": LinearRegressionAdapter
}

# ai_pipeline/data_handler.py
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ç¤ºä¾‹å› å­é…ç½®ï¼š{"age": {"type": "numeric", "normalize": True}}
def clean_and_preprocess(dataset_files, factor_config, normalize=False, logger=None):
    logger = logger or print
    logger("åŠ è½½å¹¶åˆå¹¶æ•°æ®é›†...")
    
    df_list = []
    for path in dataset_files:
        df = pd.read_csv(path)
        df_list.append(df)

    # åˆå¹¶å¹¶å¯¹é½åˆ—åï¼ˆé¡ºåºä¸ä¸€è‡´æ—¶ï¼‰
    base_cols = df_list[0].columns
    df_list = [df[base_cols] for df in df_list]
    df = pd.concat(df_list, ignore_index=True).drop_duplicates()

    logger("æ ¹æ®å› å­é…ç½®è¿›è¡Œæ•°æ®æ¸…æ´—...")
    for col, config in factor_config.items():
        if config.get("type") == "numeric":
            df = df[df[col].notna()]
            if "min" in config:
                df = df[df[col] >= config["min"]]
            if "max" in config:
                df = df[df[col] <= config["max"]]
        elif config.get("type") == "category":
            allowed = config.get("values")
            if allowed:
                df = df[df[col].isin(allowed)]

    scaler = None
    if normalize:
        numeric_cols = [col for col, conf in factor_config.items() if conf.get("type") == "numeric" and conf.get("normalize")]
        if numeric_cols:
            logger("å¯¹æ•°å€¼åž‹å­—æ®µè¿›è¡Œå½’ä¸€åŒ–...")
            scaler = MinMaxScaler()
            df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

    return df, scaler

# ai_pipeline/pipeline.py
import time
from sklearn.model_selection import train_test_split
from ai_pipeline.registry import model_registry
from ai_pipeline.logger import logger
from ai_pipeline.utils import should_execute_step
from ai_pipeline.data_handler import clean_and_preprocess

def train_pipeline(
    factor_params: dict,
    model_params: dict,
    model_type: str,
    dataset_files: list,
    user_pipeline_control: dict,
    logger_instance=None
):
    log = logger_instance or logger.log
    log(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: æ¨¡åž‹ç±»åž‹={model_type}")

    model_class = model_registry.get(model_type)
    if not model_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")

    model = model_class(model_params, logger=log)
    adapter_control = model.pipeline_control

    normalize = should_execute_step("normalize", user_pipeline_control, adapter_control)
    df, scaler = clean_and_preprocess(dataset_files, factor_params, normalize, logger=log)

    X = df.drop(columns=["target"])
    y = df["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    log("å¼€å§‹æ¨¡åž‹è®­ç»ƒ...")
    result = model.train(X_train, y_train, X_test, y_test)

    if should_execute_step("postprocess", user_pipeline_control, adapter_control):
        result = model.postprocess(result)

    model_path = f"./outputs/model_{model_type}_{int(time.time())}.pkl"
    model.save_model(model_path)
    log(f"æ¨¡åž‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è‡³: {model_path}")

    return result

```

~~~python
### ðŸ§  æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆbase.pyï¼‰
```python
# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params, logger):
        self.model_params = model_params
        self.logger = logger

    @abstractmethod
    def train(self, X_train, y_train):
        pass

    @abstractmethod
    def predict(self, X):
        pass

    @abstractmethod
    def save(self, path):
        pass
```

### âœ… æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆregistry.pyï¼‰
```python
# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter
from ai_pipeline.adapters.dnn import DNNAdapter

MODEL_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
}
```

### âž• æ–°å¢ž DNN æ¨¡åž‹é€‚é…å™¨ï¼ˆadapters/dnn.pyï¼‰
```python
# ai_pipeline/adapters/dnn.py
from ai_pipeline.base import BaseModelAdapter
from sklearn.neural_network import MLPRegressor
import joblib

class DNNAdapter(BaseModelAdapter):
    def __init__(self, model_params, logger):
        super().__init__(model_params, logger)
        self.model = MLPRegressor(**model_params)

    def train(self, X_train, y_train):
        self.logger("[DNN] å¼€å§‹è®­ç»ƒ")
        self.model.fit(X_train, y_train)
        self.logger("[DNN] è®­ç»ƒå®Œæˆ")

    def predict(self, X):
        return self.model.predict(X)

    def save(self, path):
        joblib.dump(self.model, path)
```

### ðŸ§ª ç¤ºä¾‹æµ‹è¯•è°ƒç”¨æ›´æ–°
```python
from ai_pipeline.pipeline import train_pipeline

# ä½¿ç”¨æ·±åº¦ç¥žç»ç½‘ç»œæ¨¡åž‹è®­ç»ƒ
result = train_pipeline(
    factor_params={
        "feature1": {"type": "numeric", "normalize": True},
        "feature2": {"type": "numeric", "normalize": True},
    },
    model_params={
        "hidden_layer_sizes": (50, 50),
        "activation": "relu",
        "max_iter": 200
    },
    model_type="dnn",
    dataset_files=["dataset1.csv", "dataset2.csv"],
    user_pipeline_control={
        "normalize": True,
        "postprocess": True,
    }
)
```

### âœ… æµç¨‹å…¼å®¹æ€§è¯´æ˜Ž
- æ¨¡åž‹åªéœ€è¦ç»§æ‰¿ `BaseModelAdapter`
- æ³¨å†Œåœ¨ `registry.py` çš„ `MODEL_REGISTRY` ä¸­
- æŽ§åˆ¶æµç¨‹ç»“æž„å’Œæ—¥å¿—æœºåˆ¶é€šç”¨ï¼Œé€‚é…ä»»æ„ç»“æž„åŒ–æ¨¡åž‹

å¦‚éœ€æ·»åŠ æ”¯æŒåˆ†ç±»æ¨¡åž‹ã€NLP æ¨¡åž‹ã€æˆ–å¤–éƒ¨æœåŠ¡æ¨¡åž‹ï¼ˆå¦‚ OpenAIï¼‰ï¼Œç»“æž„å®Œå…¨é€‚é…ï¼Œåªéœ€æŒ‰ä¸Šè¿°ç»“æž„æ–°å¢žé€‚é…å™¨ã€‚æ˜¯å¦è¿˜éœ€è¦æˆ‘æ·»åŠ ä¸€ä¸ªå¦‚ HuggingFace BERT çš„é€‚é…å™¨ï¼Ÿ

~~~

~~~python
### ðŸ§  æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆbase.pyï¼‰
```python
# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params, logger):
        self.model_params = model_params  # æ¨¡åž‹å‚æ•°ï¼ˆå¦‚å±‚æ•°ã€å­¦ä¹ çŽ‡ç­‰ï¼‰
        self.logger = logger              # æ—¥å¿—å‡½æ•°æˆ–æµå¼æ—¥å¿—è¾“å‡ºå™¨

    @abstractmethod
    def train(self, X_train, y_train):
        pass  # å­ç±»å®žçŽ°ï¼šè®­ç»ƒæ¨¡åž‹

    @abstractmethod
    def predict(self, X):
        pass  # å­ç±»å®žçŽ°ï¼šæ‰§è¡Œé¢„æµ‹

    @abstractmethod
    def save(self, path):
        pass  # å­ç±»å®žçŽ°ï¼šä¿å­˜æ¨¡åž‹
```

### âœ… æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆregistry.pyï¼‰
```python
# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter
from ai_pipeline.adapters.dnn import DNNAdapter
from ai_pipeline.adapters.huggingface import HuggingFaceTextAdapter

MODEL_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
    "huggingface_text": HuggingFaceTextAdapter,
}
```

### âž• HuggingFace æ–‡æœ¬æ¨¡åž‹é€‚é…å™¨ï¼ˆadapters/huggingface.pyï¼‰
```python
# ai_pipeline/adapters/huggingface.py
from ai_pipeline.base import BaseModelAdapter
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import os

class HuggingFaceTextAdapter(BaseModelAdapter):
    def __init__(self, model_params, logger):
        super().__init__(model_params, logger)
        model_name = model_params.get("model_name", "distilbert-base-uncased-finetuned-sst-2-english")
        self.logger(f"[HF] åŠ è½½æ¨¡åž‹: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.pipeline = pipeline("sentiment-analysis", model=self.model, tokenizer=self.tokenizer)

    def train(self, X_train, y_train):
        self.logger("[HF] æ–‡æœ¬æ¨¡åž‹å½“å‰ä¸ºé¢„è®­ç»ƒæŽ¨ç†æ¨¡å¼ï¼Œè·³è¿‡è®­ç»ƒé˜¶æ®µ")

    def predict(self, X):
        self.logger("[HF] å¼€å§‹æ–‡æœ¬é¢„æµ‹")
        texts = X["text"] if isinstance(X, dict) else X  # æ”¯æŒ DataFrame æˆ–åŽŸå§‹åˆ—è¡¨
        return self.pipeline(texts)

    def save(self, path):
        self.logger("[HF] ä¿å­˜æ¨¡åž‹å’Œåˆ†è¯å™¨")
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)
```

### âœ… è¯´æ˜Žï¼ˆæ¯è¡Œè§£é‡Šï¼‰
```python
model_name = model_params.get("model_name", "distilbert-base-uncased-finetuned-sst-2-english")
# èŽ·å–æ¨¡åž‹åç§°ï¼ˆç”¨æˆ·ä¼ å…¥æˆ–é»˜è®¤ä½¿ç”¨æƒ…æ„Ÿåˆ†æžæ¨¡åž‹ï¼‰

self.tokenizer = AutoTokenizer.from_pretrained(model_name)
self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
# åŠ è½½ HuggingFace æ¨¡åž‹å’Œ tokenizer

self.pipeline = pipeline("sentiment-analysis", model=self.model, tokenizer=self.tokenizer)
# å°è£…ä¸º transformers pipelineï¼Œä¾¿äºŽç›´æŽ¥æŽ¨ç†
```

### ðŸ§ª ç¤ºä¾‹æµ‹è¯•è°ƒç”¨ï¼ˆæ–‡æœ¬æ¨¡åž‹ï¼‰
```python
from ai_pipeline.pipeline import train_pipeline

result = train_pipeline(
    factor_params={},  # æ–‡æœ¬æ¨¡åž‹é€šå¸¸ä¸éœ€è¦æ•°å€¼å› å­
    model_params={"model_name": "distilbert-base-uncased-finetuned-sst-2-english"},
    model_type="huggingface_text",
    dataset_files=["texts.csv"],
    user_pipeline_control={
        "normalize": False,
        "postprocess": False,
    }
)
```

### ðŸ“¦ è¾“å…¥æ–‡æœ¬æ•°æ®è¦æ±‚
- CSV æ–‡ä»¶åº”åŒ…å« `text` å­—æ®µåˆ—ï¼š
```
text
"I love this product."
"This is terrible."
```

---
å¦‚éœ€æ‰©å±•ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ã€å¤šè¯­è¨€æ¨¡åž‹ï¼Œåªéœ€æ›´æ¢æ¨¡åž‹åå’Œ pipeline ç±»åž‹å³å¯ã€‚
æ˜¯å¦æŽ¥ä¸‹æ¥éœ€è¦æŽ¥å…¥ç±»ä¼¼ DeepSeek æˆ– OpenAI API çš„æŽ¨ç†æ¨¡åž‹ï¼Ÿ

~~~

~~~python
## âœ… AI å¹³å°é€šç”¨æ¨¡åž‹è®­ç»ƒæµç¨‹æ¨¡å—ï¼ˆè®­ç»ƒæµç¨‹ã€é€‚é…å™¨ç»“æž„ã€API æŽ¨ç†æ¨¡åž‹æ”¯æŒï¼‰

---

## ðŸ“ é¡¹ç›®æ¨¡å—æ¦‚è§ˆ
```bash
ai_pipeline/
â”œâ”€â”€ base.py                    # æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆå®šä¹‰æŽ¥å£ï¼‰
â”œâ”€â”€ pipeline.py                # ä¸»è®­ç»ƒæµç¨‹æ¨¡å—
â”œâ”€â”€ registry.py                # æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆç”¨äºŽæ¨¡åž‹ç±»åž‹æŸ¥æ‰¾ï¼‰
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ linear_regression.py   # sklearn çº¿æ€§å›žå½’æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ dnn.py                 # sklearn DNN æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ huggingface.py         # æœ¬åœ° transformer æ¨¡åž‹é€‚é…å™¨
â”‚   â””â”€â”€ openai_api.py          # å¤–éƒ¨ OpenAI / DeepSeek æŽ¥å£é€‚é…å™¨
```

---

## ðŸ“¦ æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆbase.pyï¼‰
```python
# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params, logger):
        self.model_params = model_params  # æ¨¡åž‹åˆå§‹åŒ–å‚æ•°
        self.logger = logger              # æ—¥å¿—å‡½æ•°

    @abstractmethod
    def train(self, X_train, y_train):
        pass  # è®­ç»ƒæ–¹æ³•

    @abstractmethod
    def predict(self, X):
        pass  # é¢„æµ‹æ–¹æ³•

    @abstractmethod
    def save(self, path):
        pass  # æ¨¡åž‹ä¿å­˜æ–¹æ³•
```

---

## ðŸ§  å¤–éƒ¨ API æ¨¡åž‹é€‚é…å™¨ï¼ˆå¦‚ OpenAI æˆ– DeepSeekï¼‰ï¼ˆadapters/openai_api.pyï¼‰
```python
# ai_pipeline/adapters/openai_api.py
import openai
from ai_pipeline.base import BaseModelAdapter

class OpenAIChatAdapter(BaseModelAdapter):
    def __init__(self, model_params, logger):
        super().__init__(model_params, logger)
        self.api_key = model_params.get("api_key")
        self.model = model_params.get("model", "gpt-4")
        openai.api_key = self.api_key
        logger(f"[OpenAI] ä½¿ç”¨æ¨¡åž‹ï¼š{self.model}")

    def train(self, X_train, y_train):
        self.logger("[OpenAI] å¤–éƒ¨ API æ— éœ€è®­ç»ƒ")

    def predict(self, X):
        prompt = X.get("prompt")
        self.logger(f"[OpenAI] è¯·æ±‚æç¤ºè¯: {prompt}")

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        output = ""
        for chunk in response:
            delta = chunk["choices"][0]["delta"].get("content", "")
            output += delta
            self.logger(delta, stream=True)  # æµå¼è¿”å›ž
        return output

    def save(self, path):
        self.logger("[OpenAI] API æ¨¡åž‹æ— éœ€ä¿å­˜")
```

### ðŸ” æ¯è¡Œè§£é‡Šï¼š
```python
openai.api_key = self.api_key
# è®¾ç½® OpenAI API å¯†é’¥

response = openai.ChatCompletion.create(..., stream=True)
# å¯ç”¨æµå¼å“åº”ï¼Œé€å—æŽ¥æ”¶æ¨¡åž‹è¾“å‡º

for chunk in response:
    delta = chunk["choices"][0]["delta"].get("content", "")
    self.logger(delta, stream=True)
# å°†å†…å®¹é€æ¡æµå¼è¾“å‡ºç»™å‰ç«¯
```

---

## ðŸ”Œ æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆregistry.pyï¼‰
```python
# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter
from ai_pipeline.adapters.dnn import DNNAdapter
from ai_pipeline.adapters.huggingface import HuggingFaceTextAdapter
from ai_pipeline.adapters.openai_api import OpenAIChatAdapter

MODEL_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
    "huggingface_text": HuggingFaceTextAdapter,
    "openai_chat": OpenAIChatAdapter,
}
```

---

## ðŸ” é€šç”¨è®­ç»ƒæµç¨‹å°è£…æ¨¡å—ï¼ˆpipeline.pyï¼‰
```python
# ai_pipeline/pipeline.py
import pandas as pd
from ai_pipeline.registry import MODEL_REGISTRY

def default_logger(msg, stream=False):
    print("[LOG]", msg, flush=True)  # é»˜è®¤æ—¥å¿—è¾“å‡ºå‡½æ•°

def train_pipeline(factor_params, model_params, model_type,
                   dataset_files, user_pipeline_control=None,
                   base_pipeline_control=None, logger=default_logger):

    logger("[Pipeline] åŠ è½½æ¨¡åž‹é€‚é…å™¨")
    model_cls = MODEL_REGISTRY[model_type]
    model = model_cls(model_params, logger)

    logger("[Pipeline] è¯»å–å¹¶åˆå¹¶æ•°æ®é›†")
    df = pd.concat([pd.read_csv(f) for f in dataset_files], ignore_index=True)
    df.drop_duplicates(inplace=True)

    logger("[Pipeline] åº”ç”¨å­—æ®µæ˜ å°„å’Œå› å­æ¸…æ´—")
    df = df.dropna(subset=factor_params.get("required_fields", []))

    pipeline_ctl = {"normalize": True, "postprocess": True}  # é»˜è®¤æµç¨‹å¼€å…³
    if base_pipeline_control:
        pipeline_ctl.update(base_pipeline_control)
    if user_pipeline_control:
        pipeline_ctl.update(user_pipeline_control)

    if pipeline_ctl["normalize"]:
        logger("[Pipeline] æ‰§è¡Œå½’ä¸€åŒ–")
        for col in factor_params.get("numeric_fields", []):
            df[col] = (df[col] - df[col].mean()) / df[col].std()

    logger("[Pipeline] åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†")
    train_df = df.sample(frac=0.8, random_state=42)
    test_df = df.drop(train_df.index)

    logger("[Pipeline] å¼€å§‹è®­ç»ƒæ¨¡åž‹")
    X_train, y_train = train_df.drop("target", axis=1), train_df["target"]
    model.train(X_train, y_train)

    logger("[Pipeline] æ¨¡åž‹è®­ç»ƒå®Œæˆï¼Œå¼€å§‹é¢„æµ‹")
    y_pred = model.predict(test_df.drop("target", axis=1))

    if pipeline_ctl["postprocess"]:
        logger("[Pipeline] è¿›è¡Œåå½’ä¸€åŒ–ï¼ˆä¼ªæ“ä½œï¼‰")

    logger("[Pipeline] ä¿å­˜æ¨¡åž‹")
    model.save("saved_models/{}_model.bin".format(model_type))

    return {"predictions": y_pred}
```

### ðŸ” æ¯è¡Œä»£ç è§£é‡Š
```python
model_cls = MODEL_REGISTRY[model_type]
model = model_cls(model_params, logger)
# èŽ·å–æ¨¡åž‹ç±»å¹¶åˆå§‹åŒ–å®žä¾‹

pd.concat([pd.read_csv(f) for f in dataset_files])
# åˆå¹¶å¤šä¸ª CSV æ•°æ®é›†

df.dropna(subset=factor_params.get("required_fields", []))
# åˆ é™¤å› å­ç¼ºå¤±å€¼çš„æ ·æœ¬

if pipeline_ctl["normalize"]:
    for col in factor_params.get("numeric_fields", []):
        df[col] = ...
# å¯¹æ•°å€¼åž‹å­—æ®µè¿›è¡Œå½’ä¸€åŒ–

model.train(X_train, y_train)
# å¯åŠ¨æ¨¡åž‹è®­ç»ƒ

model.predict(...)
# æ‰§è¡Œé¢„æµ‹ï¼ˆå¯è¿”å›žæ•°ç»„æˆ–æ–‡æœ¬ï¼‰

model.save(...)
# ä¿å­˜æ¨¡åž‹åˆ°æŒ‡å®šè·¯å¾„
```

---

## âœ… ç¤ºä¾‹ä½¿ç”¨ï¼š
```python
train_pipeline(
    factor_params={
        "required_fields": ["text"],
        "numeric_fields": [],
    },
    model_params={
        "api_key": "sk-xxx",
        "model": "gpt-4"
    },
    model_type="openai_chat",
    dataset_files=["chat_prompts.csv"],
    user_pipeline_control={"normalize": False, "postprocess": False}
)
```

CSV å†…å®¹ï¼š
```csv
prompt
"è¯·å¸®æˆ‘æ€»ç»“è¿™æ®µè¯"
"è§£é‡Šä»€ä¹ˆæ˜¯ Transformer æ¨¡åž‹"
```

---

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ åŠ å…¥ S3 è‡ªåŠ¨ä¸Šä¼ ã€è®­ç»ƒæ—¥å¿—å®žæ—¶ WebSocket å›žä¼ ã€æˆ–æ¨¡åž‹å‘å¸ƒé€»è¾‘ï¼Ÿ

~~~

~~~python
# å®Œæ•´çš„ AI æ¨¡åž‹è®­ç»ƒæµç¨‹æ¡†æž¶ï¼ˆå«é€‚é…å™¨ + æ³¨å†Œè¡¨ + æ•°æ®å¤„ç† + æ¨¡åž‹å®žä¾‹ï¼‰

æœ¬æž¶æž„è®¾è®¡é€‚ç”¨äºŽï¼š
- å¤šæ¨¡åž‹é€‚é…ï¼ˆçº¿æ€§å›žå½’ã€å¤§è¯­è¨€æ¨¡åž‹ã€åˆ†æžæ¨¡åž‹ç­‰ï¼‰
- æ¨¡åž‹è®­ç»ƒæµç¨‹èŠ‚ç‚¹çš„çµæ´»æŽ§åˆ¶ï¼ˆå½’ä¸€åŒ–ã€åŽå¤„ç†ç­‰ï¼‰
- æ”¯æŒç”¨æˆ·æŽ§åˆ¶ä¸Žæ¨¡åž‹åˆ›å»ºè€…æŽ§åˆ¶çš„ä¼˜å…ˆçº§æœºåˆ¶
- å¤šæ•°æ®é›†åˆå¹¶å¤„ç†ã€å­—æ®µç»Ÿä¸€ã€æ¸…æ´—ã€å½’ä¸€åŒ–ç­‰
- è®­ç»ƒæ—¥å¿—æ”¯æŒå®žæ—¶æŽ¨é€ï¼ˆå¯æŽ¥å…¥ WebSocketï¼‰

---

## é¡¹ç›®ç›®å½•ç»“æž„
```
ml_platform/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ registry.py                # æ¨¡åž‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ base_model.py             # æ¨¡åž‹é€‚é…å™¨åŸºç±»
â”‚   â””â”€â”€ linear_regression.py      # ç¤ºä¾‹æ¨¡åž‹ï¼šçº¿æ€§å›žå½’
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ trainer.py                # ä¸»è®­ç»ƒæµç¨‹æŽ§åˆ¶å™¨
â”œâ”€â”€ preprocess/
â”‚   â”œâ”€â”€ dataset_handler.py        # æ•°æ®é›†è¯»å–ä¸Žåˆå¹¶ï¼ˆæ”¯æŒ openpyxlï¼‰
â”‚   â””â”€â”€ cleaner.py                # æ•°æ®æ¸…æ´—ä¸Žé¢„å¤„ç†é€»è¾‘
â””â”€â”€ run_train.py                 # æ¨¡æ‹Ÿè®­ç»ƒæµç¨‹å…¥å£
```

---

## core/registry.py
```python
# æ¨¡åž‹æ³¨å†Œä¸­å¿ƒï¼Œæ‰€æœ‰æ¨¡åž‹éœ€æ³¨å†Œè¿›æ¥ä»¥ä¾¿è®­ç»ƒå™¨ä½¿ç”¨
MODEL_REGISTRY = {}

def register_model(model_type):
    def wrapper(cls):
        MODEL_REGISTRY[model_type] = cls
        return cls
    return wrapper

def get_model_class(model_type):
    return MODEL_REGISTRY.get(model_type)
```

---

## core/base_model.py
```python
# æ‰€æœ‰æ¨¡åž‹å¿…é¡»ç»§æ‰¿çš„åŸºç±»
from abc import ABC, abstractmethod

class BaseModel(ABC):
    def __init__(self, model_params, factor_params, control_config):
        self.model_params = model_params
        self.factor_params = factor_params
        self.control_config = control_config  # å½’ä¸€åŒ–/åŽå¤„ç†æ‰§è¡Œæ ‡å¿—

    @abstractmethod
    def train(self, train_data):
        pass

    @abstractmethod
    def predict(self, test_data):
        pass

    def should_normalize(self):
        return self.control_config.get("normalize", False)

    def should_postprocess(self):
        return self.control_config.get("postprocess", False)
```

---

## core/linear_regression.py
```python
# ç¤ºä¾‹ï¼šçº¿æ€§å›žå½’æ¨¡åž‹
import numpy as np
from sklearn.linear_model import LinearRegression
from core.base_model import BaseModel
from core.registry import register_model

@register_model("linear_regression")
class LinearRegressionModel(BaseModel):
    def __init__(self, model_params, factor_params, control_config):
        super().__init__(model_params, factor_params, control_config)
        self.model = LinearRegression(**self.model_params)

    def train(self, train_data):
        X, y = train_data.drop(columns=["target"]), train_data["target"]
        self.model.fit(X, y)

    def predict(self, test_data):
        return self.model.predict(test_data)
```

---

## preprocess/dataset_handler.py
```python
# ä½¿ç”¨ openpyxl è¯»å–æ‰€æœ‰æ•°æ®é›†å†…å®¹ï¼Œè¿”å›ž pandas DataFrame
import pandas as pd
from openpyxl import load_workbook
import io

def read_excel_as_dataframe(file):
    if isinstance(file, (str, bytes)):
        wb = load_workbook(filename=file, read_only=True)
    else:
        wb = load_workbook(filename=io.BytesIO(file.read()), read_only=True)
    sheet = wb.active
    data = [[cell.value for cell in row] for row in sheet.iter_rows()]
    headers, rows = data[0], data[1:]
    return pd.DataFrame(rows, columns=headers)

def merge_datasets(files):
    dfs = [read_excel_as_dataframe(f) for f in files]
    # å¯¹é½åˆ—åé¡ºåºåŽåˆå¹¶
    columns = dfs[0].columns
    aligned_dfs = [df[columns] for df in dfs]
    merged = pd.concat(aligned_dfs, ignore_index=True).drop_duplicates()
    return merged
```

---

## preprocess/cleaner.py
```python
# æ•°æ®æ¸…æ´—ï¼šåŒ…æ‹¬å› å­ç¼ºå¤±å‰”é™¤ã€å­—æ®µæ˜ å°„ã€å½’ä¸€åŒ–
import pandas as pd

def clean_data(df, factor_config):
    for factor in factor_config:
        name = factor["name"]
        dtype = factor.get("type")
        if dtype == "number":
            min_val = factor.get("min")
            max_val = factor.get("max")
            enum = factor.get("enum")
            if min_val is not None:
                df = df[df[name] >= min_val]
            if max_val is not None:
                df = df[df[name] <= max_val]
            if enum:
                df = df[df[name].isin(enum)]
        elif dtype == "string":
            enum = factor.get("enum")
            if enum:
                df = df[df[name].isin(enum)]
    df = df.dropna(subset=[f["name"] for f in factor_config])
    return df

def normalize_data(df, factor_config):
    for factor in factor_config:
        if factor.get("normalize") and factor.get("type") == "number":
            col = factor["name"]
            min_val = df[col].min()
            max_val = df[col].max()
            df[col] = (df[col] - min_val) / (max_val - min_val)
    return df

def split_data(df, train_ratio=0.8):
    df = df.sample(frac=1).reset_index(drop=True)
    split = int(len(df) * train_ratio)
    return df[:split], df[split:]
```

---

## pipeline/trainer.py
```python
from core.registry import get_model_class
from preprocess.dataset_handler import merge_datasets
from preprocess.cleaner import clean_data, normalize_data, split_data

class TrainingPipeline:
    def __init__(self, model_type, model_params, factor_params, dataset_files, control_flags):
        self.model_type = model_type
        self.model_params = model_params
        self.factor_params = factor_params
        self.dataset_files = dataset_files
        self.control_flags = control_flags

    def run(self):
        print("[æµç¨‹] åˆå¹¶æ•°æ®é›†...")
        df = merge_datasets(self.dataset_files)

        print("[æµç¨‹] æ¸…æ´—æ•°æ®...")
        df = clean_data(df, self.factor_params)

        if self.control_flags.get("normalize"):
            print("[æµç¨‹] æ•°æ®å½’ä¸€åŒ–...")
            df = normalize_data(df, self.factor_params)

        print("[æµç¨‹] åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†...")
        train_df, test_df = split_data(df)

        print("[æµç¨‹] åŠ è½½æ¨¡åž‹å¹¶è®­ç»ƒ...")
        model_cls = get_model_class(self.model_type)
        if not model_cls:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {self.model_type}")

        model = model_cls(self.model_params, self.factor_params, self.control_flags)
        model.train(train_df)

        print("[æµç¨‹] è®­ç»ƒå®Œæˆï¼Œæ‰§è¡Œé¢„æµ‹...")
        preds = model.predict(test_df.drop(columns=["target"]))

        if model.should_postprocess():
            print("[æµç¨‹] æ‰§è¡ŒåŽå¤„ç†...")
            # å¯ä»¥æ·»åŠ åå½’ä¸€åŒ–ç­‰é€»è¾‘

        print("[æµç¨‹] ä¿å­˜æ¨¡åž‹ä¸Žç»“æžœ...")
        # TODO: ä¿å­˜æ¨¡åž‹åˆ° S3ã€æœ¬åœ°ç­‰

        return preds
```

---

## run_train.pyï¼ˆè®­ç»ƒå…¥å£æ¨¡æ‹Ÿï¼‰
```python
from pipeline.trainer import TrainingPipeline

# æ¨¡æ‹Ÿå‰ç«¯è¯·æ±‚ä¼ å‚
model_type = "linear_regression"
model_params = {"fit_intercept": True}
factor_params = [
    {"name": "age", "type": "number", "normalize": True, "min": 10, "max": 80},
    {"name": "gender", "type": "string", "enum": ["male", "female"]}
]
dataset_files = ["./datasets/sample1.xlsx", "./datasets/sample2.xlsx"]
control_flags = {"normalize": True, "postprocess": False}

pipeline = TrainingPipeline(
    model_type=model_type,
    model_params=model_params,
    factor_params=factor_params,
    dataset_files=dataset_files,
    control_flags=control_flags
)
preds = pipeline.run()
print("é¢„æµ‹ç»“æžœï¼š", preds)
```

---

## âœ… æ€»ç»“
è¯¥æ¡†æž¶æ”¯æŒï¼š
- å„ç§æ¨¡åž‹æ³¨å†Œä¸Žè®­ç»ƒæµç¨‹è§£è€¦
- ä½¿ç”¨ openpyxl è¿›è¡Œ Excel è¯»å–
- åŸºäºŽå› å­é…ç½®çš„æ•°æ®æ¸…æ´—ã€å½’ä¸€åŒ–ä¸Žæ‹†åˆ†
- æ¨¡åž‹è®­ç»ƒä¸Žé¢„æµ‹æŽ¥å£æ ‡å‡†ç»Ÿä¸€
- æŽ§åˆ¶é€»è¾‘çµæ´»ï¼Œå¯æ‰©å±•å…¶ä»–æ¨¡åž‹

ðŸ“Œ å¦‚éœ€ï¼š
- S3 å‘å¸ƒä¸Žæ¨¡åž‹æŒä¹…åŒ–
- ä½¿ç”¨ Celery åŠ å…¥ä»»åŠ¡é˜Ÿåˆ—
- æ—¥å¿—å®žæ—¶å›žä¼ ï¼ˆWebSocket æˆ– SSEï¼‰

å¯ç»§ç»­æ‰©å±•ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æ·»åŠ ç›¸å…³æ¨¡å—ã€‚

~~~

