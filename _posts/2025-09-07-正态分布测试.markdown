---
layout:     post
title:      "æ­£æ€åˆ†å¸ƒæµ‹è¯•"
subtitle:   ""
date:       2025-09-07
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - å­¦ä¹ æ¨¡å‹
---

# å¦‚ä½•éªŒè¯ä¸€ç»„æ•°æ®æ˜¯å¦ç¬¦åˆâ€œÏƒï¼ˆsigmaï¼‰æ­£æ€åˆ†å¸ƒâ€è§„åˆ™ï¼ˆå®ç”¨æŒ‡å—ï¼‰

ä¸‹é¢ç»™å‡ºä¸€å¥—æ¸…æ™°çš„ã€å¯å¤ç”¨çš„æ­¥éª¤â€”â€”æ—¢æœ‰ç›´è§‚å¯è§†åŒ–ï¼Œä¹Ÿæœ‰å®šé‡æ£€éªŒï¼Œå¹¶é™„å¸¦ Python å®ç”¨å‡½æ•°ã€‚ä¸»è¦æ€è·¯æ˜¯åŒæ—¶æ£€æŸ¥**ç»éªŒè§„åˆ™ï¼ˆ68â€“95â€“99.7%ï¼‰\**ä¸\**ç»Ÿè®¡æ£€éªŒ/å›¾å½¢è¯Šæ–­**ï¼Œå› ä¸ºå•ä¸€æ–¹æ³•å®¹æ˜“è¯¯å¯¼ã€‚

------

## 1) å¿«é€Ÿæ•°å€¼æ£€æŸ¥ï¼ˆå‡å€¼ / æ ‡å‡†å·® / z-score /ç»éªŒæ¯”ä¾‹ï¼‰

è®¡ç®—æ ·æœ¬å‡å€¼ `Î¼` å’Œæ ‡å‡†å·® `Ïƒ`ï¼Œç„¶åç»Ÿè®¡æ•°æ®è½åœ¨åŒºé—´ `Î¼Â±1Ïƒ`ã€`Î¼Â±2Ïƒ`ã€`Î¼Â±3Ïƒ` çš„æ¯”ä¾‹ï¼Œå’Œç†è®ºæ¯”ä¾‹å¯¹æ¯”ï¼š

- ç†è®ºï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼š
  - Â±1Ïƒ â‰ˆ 68.27%
  - Â±2Ïƒ â‰ˆ 95.45%
  - Â±3Ïƒ â‰ˆ 99.73%

**åˆ¤æ–­å‚è€ƒï¼ˆç»éªŒï¼‰**ï¼šè‹¥å®æµ‹æ¯”ä¾‹æ¥è¿‘ç†è®ºå€¼ï¼ˆä¾‹å¦‚åœ¨ Â±3% æˆ– Â±5% çš„å®¹å·®å†…ï¼‰ï¼Œè¯´æ˜ä¸æ­£æ€ç›¸ç¬¦ï¼›è‹¥å·®å¼‚å¾ˆå¤§ï¼Œåˆ™å¯èƒ½ä¸æ˜¯æ­£æ€ã€‚

------

## 2) å¯è§†åŒ–è¯Šæ–­ï¼ˆå¿…åšï¼‰

- **ç›´æ–¹å›¾ + æ‹Ÿåˆæ­£æ€æ›²çº¿**ï¼šè§‚å¯Ÿå½¢çŠ¶ï¼ˆå¯¹ç§°æ€§ã€å°¾éƒ¨ï¼‰ã€‚
- **Qâ€“Q å›¾ï¼ˆQuantile-Quantile plotï¼‰**ï¼šè‹¥ç‚¹æ¥è¿‘å¯¹è§’çº¿ï¼Œè¯´æ˜æ¥è¿‘æ­£æ€ï¼›ç³»ç»Ÿæ€§åç¦»è¡¨æ˜åæ–œæˆ–åš/è–„å°¾ã€‚
- **ç®±çº¿å›¾** å¯ä»¥å¸®åŠ©æŸ¥çœ‹å¼‚å¸¸å€¼ã€‚

å¯è§†åŒ–é€šå¸¸æ¯”å•ä¸ª p å€¼æ›´æœ‰ä¿¡æ¯é‡ã€‚

------

## 3) ç»Ÿè®¡æ£€éªŒï¼ˆå®šé‡ï¼‰

- **Shapiroâ€“Wilk æ£€éªŒ**ï¼ˆ`scipy.stats.shapiro`ï¼‰ï¼šå¯¹å°/ä¸­æ ·æœ¬ï¼ˆä¾‹å¦‚ n < 5000ï¼‰å¾ˆå¸¸ç”¨ã€‚`p > 0.05` é€šå¸¸è¡¨ç¤ºä¸èƒ½æ‹’ç»æ­£æ€æ€§ï¼ˆå³â€œçœ‹èµ·æ¥åƒæ­£æ€â€ï¼‰ã€‚
- **Andersonâ€“Darling æ£€éªŒ**ï¼ˆ`scipy.stats.anderson`ï¼‰ï¼šç»™å‡ºæ›´æ•æ„Ÿçš„åå·®æ£€æµ‹ï¼Œè¿”å›ç»Ÿè®¡é‡å¹¶æœ‰æ˜¾è‘—æ€§è¡¨æ ¼ã€‚
- **Kolmogorovâ€“Smirnov æ£€éªŒï¼ˆå¯¹æ¯”æ­£æ€ï¼‰**ï¼šåœ¨ç”¨äºæ¯”è¾ƒæ€»ä½“å’ŒæŒ‡å®šåˆ†å¸ƒæ—¶è¦è°¨æ…ï¼ˆå‚æ•°éœ€ç”¨æ ·æœ¬ä¼°è®¡æ—¶ä¸´ç•Œå€¼æ”¹å˜ï¼‰ã€‚
- æ³¨æ„ï¼šå½“æ ·æœ¬é‡å¾ˆå¤§æ—¶ï¼ˆä¾‹å¦‚ n å¾ˆå¤§ï¼‰ï¼Œå¾®å°åå·®ä¼šå¯¼è‡´æ£€éªŒæ˜¾è‘—ï¼ˆæ‹’ç»æ­£æ€æ€§ï¼‰ï¼Œå› æ­¤éœ€ç»“åˆå¯è§†åŒ–ä¸å®é™…åº”ç”¨åœºæ™¯åˆ¤æ–­ã€‚

------

## 4) å…¶ä»–æŒ‡æ ‡

- **ååº¦ï¼ˆskewnessï¼‰** å’Œ **å³°åº¦ï¼ˆkurtosisï¼‰**ï¼šæ­£æ€åˆ†å¸ƒçš„ååº¦â‰ˆ0ï¼Œå³°åº¦â‰ˆ3ï¼ˆå¸¸ç”¨ excess kurtosis = kurtosis-3ï¼‰ã€‚æ˜¾è‘—åç¦»è¡¨æ˜ä¸æ˜¯æ­£æ€ã€‚
- **ç¦»ç¾¤ç‚¹æ£€æµ‹**ï¼šè‹¥å°‘æ•°æç«¯å€¼å½±å“æ•´ä½“åˆ†å¸ƒï¼Œè€ƒè™‘å…ˆå¤„ç†æˆ–æŠ¥å‘Šã€‚

------

## 5) å®ç”¨ Python å‡½æ•°ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

def check_sigma_distribution(arr, plot=True, tolerance_pct=5.0):
    """
    è¾“å…¥: ä¸€ç»´æ•°ç»„æˆ– pandas Series
    è¾“å‡º: ä¸€ä¸ªå­—å…¸æŠ¥å‘Š + å¯é€‰å›¾å½¢
    tolerance_pct: ç”¨äºç»éªŒè§„åˆ™æ¯”è¾ƒçš„å®¹å·®ï¼ˆç™¾åˆ†æ¯”ï¼Œä¾‹å¦‚5%ï¼‰
    """
    x = np.asarray(arr).astype(float)
    x = x[~np.isnan(x)]
    n = x.size
    mu = x.mean()
    sigma = x.std(ddof=1)  # æ ·æœ¬æ ‡å‡†å·®

    # empirical proportions
    within_1 = np.mean((x >= mu - sigma) & (x <= mu + sigma)) * 100
    within_2 = np.mean((x >= mu - 2*sigma) & (x <= mu + 2*sigma)) * 100
    within_3 = np.mean((x >= mu - 3*sigma) & (x <= mu + 3*sigma)) * 100

    empirical = {'Â±1Ïƒ': within_1, 'Â±2Ïƒ': within_2, 'Â±3Ïƒ': within_3}
    theoretical = {'Â±1Ïƒ': 68.27, 'Â±2Ïƒ': 95.45, 'Â±3Ïƒ': 99.73}
    diffs = {k: empirical[k] - theoretical[k] for k in empirical}

    # statistical tests
    sw_stat, sw_p = (None, None)
    if n >= 3 and n <= 5000:
        sw_stat, sw_p = stats.shapiro(x)
    ad_result = stats.anderson(x, dist='norm')  # returns statistic + critical values

    skew = stats.skew(x)
    kurt = stats.kurtosis(x, fisher=False)  # Pearson's definition (â‰ˆ3 for normal)
    excess_kurtosis = kurt - 3.0

    report = {
        'n': n, 'mean': mu, 'std_sample': sigma,
        'empirical_pct': empirical, 'theoretical_pct': theoretical, 'diffs_pct': diffs,
        'skewness': skew, 'kurtosis': kurt, 'excess_kurtosis': excess_kurtosis,
        'shapiro': {'stat': sw_stat, 'p': sw_p} if sw_stat is not None else 'skipped (n out of range)',
        'anderson': {'stat': ad_result.statistic, 'critical_values': ad_result.critical_values, 'significance_level': ad_result.significance_level}
    }

    # simple rule of thumb decision (heuristic)
    within_tolerance = all(abs(diffs[k]) <= tolerance_pct for k in diffs)
    normality_hint = {
        'empirical_rule_ok': within_tolerance,
        'shapiro_pass': (sw_p is not None and sw_p > 0.05),
        'anderson_stat': ad_result.statistic  # lower is better
    }
    report['hint'] = normality_hint

    if plot:
        # histogram + normal pdf
        plt.figure(figsize=(10,4))
        plt.subplot(1,2,1)
        count, bins, _ = plt.hist(x, bins='auto', density=True, alpha=0.6)
        # plot normal pdf
        xs = np.linspace(x.min(), x.max(), 200)
        plt.plot(xs, stats.norm.pdf(xs, mu, sigma), linewidth=2)
        plt.title('Histogram & fitted normal')

        # Q-Q plot
        plt.subplot(1,2,2)
        stats.probplot(x, dist="norm", sparams=(mu, sigma), plot=plt)
        plt.title('Q-Q plot')
        plt.tight_layout()
        plt.show()

    return report

# ä½¿ç”¨ç¤ºä¾‹:
# data = pd.Series(...) æˆ– numpy array
# r = check_sigma_distribution(data)
# print(r)
```

------

## 6) å®ç”¨å»ºè®® / å†³ç­–æµç¨‹

1. å…ˆç”¨ç›´æ–¹å›¾ä¸ Qâ€“Q å›¾è§‚å¯Ÿæ€»ä½“å½¢çŠ¶ã€‚
2. è®¡ç®— Â±1/2/3Ïƒ çš„ç»éªŒæ¯”ä¾‹ï¼Œçœ‹æ˜¯å¦åç¦»å¤ªå¤šã€‚
3. æ ¹æ®æ ·æœ¬å¤§å°ç”¨ Shapiro æˆ– AD æ£€éªŒï¼ˆå¹¶è§£è¯» p å€¼å’Œç»Ÿè®¡é‡ï¼‰ã€‚
4. å¦‚æœè½»å¾®åå·®ä½†ä¸šåŠ¡ä¸æ•æ„Ÿ â€”â€” å¯ä»¥å½“ä½œâ€œè¿‘ä¼¼æ­£æ€â€ï¼›è‹¥ç²¾ç¡®åˆ†å¸ƒå¾ˆå…³é”®ï¼ˆä¾‹å¦‚ç»Ÿè®¡æ¨æ–­å‡è®¾ï¼‰ï¼Œåˆ™ä¸è¦å°†å…¶è§†ä¸ºæ­£æ€ï¼Œè€ƒè™‘éå‚æ•°æ–¹æ³•æˆ–å˜æ¢ï¼ˆå¯¹æ•°ã€Boxâ€“Coxï¼‰ã€‚
5. å¯¹äºæ˜æ˜¾åæ–œæˆ–åšå°¾ï¼šè€ƒè™‘å»æå€¼ã€å¯¹æ•°/æ ¹å·å˜æ¢æˆ–ä½¿ç”¨æ›´ç¨³å¥çš„åˆ†å¸ƒå»ºæ¨¡ï¼ˆå¦‚ t åˆ†å¸ƒã€å¯¹æ•°æ­£æ€ç­‰ï¼‰ã€‚

------

## 7) æ³¨æ„äº‹é¡¹

- **æ ·æœ¬é‡å½±å“æ£€éªŒ**ï¼šå°æ ·æœ¬æ£€éªŒåŠ›ä½ï¼ˆéš¾ä»¥å‘ç°åå·®ï¼‰ï¼›å¤§æ ·æœ¬ä¼šæŠŠå¾®å°åå·®æ£€æµ‹ä¸ºæ˜¾è‘—ã€‚å§‹ç»ˆç»“åˆå›¾å½¢åˆ¤æ–­ã€‚
- **ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾**ï¼šÏƒ è§„åˆ™åŸºäºç‹¬ç«‹æ ·æœ¬ï¼Œè‹¥æ•°æ®æœ‰æ—¶é—´åºåˆ—ç›¸å…³æ€§ï¼ˆè‡ªç›¸å…³ï¼‰æˆ–åˆ†ç»„å·®å¼‚ï¼Œéœ€å…ˆå¤„ç†ä¾èµ–æ€§/åˆ†å±‚ã€‚
- **ç¼ºå¤±å€¼/æç«¯å€¼**ï¼šå…ˆå®¡è§†å¹¶å†³å®šæ˜¯å¦æ¸…æ´—æˆ–ç‰¹åˆ«æ ‡æ³¨ã€‚

------

æ˜ç™½âœ…

å¦‚æœä½ æƒ³**ç”¨ä¸€ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥åˆ¤æ–­æ•°æ®æ˜¯å¦è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ**ï¼Œæœ€å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨ **Shapiroâ€“Wilk æ£€éªŒçš„ p å€¼**ï¼Œå› ä¸ºå®ƒä¸“é—¨é’ˆå¯¹æ­£æ€æ€§ã€‚

è§„åˆ™å¾ˆç®€å•ï¼š

- **p > 0.05** â†’ ä¸èƒ½æ‹’ç»æ­£æ€æ€§ï¼ˆæ•°æ®çœ‹èµ·æ¥ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼‰
- **p â‰¤ 0.05** â†’ æ‹’ç»æ­£æ€æ€§ï¼ˆæ•°æ®æ˜¾è‘—åç¦»æ­£æ€åˆ†å¸ƒï¼‰

------

### Python ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def is_normal(data, alpha=0.05):
    """
    ä½¿ç”¨ Shapiro-Wilk æ£€éªŒåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    :return: (æ˜¯å¦æ­£æ€, på€¼)
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    
    stat, p = stats.shapiro(data)
    return (p > alpha, p)

# ç¤ºä¾‹
data1 = np.random.normal(0, 1, 500)  # æ­£æ€åˆ†å¸ƒ
data2 = np.random.exponential(1, 500)  # éæ­£æ€åˆ†å¸ƒ

print("data1:", is_normal(data1))
print("data2:", is_normal(data2))
```

------

### è¾“å‡ºç¤ºä¾‹

```
data1: (True, 0.31)   # p=0.31 > 0.05 â†’ ç¬¦åˆæ­£æ€åˆ†å¸ƒ
data2: (False, 1.2e-12)  # pâ‰ˆ0 â†’ æ˜¾è‘—ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒ
```

------

å¥½çš„âœ…
 æˆ‘ç»™ä½ å†™ä¸€ä¸ª**é€šç”¨å‡½æ•°**ï¼Œå¯ä»¥é€šè¿‡å‚æ•°é€‰æ‹©ä¸åŒçš„åˆ¤æ–­æŒ‡æ ‡ï¼Œè¿”å› **True/False** æ˜¯å¦ä¸ºæ­£æ€åˆ†å¸ƒï¼Œå¹¶é™„å¸¦ p å€¼æˆ–ç»Ÿè®¡é‡ï¼Œæ–¹ä¾¿å‚è€ƒã€‚

------

## ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def is_normal(data, method="shapiro", alpha=0.05):
    """
    åˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    
    :param data: list æˆ– numpy array
    :param method: æ£€éªŒæ–¹æ³•ï¼Œå¯é€‰ï¼š
                   - "shapiro" (Shapiro-Wilk)
                   - "anderson" (Anderson-Darling)
                   - "kstest" (Kolmogorov-Smirnov, ä¸æ‹Ÿåˆçš„æ­£æ€åˆ†å¸ƒæ¯”è¾ƒ)
                   - "skewkurtosis" (åŸºäºååº¦+å³°åº¦çš„Zæ£€éªŒ)
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤ 0.05
    :return: (æ˜¯å¦æ­£æ€, ç»Ÿè®¡ä¿¡æ¯)
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)

    if method == "shapiro":
        stat, p = stats.shapiro(data)
        return (p > alpha, {"method": "Shapiro-Wilk", "stat": stat, "p": p})

    elif method == "anderson":
        result = stats.anderson(data, dist="norm")
        # å¦‚æœç»Ÿè®¡é‡ < ä¸´ç•Œå€¼ï¼Œåˆ™è®¤ä¸ºè¿‘ä¼¼æ­£æ€
        is_norm = result.statistic < result.critical_values[list(result.significance_level).index(int(alpha*100))]
        return (is_norm, {"method": "Anderson-Darling", "stat": result.statistic,
                          "critical_values": result.critical_values,
                          "significance_level": result.significance_level})

    elif method == "kstest":
        mu, sigma = np.mean(data), np.std(data, ddof=1)
        stat, p = stats.kstest(data, "norm", args=(mu, sigma))
        return (p > alpha, {"method": "Kolmogorov-Smirnov", "stat": stat, "p": p})

    elif method == "skewkurtosis":
        # ååº¦ä¸å³°åº¦çš„Zæ£€éªŒ
        skew, skew_p = stats.skewtest(data)
        kurt, kurt_p = stats.kurtosistest(data)
        is_norm = (skew_p > alpha) and (kurt_p > alpha)
        return (is_norm, {"method": "Skewness-Kurtosis",
                          "skew_stat": skew, "skew_p": skew_p,
                          "kurt_stat": kurt, "kurt_p": kurt_p})

    else:
        raise ValueError("method å‚æ•°å¿…é¡»æ˜¯ 'shapiro'ã€'anderson'ã€'kstest' æˆ– 'skewkurtosis'")

# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)
    data_non_norm = np.random.exponential(1, 500)

    for method in ["shapiro", "anderson", "kstest", "skewkurtosis"]:
        print(f"æ–¹æ³•: {method}")
        print("æ­£æ€æ•°æ®:", is_normal(data_norm, method))
        print("éæ­£æ€æ•°æ®:", is_normal(data_non_norm, method))
        print("------")
```

------

## è¯´æ˜

- `shapiro`ï¼šé€‚åˆå°ä¸­æ ·æœ¬ï¼Œå¸¸ç”¨é»˜è®¤æ–¹æ³•ã€‚
- `anderson`ï¼šè¿”å›å¤šä¸ªæ˜¾è‘—æ€§æ°´å¹³ä¸‹çš„åˆ¤æ–­ï¼Œæ›´ä¸¥æ ¼ã€‚
- `kstest`ï¼šåŸºäºæ ·æœ¬å‡å€¼å’Œæ–¹å·®æ‹Ÿåˆæ­£æ€åˆ†å¸ƒï¼Œå†åš K-S æ£€éªŒã€‚
- `skewkurtosis`ï¼šååº¦+å³°åº¦è”åˆæ£€éªŒï¼Œå¿«é€Ÿç›´è§‚ã€‚

------

å¥½çš„âœ… æˆ‘å¸®ä½ å°è£…ä¸€ä¸ª **ç»¼åˆåˆ¤æ–­å‡½æ•°**ï¼Œå®ƒä¼šè‡ªåŠ¨è·‘ **Shapiro-Wilkã€Anderson-Darlingã€K-S æ£€éªŒã€ååº¦-å³°åº¦æ£€éªŒ** å››ç§æ–¹æ³•ï¼Œç„¶åç»™å‡ºæ¯ä¸ªæ–¹æ³•çš„ç»“æœï¼Œå¹¶æ±‡æ€»ä¸€ä¸ªâ€œå¤šæ•°æŠ•ç¥¨â€çš„ç»¼åˆç»“è®ºã€‚

------

## ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def check_normality(data, alpha=0.05):
    """
    ç»¼åˆåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤ 0.05
    :return: dictï¼ŒåŒ…å«å„æ–¹æ³•ç»“æœ + ç»¼åˆç»“è®º
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)
    
    results = {}

    # 1. Shapiro-Wilk
    if 3 <= n <= 5000:  # è¶…è¿‡5000ä¼šæŠ¥é”™
        stat, p = stats.shapiro(data)
        results["shapiro"] = {"is_normal": p > alpha, "stat": stat, "p": p}
    else:
        results["shapiro"] = {"is_normal": None, "stat": None, "p": None, "note": "æ ·æœ¬é‡è¶…å‡ºShapiroé€‚ç”¨èŒƒå›´"}

    # 2. Anderson-Darling
    ad_result = stats.anderson(data, dist="norm")
    crit_dict = dict(zip(ad_result.significance_level, ad_result.critical_values))
    crit_value = crit_dict.get(int(alpha * 100), ad_result.critical_values[2])  # æ‰¾æœ€è¿‘çš„æ°´å¹³
    is_norm_ad = ad_result.statistic < crit_value
    results["anderson"] = {
        "is_normal": is_norm_ad,
        "stat": ad_result.statistic,
        "critical_value": crit_value
    }

    # 3. Kolmogorov-Smirnov
    mu, sigma = np.mean(data), np.std(data, ddof=1)
    stat, p = stats.kstest(data, "norm", args=(mu, sigma))
    results["kstest"] = {"is_normal": p > alpha, "stat": stat, "p": p}

    # 4. Skewness & Kurtosis æ£€éªŒ
    skew_stat, skew_p = stats.skewtest(data)
    kurt_stat, kurt_p = stats.kurtosistest(data)
    is_norm_sk = (skew_p > alpha) and (kurt_p > alpha)
    results["skewkurtosis"] = {
        "is_normal": is_norm_sk,
        "skew_stat": skew_stat, "skew_p": skew_p,
        "kurt_stat": kurt_stat, "kurt_p": kurt_p
    }

    # ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨æ³•ï¼Œæ’é™¤ Noneï¼‰
    votes = [res["is_normal"] for res in results.values() if res["is_normal"] is not None]
    if len(votes) > 0:
        final_decision = votes.count(True) >= votes.count(False)
    else:
        final_decision = None

    return {
        "methods": results,
        "final_decision": final_decision
    }


# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)
    data_non_norm = np.random.exponential(1, 500)

    print("æ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_norm))

    print("\néæ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_non_norm))
```

------

## è¾“å‡ºç¤ºä¾‹ï¼ˆç®€åŒ–ï¼‰

```python
æ­£æ€æ•°æ®ç»“æœ:
{
  'methods': {
    'shapiro': {'is_normal': True, 'stat': 0.998, 'p': 0.64},
    'anderson': {'is_normal': True, 'stat': 0.29, 'critical_value': 0.74},
    'kstest': {'is_normal': True, 'stat': 0.03, 'p': 0.56},
    'skewkurtosis': {'is_normal': True, 'skew_stat': 0.12, 'skew_p': 0.90, 'kurt_stat': 0.33, 'kurt_p': 0.74}
  },
  'final_decision': True
}

éæ­£æ€æ•°æ®ç»“æœ:
{
  'methods': {
    'shapiro': {'is_normal': False, 'stat': 0.81, 'p': 1.2e-12},
    'anderson': {'is_normal': False, 'stat': 7.25, 'critical_value': 0.74},
    'kstest': {'is_normal': False, 'stat': 0.28, 'p': 1.3e-15},
    'skewkurtosis': {'is_normal': False, 'skew_stat': 12.1, 'skew_p': 0.0, 'kurt_stat': 8.3, 'kurt_p': 0.0}
  },
  'final_decision': False
}
```

------

è¿™æ ·ä½ å°±èƒ½ä¸€è¡Œè°ƒç”¨ï¼Œè‡ªåŠ¨è·‘ 4 ç§æ–¹æ³•ï¼Œæœ€åçœ‹ `final_decision` å°±è¡Œã€‚

ğŸ‘Œ é‚£æˆ‘å¸®ä½ æŠŠ**å¯è§†åŒ–**æ•´åˆè¿›å»ï¼šåœ¨ç»¼åˆæ£€éªŒæ—¶ï¼Œå¯ä»¥ç”»å‡º

1. **ç›´æ–¹å›¾ + æ‹Ÿåˆæ­£æ€æ›²çº¿**
2. **Qâ€“Q å›¾**ï¼ˆåˆ†ä½æ•°-åˆ†ä½æ•°å›¾ï¼‰

è¿™æ ·ä½ æ—¢æœ‰æ•°å€¼åˆ¤æ–­ï¼ˆå››ç§æ–¹æ³• + æŠ•ç¥¨ç»“è®ºï¼‰ï¼Œåˆæœ‰å›¾å½¢ç›´è§‚éªŒè¯ã€‚

------

## å®Œæ•´ä»£ç ï¼ˆå¸¦å¯è§†åŒ–ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def check_normality(data, alpha=0.05, plot=True):
    """
    ç»¼åˆåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œå¹¶å¯è§†åŒ–
    
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³
    :param plot: æ˜¯å¦ç»˜åˆ¶å›¾å½¢
    :return: dictï¼ŒåŒ…å«å„æ–¹æ³•ç»“æœ + ç»¼åˆç»“è®º
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)
    
    results = {}

    # 1. Shapiro-Wilk
    if 3 <= n <= 5000:  # è¶…è¿‡5000ä¼šæŠ¥é”™
        stat, p = stats.shapiro(data)
        results["shapiro"] = {"is_normal": p > alpha, "stat": stat, "p": p}
    else:
        results["shapiro"] = {"is_normal": None, "note": "æ ·æœ¬é‡è¶…å‡ºShapiroé€‚ç”¨èŒƒå›´"}

    # 2. Anderson-Darling
    ad_result = stats.anderson(data, dist="norm")
    crit_dict = dict(zip(ad_result.significance_level, ad_result.critical_values))
    crit_value = crit_dict.get(int(alpha * 100), ad_result.critical_values[2])  # æ‰¾æœ€è¿‘æ°´å¹³
    is_norm_ad = ad_result.statistic < crit_value
    results["anderson"] = {
        "is_normal": is_norm_ad,
        "stat": ad_result.statistic,
        "critical_value": crit_value
    }

    # 3. Kolmogorov-Smirnov
    mu, sigma = np.mean(data), np.std(data, ddof=1)
    stat, p = stats.kstest(data, "norm", args=(mu, sigma))
    results["kstest"] = {"is_normal": p > alpha, "stat": stat, "p": p}

    # 4. Skewness & Kurtosis
    skew_stat, skew_p = stats.skewtest(data)
    kurt_stat, kurt_p = stats.kurtosistest(data)
    is_norm_sk = (skew_p > alpha) and (kurt_p > alpha)
    results["skewkurtosis"] = {
        "is_normal": is_norm_sk,
        "skew_stat": skew_stat, "skew_p": skew_p,
        "kurt_stat": kurt_stat, "kurt_p": kurt_p
    }

    # ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨æ³•ï¼Œæ’é™¤ Noneï¼‰
    votes = [res["is_normal"] for res in results.values() if res["is_normal"] is not None]
    if len(votes) > 0:
        final_decision = votes.count(True) >= votes.count(False)
    else:
        final_decision = None

    report = {"methods": results, "final_decision": final_decision}

    # ---------- å¯è§†åŒ– ----------
    if plot:
        fig, axes = plt.subplots(1, 2, figsize=(10, 4))

        # ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿
        axes[0].hist(data, bins="auto", density=True, alpha=0.6, color="skyblue", edgecolor="black")
        xs = np.linspace(data.min(), data.max(), 200)
        axes[0].plot(xs, stats.norm.pdf(xs, mu, sigma), 'r-', lw=2)
        axes[0].set_title("ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿")

        # Q-Q å›¾
        stats.probplot(data, dist="norm", plot=axes[1])
        axes[1].set_title("Q-Q å›¾")

        plt.tight_layout()
        plt.show()

    return report


# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)   # æ­£æ€æ•°æ®
    data_non_norm = np.random.exponential(1, 500)  # éæ­£æ€æ•°æ®

    print("æ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_norm, plot=True))

    print("\néæ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_non_norm, plot=True))
```

------

## ä½¿ç”¨æ•ˆæœ

- **æ‰“å°è¾“å‡º**ï¼šæ¯ä¸ªæ–¹æ³•çš„ç»“æœï¼ˆæ˜¯å¦æ­£æ€ã€ç»Ÿè®¡é‡ã€p å€¼ç­‰ï¼‰+ ç»¼åˆç»“è®º `final_decision`ã€‚
- **ç»˜å›¾**ï¼š
  - å·¦è¾¹ï¼šæ•°æ®ç›´æ–¹å›¾ + æ‹Ÿåˆçš„æ­£æ€æ›²çº¿
  - å³è¾¹ï¼šQ-Q å›¾ï¼ˆç‚¹è¶Šè´´è¿‘å¯¹è§’çº¿ï¼Œè¶Šç¬¦åˆæ­£æ€ï¼‰

------

ğŸ‘Œ é‚£æˆ‘åœ¨ä¸Šé¢çš„ç»¼åˆå‡½æ•°é‡ŒåŠ ä¸Š **ç»éªŒ 68â€“95â€“99.7% æ£€æŸ¥**ï¼Œä¹Ÿå°±æ˜¯ç»Ÿè®¡æ•°æ®è½åœ¨ `Î¼Â±1Ïƒã€Î¼Â±2Ïƒã€Î¼Â±3Ïƒ` åŒºé—´çš„æ¯”ä¾‹ï¼Œå¹¶å’Œç†è®ºå€¼åšå¯¹æ¯”ã€‚

è¿™æ ·ä½ å°±èƒ½åŒæ—¶çœ‹åˆ°ï¼š

- å››ç§ç»Ÿè®¡æ£€éªŒç»“æœ
- ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨ï¼‰
- Sigma Ruleï¼ˆç»éªŒæ³•åˆ™ï¼‰çš„æ¯”ä¾‹æƒ…å†µ
- å¯è§†åŒ–ç»“æœ

------

## å®Œæ•´ä»£ç ï¼ˆå¸¦ Sigma Rule æ£€æŸ¥ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def check_normality(data, alpha=0.05, plot=True):
    """
    ç»¼åˆåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼ˆå¤šç§æ–¹æ³• + Sigma Rule + å¯è§†åŒ–ï¼‰
    
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³
    :param plot: æ˜¯å¦ç»˜åˆ¶å›¾å½¢
    :return: dictï¼ŒåŒ…å«å„æ–¹æ³•ç»“æœ + Sigma Rule + ç»¼åˆç»“è®º
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)
    
    results = {}

    # 1. Shapiro-Wilk
    if 3 <= n <= 5000:  # è¶…è¿‡5000ä¼šæŠ¥é”™
        stat, p = stats.shapiro(data)
        results["shapiro"] = {"is_normal": p > alpha, "stat": stat, "p": p}
    else:
        results["shapiro"] = {"is_normal": None, "note": "æ ·æœ¬é‡è¶…å‡ºShapiroé€‚ç”¨èŒƒå›´"}

    # 2. Anderson-Darling
    ad_result = stats.anderson(data, dist="norm")
    crit_dict = dict(zip(ad_result.significance_level, ad_result.critical_values))
    crit_value = crit_dict.get(int(alpha * 100), ad_result.critical_values[2])  # æ‰¾æœ€è¿‘æ°´å¹³
    is_norm_ad = ad_result.statistic < crit_value
    results["anderson"] = {
        "is_normal": is_norm_ad,
        "stat": ad_result.statistic,
        "critical_value": crit_value
    }

    # 3. Kolmogorov-Smirnov
    mu, sigma = np.mean(data), np.std(data, ddof=1)
    stat, p = stats.kstest(data, "norm", args=(mu, sigma))
    results["kstest"] = {"is_normal": p > alpha, "stat": stat, "p": p}

    # 4. Skewness & Kurtosis
    skew_stat, skew_p = stats.skewtest(data)
    kurt_stat, kurt_p = stats.kurtosistest(data)
    is_norm_sk = (skew_p > alpha) and (kurt_p > alpha)
    results["skewkurtosis"] = {
        "is_normal": is_norm_sk,
        "skew_stat": skew_stat, "skew_p": skew_p,
        "kurt_stat": kurt_stat, "kurt_p": kurt_p
    }

    # 5. Sigma Rule æ£€æŸ¥
    within_1 = np.mean((data >= mu - sigma) & (data <= mu + sigma)) * 100
    within_2 = np.mean((data >= mu - 2*sigma) & (data <= mu + 2*sigma)) * 100
    within_3 = np.mean((data >= mu - 3*sigma) & (data <= mu + 3*sigma)) * 100

    sigma_rule = {
        "empirical": {"Â±1Ïƒ": within_1, "Â±2Ïƒ": within_2, "Â±3Ïƒ": within_3},
        "theoretical": {"Â±1Ïƒ": 68.27, "Â±2Ïƒ": 95.45, "Â±3Ïƒ": 99.73}
    }

    # ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨æ³•ï¼Œæ’é™¤ Noneï¼‰
    votes = [res["is_normal"] for res in results.values() if res["is_normal"] is not None]
    if len(votes) > 0:
        final_decision = votes.count(True) >= votes.count(False)
    else:
        final_decision = None

    report = {"methods": results, "sigma_rule": sigma_rule, "final_decision": final_decision}

    # ---------- å¯è§†åŒ– ----------
    if plot:
        fig, axes = plt.subplots(1, 2, figsize=(10, 4))

        # ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿
        axes[0].hist(data, bins="auto", density=True, alpha=0.6, color="skyblue", edgecolor="black")
        xs = np.linspace(data.min(), data.max(), 200)
        axes[0].plot(xs, stats.norm.pdf(xs, mu, sigma), 'r-', lw=2)
        axes[0].set_title("ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿")

        # Q-Q å›¾
        stats.probplot(data, dist="norm", plot=axes[1])
        axes[1].set_title("Q-Q å›¾")

        plt.tight_layout()
        plt.show()

    return report


# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)   # æ­£æ€æ•°æ®
    data_non_norm = np.random.exponential(1, 500)  # éæ­£æ€æ•°æ®

    print("æ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_norm, plot=True))

    print("\néæ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_non_norm, plot=True))
```

------

## è¾“å‡ºç¤ºä¾‹ï¼ˆéƒ¨åˆ†ï¼‰

```python
"final_decision": True,
"sigma_rule": {
  "empirical": {"Â±1Ïƒ": 67.8, "Â±2Ïƒ": 95.0, "Â±3Ïƒ": 99.4},
  "theoretical": {"Â±1Ïƒ": 68.27, "Â±2Ïƒ": 95.45, "Â±3Ïƒ": 99.73}
}
```

è¿™æ ·å°±èƒ½çœ‹åˆ°ç»éªŒæ¯”ä¾‹å’Œç†è®ºå€¼çš„å¯¹æ¯”ã€‚

------

