---
layout:     post
title:      "dnn"
subtitle:   ""
date:       2025-05-13
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - torch
---

æˆ‘ä»¬é€æ­¥è®²è§£ DNNï¼ˆDeep Neural Networkï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼‰æ¨¡å‹çš„å®ç°ã€åŸç†ä»¥åŠè®¾è®¡æ–¹æ³•ï¼Œå¹¶é™„ä¸€ä¸ªç®€å•çš„å®ç°ç¤ºä¾‹ã€‚

------

## ä¸€ã€ä»€ä¹ˆæ˜¯ DNNï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰ï¼Ÿ

DNN æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼‰ï¼Œç”±å¤šä¸ª**éšè—å±‚**ç»„æˆï¼Œæ¯ä¸€å±‚éƒ½åŒ…å«å¤šä¸ª**ç¥ç»å…ƒï¼ˆç¥ç»èŠ‚ç‚¹ï¼‰**ã€‚å¸¸ç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®ã€å›¾åƒã€è¯­éŸ³ã€è‡ªç„¶è¯­è¨€ç­‰ä»»åŠ¡ã€‚

### åŸºæœ¬ç»“æ„ï¼š

- è¾“å…¥å±‚ï¼šæ¥æ”¶åŸå§‹æ•°æ®ã€‚
- éšè—å±‚ï¼šé€šè¿‡éçº¿æ€§å˜æ¢æå–ç‰¹å¾ã€‚
- è¾“å‡ºå±‚ï¼šè¾“å‡ºç»“æœï¼Œå¦‚åˆ†ç±»æ¦‚ç‡ã€æ•°å€¼é¢„æµ‹ç­‰ã€‚

------

## äºŒã€DNN ä¸ºä»€ä¹ˆå¯ä»¥å®ç°é¢„æµ‹ï¼Ÿ

DNN æ¨¡å‹æœ¬è´¨æ˜¯ä¸€ä¸ª**å¤åˆå‡½æ•°**ï¼Œå®ƒé€šè¿‡å åŠ å¤šä¸ªçº¿æ€§å˜æ¢å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæ¨¡æ‹Ÿå¤æ‚çš„è¾“å…¥è¾“å‡ºæ˜ å°„å…³ç³»ã€‚

ä¾‹å¦‚ï¼Œä¸‰å±‚ç½‘ç»œç»“æ„å¦‚ä¸‹ï¼š

```
Output = f3(W3 Â· f2(W2 Â· f1(W1 Â· X + b1) + b2) + b3)
```

- `W` è¡¨ç¤ºæƒé‡çŸ©é˜µï¼Œ`b` è¡¨ç¤ºåç½®ï¼Œ`f` æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰ã€‚
- æ¯ä¸€å±‚å­¦åˆ°çš„æƒé‡ç»„åˆå’Œéçº¿æ€§å¯ä»¥é€¼è¿‘ä»»æ„å¤æ‚å‡½æ•°ï¼ˆå³**é€šç”¨é€¼è¿‘å®šç†**ï¼‰ã€‚

------

## ä¸‰ã€DNN å¦‚ä½•å®ç°ï¼šä»£ç ç¤ºä¾‹ï¼ˆPyTorchï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# å®šä¹‰ä¸€ä¸ªç®€å•çš„ DNN æ¨¡å‹
class SimpleDNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(SimpleDNN, self).__init__()
        layers = []

        last_size = input_size
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(last_size, hidden_size))
            layers.append(nn.ReLU())  # éçº¿æ€§æ¿€æ´»å‡½æ•°
            last_size = hidden_size

        layers.append(nn.Linear(last_size, output_size))  # è¾“å‡ºå±‚

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# æ¨¡å‹å®ä¾‹åŒ–
model = SimpleDNN(input_size=10, hidden_sizes=[64, 32], output_size=1)

# ç¤ºä¾‹è¾“å…¥
x = torch.randn(5, 10)  # 5ä¸ªæ ·æœ¬ï¼Œ10ç»´ç‰¹å¾
output = model(x)
print(output)
```

------

## å››ã€éšè—å±‚æ€ä¹ˆè®¾è®¡ï¼Ÿæœ‰ä»€ä¹ˆé€šç”¨è§„åˆ™ï¼Ÿ

è™½ç„¶æ²¡æœ‰**ç»å¯¹å›ºå®š**çš„è®¾è®¡æ–¹å¼ï¼Œä½†ä»¥ä¸‹æ˜¯ç»éªŒæ³•åˆ™ï¼š

### 1. å±‚æ•°ï¼ˆDepthï¼‰ï¼š

- **æµ…å±‚ï¼ˆ1-2å±‚ï¼‰**ï¼šé€‚åˆç®€å•ä»»åŠ¡æˆ–å°æ•°æ®é›†ã€‚
- **æ·±å±‚ï¼ˆ>3å±‚ï¼‰**ï¼šé€‚åˆå¤æ‚ä»»åŠ¡ï¼Œå¦‚å›¾åƒã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚

### 2. æ¯å±‚ç¥ç»å…ƒæ•°é‡ï¼ˆWidthï¼‰ï¼š

- é€šå¸¸ä»è¾“å…¥å¤§å°å‘ä¸­é—´å‹ç¼©æˆ–æ‰©å±•ï¼Œç„¶åå†åˆ°è¾“å‡ºå¤§å°æ”¶ç¼©ã€‚
- å¯å°è¯•ï¼š
  - é€’å‡ï¼šå¦‚ `[128, 64, 32]`
  - æ’å®šï¼šå¦‚ `[64, 64, 64]`

### 3. æ¿€æ´»å‡½æ•°ï¼š

- æ¨èä½¿ç”¨ `ReLU`ã€`LeakyReLU`ã€æˆ– `GELU`ï¼Œæ¯” `Sigmoid`/`Tanh` æ›´æ˜“è®­ç»ƒã€‚

### 4. é˜²æ­¢è¿‡æ‹Ÿåˆï¼š

- åŠ å…¥ **Dropout** å±‚ã€‚
- ä½¿ç”¨ **BatchNorm** å±‚ã€‚
- è®­ç»ƒæ—¶ä½¿ç”¨æ­£åˆ™åŒ–ã€æ—©åœç­‰æŠ€å·§ã€‚

------

## äº”ã€å¤æ‚ DNN çš„è®¾è®¡å»ºè®®

1. **é€æ­¥å¢åŠ å±‚æ•°å’Œå®½åº¦**ï¼Œè§‚å¯ŸéªŒè¯é›†æ€§èƒ½å˜åŒ–ï¼›
2. **å­¦ä¹ æ›²çº¿åˆ†æ**ï¼šè®­ç»ƒè¯¯å·®ä½ä½†éªŒè¯è¯¯å·®é«˜ â†’ æ¨¡å‹è¿‡æ‹Ÿåˆï¼›
3. **ä½¿ç”¨è‡ªåŠ¨è°ƒå‚**ï¼ˆå¦‚ Optunaã€Ray Tuneï¼‰ï¼›
4. **å¯è§†åŒ–ç½‘ç»œç»“æ„**å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œä¾¿äºè°ƒè¯•ï¼›
5. **åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆåœ¨å›¾åƒã€æ–‡æœ¬é¢†åŸŸå°¤ä¸ºæœ‰æ•ˆï¼‰ï¼›

------

## æ€»ç»“ï¼šDNN æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

1. å¤šå±‚ç¥ç»å…ƒå åŠ æ„æˆä¸€ä¸ªå¤æ‚çš„å‡½æ•°ï¼›
2. ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è®­ç»ƒæƒé‡ï¼›
3. æ¯å±‚é€šè¿‡çº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•°é€æ­¥æå–é«˜é˜¶ç‰¹å¾ï¼›
4. è¶Šå¤æ‚çš„é—®é¢˜ï¼Œéœ€è¦è¶Šæ·±ã€è¶Šå¹¿çš„ç½‘ç»œç»“æ„ã€‚

------

éœ€è¦æˆ‘ä¸ºä½ è®¾è®¡ä¸€ä¸ªé€‚ç”¨äºæŸä¸ªå…·ä½“ä»»åŠ¡çš„ DNN æ¨¡å‹ç»“æ„å—ï¼Ÿæ¯”å¦‚é¢„æµ‹æˆ¿ä»·ã€å›¾åƒåˆ†ç±»æˆ–æ–‡æœ¬æƒ…æ„Ÿåˆ†æç­‰ï¼Ÿ

å¥½çš„ï¼è¯·ä½ å…ˆå‘Šè¯‰æˆ‘ä»¥ä¸‹ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘æ‰èƒ½ä¸ºä½ è®¾è®¡ä¸€ä¸ªåˆé€‚çš„ DNN æ¨¡å‹ç»“æ„ï¼š

------

### â“1. **ä»»åŠ¡ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ**

- å›å½’ï¼ˆé¢„æµ‹æˆ¿ä»·ã€è‚¡ç¥¨ä»·æ ¼ï¼‰
- åˆ†ç±»ï¼ˆäºŒåˆ†ç±»/å¤šåˆ†ç±»ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€å›¾åƒè¯†åˆ«ï¼‰
- å…¶ä»–ï¼ˆå¦‚å¤šæ ‡ç­¾åˆ†ç±»ã€æ—¶é—´åºåˆ—é¢„æµ‹ï¼‰

### â“2. **è¾“å…¥æ•°æ®æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ**

- æœ‰å¤šå°‘ä¸ªç‰¹å¾ï¼ˆç»´åº¦ï¼‰ï¼Ÿ
- æ˜¯ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ã€å›¾åƒã€æ–‡æœ¬ï¼Œè¿˜æ˜¯å…¶ä»–å½¢å¼ï¼Ÿ

### â“3. **é¢„æœŸè¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ**

- ä¸€ä¸ªæ•°å€¼ï¼ˆå›å½’ï¼‰
- ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ï¼ˆåˆ†ç±»ï¼‰
- å¤šä¸ªæ¦‚ç‡ï¼ˆå¤šæ ‡ç­¾ï¼‰

------

ä¾‹å¦‚ï¼Œå¦‚æœä½ è¦åšâ€œæˆ¿ä»·é¢„æµ‹â€ï¼Œä½ å¯ä»¥å‘Šè¯‰æˆ‘ï¼š

- ä»»åŠ¡ï¼šå›å½’
- è¾“å…¥ï¼šç»“æ„åŒ–æ•°æ®ï¼Œæœ‰ 15 ä¸ªç‰¹å¾ï¼ˆå¦‚é¢ç§¯ã€ä½ç½®ã€æˆ¿é—´æ•°ç­‰ï¼‰
- è¾“å‡ºï¼šä¸€ä¸ªè¿ç»­çš„æˆ¿ä»·æ•°å€¼

åªè¦ä½ æä¾›è¿™äº›åŸºæœ¬ä¿¡æ¯ï¼Œæˆ‘å°±å¯ä»¥ç»™å‡º**æ¨¡å‹ç»“æ„ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€è®­ç»ƒå»ºè®®å’Œä»£ç å®ç°**ã€‚ä½ ç°åœ¨æ‰“ç®—å®ç°å“ªä¸€ç±»ä»»åŠ¡ï¼Ÿ

å¾ˆå¥½ï¼ä½ çš„ä»»åŠ¡æ˜¯ä¸€ä¸ª**å›å½’é—®é¢˜**ï¼Œè¾“å…¥æ˜¯**ç»“æ„åŒ–è¡¨æ ¼æ•°æ®**ï¼Œç›®æ ‡æ˜¯é¢„æµ‹**ä¸€ä¸ªæ•°å€¼**ï¼ˆå…¸å‹çš„å¦‚æˆ¿ä»·é¢„æµ‹ã€é”€å”®é¢é¢„æµ‹ç­‰ï¼‰ã€‚

------

## âœ… 1. ç½‘ç»œè®¾è®¡åŸåˆ™ï¼ˆé€‚ç”¨äºç»“æ„åŒ–æ•°æ®å›å½’ï¼‰

- **è¾“å…¥å±‚**ï¼šç‰¹å¾æ•°é‡ = ä½ çš„æ•°æ®ä¸­æ¯ä¸ªæ ·æœ¬çš„åˆ—æ•°ï¼ˆæ¯”å¦‚ 10 ä¸ªå­—æ®µå°±è¾“å…¥ 10ï¼‰ã€‚
- **éšè—å±‚**ï¼š
  - 2ï½4 å±‚é€šå¸¸å°±è¶³å¤Ÿã€‚
  - ç¥ç»å…ƒä¸ªæ•°å¯ä» 64 å¼€å§‹é€å±‚å‡å°‘ï¼ˆä¾‹å¦‚ 64 â†’ 32 â†’ 16ï¼‰ã€‚
- **æ¿€æ´»å‡½æ•°**ï¼š`ReLU`ï¼ˆå¿«é€Ÿæ”¶æ•›ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚
- **è¾“å‡ºå±‚**ï¼š
  - åªæœ‰ **1 ä¸ªç¥ç»å…ƒ**ï¼Œå› ä¸ºä½ åªé¢„æµ‹ä¸€ä¸ªæ•°å€¼ã€‚
  - **æ— æ¿€æ´»å‡½æ•°**ï¼ˆå³çº¿æ€§è¾“å‡ºï¼‰ç”¨äºå›å½’ã€‚
- **æŸå¤±å‡½æ•°**ï¼š`MSELoss`ï¼ˆå‡æ–¹è¯¯å·®ï¼‰ã€‚

------

## âœ… 2. PyTorch å®ç°ä»£ç ï¼ˆé€‚åˆå…¥é—¨å›å½’ä»»åŠ¡ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim

# å‡è®¾ä½ çš„è¾“å…¥ç‰¹å¾æœ‰ 10 ä¸ª
class DNNRegressor(nn.Module):
    def __init__(self, input_size):
        super(DNNRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)  # å›å½’è¾“å‡ºå±‚ï¼Œæ— æ¿€æ´»
        )

    def forward(self, x):
        return self.model(x)

# æ¨¡æ‹Ÿè¾“å…¥
input_size = 10
model = DNNRegressor(input_size)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ç¤ºä¾‹è®­ç»ƒï¼ˆä¼ªæ•°æ®ï¼‰
x_train = torch.randn(100, input_size)
y_train = torch.randn(100, 1)

for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

------

## âœ… 3. è®­ç»ƒå»ºè®®

- **å½’ä¸€åŒ–è¾“å…¥æ•°æ®**ï¼ˆæ ‡å‡†åŒ–ç‰¹å¾ï¼Œèƒ½æ˜¾è‘—æå‡è®­ç»ƒæ•ˆæœï¼‰ï¼›
- **æ‰¹è®­ç»ƒï¼ˆBatch Trainingï¼‰**ï¼šä½¿ç”¨ `DataLoader` æŒ‰å°æ‰¹é‡è®­ç»ƒæ›´ç¨³å®šï¼›
- **å­¦ä¹ ç‡è°ƒä¼˜**ï¼šå¯ä»¥å°è¯•ä» `0.001` å¼€å§‹ï¼›
- **æ—©åœæ³• Early Stopping**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šé™¤äº† MSEï¼Œä½ è¿˜å¯ä»¥ç›‘æ§ MAEã€RÂ² åˆ†æ•°ç­‰ï¼›

------

## âœ… 4. ç¤ºä¾‹æ¨¡å‹ç»“æ„å›¾

```
Input (10 features)
    â†“
Linear(10 â†’ 64) + ReLU
    â†“
Linear(64 â†’ 32) + ReLU
    â†“
Linear(32 â†’ 16) + ReLU
    â†“
Linear(16 â†’ 1) â†’ è¾“å‡ºé¢„æµ‹å€¼
```

------

éœ€è¦æˆ‘å¸®ä½ åŠ ä¸Š `DataLoader` æˆ–æ”¯æŒ CSV æ–‡ä»¶è¯»å–ã€å®Œæ•´è®­ç»ƒ/éªŒè¯æµç¨‹å—ï¼Ÿ

éå¸¸å¥½ï¼ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ DNN å›å½’æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œ**æ”¯æŒè¯»å– CSV æ–‡ä»¶ã€ä½¿ç”¨ DataLoaderã€æ ‡å‡†åŒ–æ•°æ®ã€è®­ç»ƒå’ŒéªŒè¯æµç¨‹**ã€‚æˆ‘ä¼šé€æ®µè§£é‡Šæ¯éƒ¨åˆ†ä»£ç çš„ä½œç”¨ã€‚

------

## âœ… ç¯å¢ƒä¾èµ–

ä½ éœ€è¦å…ˆå®‰è£… PyTorch å’Œ Pandasï¼ˆç”¨äºè¯»å– CSV æ–‡ä»¶ï¼‰ï¼š

```bash
pip install torch pandas scikit-learn
```

------

## âœ… å®Œæ•´ä»£ç ï¼ˆå«è§£é‡Šï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

### âœ… è¯´æ˜ï¼š

- å¼•å…¥æ‰€éœ€çš„åº“ï¼›
- `torch.utils.data.Dataset` æ˜¯ç”¨äºå®šä¹‰è‡ªå®šä¹‰æ•°æ®é›†ï¼›
- `StandardScaler` ç”¨äºç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæå‡è®­ç»ƒæ•ˆæœï¼‰ã€‚

------

```python
# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # ä¿è¯è¾“å‡ºç»´åº¦ä¸€è‡´

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

### âœ… è¯´æ˜ï¼š

- æŠŠ `numpy` æ ¼å¼çš„ç‰¹å¾å’Œæ ‡ç­¾è½¬æ¢ä¸º `torch.Tensor`ï¼›
- å®ç°äº† `__getitem__`ï¼Œæ–¹ä¾¿ DataLoader è°ƒç”¨ã€‚

------

```python
# DNN æ¨¡å‹ç»“æ„
class DNNRegressor(nn.Module):
    def __init__(self, input_size):
        super(DNNRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)  # å›å½’ï¼Œè¾“å‡ºä¸€ä¸ªå€¼
        )

    def forward(self, x):
        return self.model(x)
```

### âœ… è¯´æ˜ï¼š

- 4 å±‚çº¿æ€§å±‚ï¼›
- æ¯å±‚ä½¿ç”¨ `ReLU` æ¿€æ´»ï¼›
- æœ€åä¸€å±‚æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºä¸€ä¸ªè¿ç»­æ•°å€¼ã€‚

------

```python
# è¯»å– CSV æ–‡ä»¶ï¼ˆä½ å¯ä»¥æ¢æˆè‡ªå·±çš„æ–‡ä»¶è·¯å¾„ï¼‰
df = pd.read_csv("your_data.csv")

# å‡è®¾ 'target' æ˜¯ä½ è¦é¢„æµ‹çš„åˆ—
X = df.drop(columns=["target"]).values
y = df["target"].values

# æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# æ„é€ æ•°æ®é›†å’ŒåŠ è½½å™¨
train_dataset = RegressionDataset(X_train, y_train)
val_dataset = RegressionDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
```

### âœ… è¯´æ˜ï¼š

- ä» CSV è¯»å–æ•°æ®ï¼›
- æŠŠ `target` ä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼›
- æ ‡å‡†åŒ–ç‰¹å¾ï¼›
- åˆ†å‡ºéªŒè¯é›†ï¼ˆ20%ï¼‰ï¼›
- ä½¿ç”¨ `DataLoader` æ–¹ä¾¿æ‰¹é‡è®­ç»ƒã€‚

------

```python
# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = DNNRegressor(input_size=X.shape[1]).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

### âœ… è¯´æ˜ï¼š

- å°†æ¨¡å‹æ”¾åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼›
- ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼›
- ä½¿ç”¨ Adam ä¼˜åŒ–å™¨è®­ç»ƒç½‘ç»œå‚æ•°ã€‚

------

```python
# è®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹
num_epochs = 50
for epoch in range(num_epochs):
    # Training
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        output = model(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * X_batch.size(0)

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            output = model(X_batch)
            loss = criterion(output, y_batch)
            val_loss += loss.item() * X_batch.size(0)

    train_loss /= len(train_loader.dataset)
    val_loss /= len(val_loader.dataset)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
```

### âœ… è¯´æ˜ï¼š

- æ¯ä¸ª epoch åŒ…å«è®­ç»ƒå’ŒéªŒè¯ä¸¤ä¸ªé˜¶æ®µï¼›
- ä½¿ç”¨ `model.train()` å’Œ `model.eval()` æ§åˆ¶è¡Œä¸ºï¼›
- ç´¯è®¡æŸå¤±å€¼ï¼Œè®¡ç®—å¹³å‡è®­ç»ƒ/éªŒè¯è¯¯å·®ã€‚

------

## âœ… æ€»ç»“ç»“æ„å›¾

```text
CSV â†’ pandas â†’ æ ‡å‡†åŒ– â†’ DataLoader
                  â†“
              DNNRegressor
                  â†“
         MSELoss + Adamä¼˜åŒ–
                  â†“
         é¢„æµ‹ç»“æœ + éªŒè¯è¯„ä¼°
```

------

## âœ… å¯é€‰å¢å¼ºé¡¹ï¼ˆåç»­å¯åŠ ï¼‰ï¼š

- ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆ`torch.save`ï¼‰
- åŠ å…¥ EarlyStopping
- æ·»åŠ  TensorBoard æ—¥å¿—å¯è§†åŒ–
- å°è£…ä¸ºä¸€ä¸ªå¯å¤ç”¨ç±»/è®­ç»ƒå™¨

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥åŸºäºè¿™ä¸ªæ¨¡æ¿å¸®ä½ **å°è£…æˆä¸€ä¸ªè®­ç»ƒè„šæœ¬æˆ–æ¨¡å—**ï¼Œæˆ–è€…é›†æˆæ›´å¤æ‚çš„åŠŸèƒ½ï¼ˆå¦‚è‡ªåŠ¨è°ƒå‚ã€ç»˜å›¾ç­‰ï¼‰ã€‚ä½ å¸Œæœ›è¿è¡Œçš„ CSV æ–‡ä»¶æ ·ä¾‹é•¿ä»€ä¹ˆæ ·ï¼Ÿæˆ‘å¯ä»¥å¸®ä½ å†™ä¸€ä¸ªæ ·æœ¬ã€‚

éå¸¸å¥½ï¼ä¸‹é¢æˆ‘å°†ä¸ºä½ æ‰©å±• DNN å›å½’è®­ç»ƒæ¡†æ¶ï¼Œé›†æˆä»¥ä¸‹**å¤æ‚åŠŸèƒ½**ï¼Œé€‚åˆç”Ÿäº§çº§è®­ç»ƒï¼š

------

## âœ… **é›†æˆåŠŸèƒ½æ¦‚è§ˆ**

1. âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›† Loss æœ€ä½ï¼‰
2. âœ… Early Stoppingï¼ˆæå‰ç»ˆæ­¢è®­ç»ƒä»¥é¿å…è¿‡æ‹Ÿåˆï¼‰
3. âœ… TensorBoard å¯è§†åŒ–è®­ç»ƒæ—¥å¿—ï¼ˆè®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿ï¼‰
4. âœ… æ›´è§„èŒƒçš„è®­ç»ƒæµç¨‹å°è£…

------

## âœ… å®Œæ•´ä»£ç ç»“æ„

æˆ‘å°†å®Œæ•´è®­ç»ƒæµç¨‹æ‹†æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œç»„ç»‡æ›´æ¸…æ™°ï¼š

```
train.py                # ä¸»ç¨‹åºå…¥å£
models.py               # DNN æ¨¡å‹ç»“æ„
dataset.py              # è‡ªå®šä¹‰æ•°æ®é›†ç±»
utils.py                # è®­ç»ƒå·¥å…·å‡½æ•°ï¼ˆearly stoppingã€ä¿å­˜æ¨¡å‹ç­‰ï¼‰
logs/                   # TensorBoard æ—¥å¿—ç›®å½•
checkpoints/            # æ¨¡å‹ä¿å­˜ç›®å½•
```

------

## âœ… æ ·ä¾‹ä»£ç å®ç°ï¼ˆä»¥å•æ–‡ä»¶å½¢å¼å±•ç¤ºï¼Œå®é™…å¯æŒ‰æ¨¡å—æ‹†åˆ†ï¼‰

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# ======================== æ•°æ®é›†å®šä¹‰ ========================
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ======================== æ¨¡å‹å®šä¹‰ ========================
class DNNRegressor(nn.Module):
    def __init__(self, input_size):
        super(DNNRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.model(x)

# ======================== è®­ç»ƒè¾…åŠ©åŠŸèƒ½ ========================
class EarlyStopping:
    def __init__(self, patience=10, verbose=True, delta=1e-4):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_loss = np.Inf
        self.early_stop = False
        self.delta = delta

    def __call__(self, val_loss, model, path):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.counter = 0
            torch.save(model.state_dict(), path)
            if self.verbose:
                print(f"â†’ Validation loss improved, model saved.")
        else:
            self.counter += 1
            if self.verbose:
                print(f"â†’ No improvement. Early stop counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True

# ======================== ä¸»è®­ç»ƒå‡½æ•° ========================
def train():
    # å‚æ•°
    csv_path = "your_data.csv"
    target_column = "target"
    log_dir = "logs"
    model_path = "checkpoints/best_model.pt"
    os.makedirs("checkpoints", exist_ok=True)

    # åŠ è½½æ•°æ®
    df = pd.read_csv(csv_path)
    X = df.drop(columns=[target_column]).values
    y = df[target_column].values

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ‡åˆ†æ•°æ®
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    train_loader = DataLoader(RegressionDataset(X_train, y_train), batch_size=32, shuffle=True)
    val_loader = DataLoader(RegressionDataset(X_val, y_val), batch_size=32)

    # è®¾å¤‡é€‰æ‹©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆå§‹åŒ–æ¨¡å‹
    model = DNNRegressor(input_size=X.shape[1]).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    early_stopper = EarlyStopping(patience=10)
    writer = SummaryWriter(log_dir=log_dir)

    # è®­ç»ƒå¾ªç¯
    epochs = 100
    for epoch in range(epochs):
        model.train()
        total_train_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item() * X_batch.size(0)

        avg_train_loss = total_train_loss / len(train_loader.dataset)

        # éªŒè¯
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                loss = criterion(output, y_batch)
                total_val_loss += loss.item() * X_batch.size(0)

        avg_val_loss = total_val_loss / len(val_loader.dataset)

        # æ—¥å¿—è®°å½•
        writer.add_scalars("Loss", {
            "Train": avg_train_loss,
            "Validation": avg_val_loss
        }, epoch)

        print(f"[Epoch {epoch+1:03d}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        early_stopper(avg_val_loss, model, model_path)
        if early_stopper.early_stop:
            print("âœ‹ Early stopping triggered.")
            break

    writer.close()
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

if __name__ == "__main__":
    train()
```

------

## âœ… ä½¿ç”¨è¯´æ˜

1. å°†ä½ çš„ CSV æ–‡ä»¶æ”¾åœ¨åŒä¸€ç›®å½•ï¼Œå¹¶æ›¿æ¢è·¯å¾„ `your_data.csv`

2. `target_column` æ”¹ä¸ºä½ è¦é¢„æµ‹çš„åˆ—å

3. è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

   ```bash
   python train.py
   ```

4. è¿è¡Œ TensorBoard æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š

   ```bash
   tensorboard --logdir=logs
   ```

------

## âœ… å¯è§†åŒ–æ•ˆæœï¼ˆåœ¨ TensorBoardï¼‰

- æŸ¥çœ‹æŸå¤±æ›²çº¿æ”¶æ•›æƒ…å†µï¼›
- ç›‘æ§æ˜¯å¦è¿‡æ‹Ÿåˆï¼›
- ç›´è§‚å¯¹æ¯”ä¸åŒæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

------

## âœ… ä¸‹ä¸€æ­¥å»ºè®®

å¦‚æœä½ è¿˜éœ€è¦ï¼š

- åŠ å…¥éªŒè¯æŒ‡æ ‡ï¼ˆå¦‚ MAEã€RÂ²ï¼‰
- å°è£…æˆç±» / è®­ç»ƒå™¨ç»“æ„
- å¤šæ¨¡å‹å¯¹æ¯”è‡ªåŠ¨è°ƒå‚ï¼ˆå¦‚ GridSearchã€Optunaï¼‰

æˆ‘éƒ½å¯ä»¥ç»§ç»­å¸®ä½ æ‰©å±•ã€‚

ä½ æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ **è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªé¡¹ç›®ç»“æ„**æˆ–è€…åˆ›å»ºä¸€ä¸ª `.ipynb` çš„ notebook ç‰ˆæœ¬ç”¨äºè°ƒè¯•ï¼Ÿ

å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç»è¿‡å®Œæ•´å°è£…çš„ DNN å›å½’è®­ç»ƒæ¡†æ¶ï¼Œå…·å¤‡ä»¥ä¸‹é«˜çº§ç‰¹æ€§ï¼š

------

## âœ… åŠŸèƒ½åˆ—è¡¨

| åŠŸèƒ½                   | æè¿°                                                   |
| ---------------------- | ------------------------------------------------------ |
| **éªŒè¯æŒ‡æ ‡**           | æ·»åŠ  MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰ã€RÂ²ï¼ˆåˆ¤å®šç³»æ•°ï¼‰ä½œä¸ºè¯„ä¼°æŒ‡æ ‡   |
| **è®­ç»ƒå™¨ç»“æ„å°è£…**     | è®­ç»ƒè¿‡ç¨‹ã€éªŒè¯è¿‡ç¨‹ã€EarlyStopping å’Œä¿å­˜ç­‰å°è£…è¿›ç±»ä¸­   |
| **æ”¯æŒå¤šæ¨¡å‹ç»“æ„å¯¹æ¯”** | å¯ä¼ å…¥ä¸åŒæ¨¡å‹ç±»è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°                         |
| **æ”¯æŒè¶…å‚æ•°è°ƒä¼˜æ¥å£** | å¯é€šè¿‡å¤–éƒ¨è‡ªåŠ¨æœç´¢å™¨ï¼ˆå¦‚ Optunaã€Grid Searchï¼‰è¿›è¡Œè°ƒå‚ |

------

## âœ… ç»“æ„é¢„è§ˆï¼ˆå¯ä¸º `.py` æ¨¡å—æˆ– `.ipynb`ï¼‰

### 1. æ¨¡å‹å®šä¹‰ï¼ˆæ”¯æŒå¤šä¸ªï¼‰

```python
import torch.nn as nn

class DNN_V1(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)


class DNN_V2(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)
```

------

### 2. å›å½’è®­ç»ƒå™¨ï¼ˆ`RegressionTrainer` ç±»ï¼‰

```python
import torch
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np
import os

class RegressionTrainer:
    def __init__(self, model, optimizer, criterion, device, model_name="default"):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.writer = SummaryWriter(log_dir=f"logs/{model_name}")
        self.model_name = model_name
        self.best_val_loss = np.Inf
        self.patience = 10
        self.counter = 0
        os.makedirs("checkpoints", exist_ok=True)
        self.save_path = f"checkpoints/{model_name}.pt"

    def train(self, train_loader, val_loader, epochs=100):
        for epoch in range(epochs):
            train_loss = self._train_one_epoch(train_loader)
            val_loss, mae, r2 = self._validate(val_loader)

            self.writer.add_scalars("Loss", {
                "Train": train_loss,
                "Val": val_loss
            }, epoch)
            self.writer.add_scalar("MAE", mae, epoch)
            self.writer.add_scalar("R2", r2, epoch)

            print(f"[Epoch {epoch+1}] Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}")
            if self._check_early_stopping(val_loss):
                print("â†’ Early stopping.")
                break
        self.writer.close()

    def _train_one_epoch(self, loader):
        self.model.train()
        total_loss = 0
        for X, y in loader:
            X, y = X.to(self.device), y.to(self.device)
            self.optimizer.zero_grad()
            output = self.model(X)
            loss = self.criterion(output, y)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item() * X.size(0)
        return total_loss / len(loader.dataset)

    def _validate(self, loader):
        self.model.eval()
        total_loss = 0
        all_preds, all_targets = [], []
        with torch.no_grad():
            for X, y in loader:
                X, y = X.to(self.device), y.to(self.device)
                output = self.model(X)
                loss = self.criterion(output, y)
                total_loss += loss.item() * X.size(0)
                all_preds.append(output.cpu().numpy())
                all_targets.append(y.cpu().numpy())
        preds = np.concatenate(all_preds)
        targets = np.concatenate(all_targets)
        mae = mean_absolute_error(targets, preds)
        r2 = r2_score(targets, preds)
        return total_loss / len(loader.dataset), mae, r2

    def _check_early_stopping(self, val_loss):
        if val_loss < self.best_val_loss - 1e-4:
            self.best_val_loss = val_loss
            self.counter = 0
            torch.save(self.model.state_dict(), self.save_path)
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience
```

------

### 3. ä¸»è®­ç»ƒæµç¨‹

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim

# æ•°æ®é›†
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# æ•°æ®å‡†å¤‡
df = pd.read_csv("your_data.csv")
X = df.drop(columns=["target"]).values
y = df["target"].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2)

train_loader = DataLoader(RegressionDataset(X_train, y_train), batch_size=32, shuffle=True)
val_loader = DataLoader(RegressionDataset(X_val, y_val), batch_size=32)

# è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å¤šæ¨¡å‹æ¯”è¾ƒ
from models import DNN_V1, DNN_V2
from trainer import RegressionTrainer

models = {
    "DNN_V1": DNN_V1(X.shape[1]),
    "DNN_V2": DNN_V2(X.shape[1]),
}

for name, model in models.items():
    print(f"\nğŸ” Training {name}")
    trainer = RegressionTrainer(
        model=model,
        optimizer=optim.Adam(model.parameters(), lr=0.001),
        criterion=nn.MSELoss(),
        device=device,
        model_name=name
    )
    trainer.train(train_loader, val_loader)
```

------

## âœ… å¯é€‰ï¼šåŠ å…¥ Optuna è‡ªåŠ¨è°ƒå‚æ¥å£ï¼ˆç¤ºä¾‹ï¼‰

```python
import optuna

def objective(trial):
    hidden = trial.suggest_int("hidden", 32, 128)
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)

    model = nn.Sequential(
        nn.Linear(X.shape[1], hidden),
        nn.ReLU(),
        nn.Linear(hidden, 1)
    )

    trainer = RegressionTrainer(
        model=model,
        optimizer=optim.Adam(model.parameters(), lr=lr),
        criterion=nn.MSELoss(),
        device=device,
        model_name=f"trial_{trial.number}"
    )

    trainer.train(train_loader, val_loader)
    return trainer.best_val_loss

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=20)
```

------

## âœ… ç»“æœè¾“å‡ºä¸åˆ†æ

ä½ å°†ä¼šæ‹¥æœ‰ï¼š

- æ¯ä¸ªæ¨¡å‹å•ç‹¬çš„ TensorBoard æ—¥å¿—æ›²çº¿
- æ¯ä¸ªæ¨¡å‹è®­ç»ƒæ—¶éªŒè¯ loss / MAE / RÂ² çš„æ‰“å°è¾“å‡º
- æ¯ä¸ªæ¨¡å‹çš„ `.pt` æƒé‡æ–‡ä»¶ä¿å­˜åœ¨ `checkpoints/`

------

æ˜¯å¦éœ€è¦æˆ‘ï¼š

- æ•´ç†æˆ `.ipynb` æ–‡ä»¶ä¸Šä¼ ç»™ä½ ï¼Ÿ
- å¸®ä½ å®ç°å¤šçº¿ç¨‹/å¤šå¡å¹¶è¡Œæ¯”è¾ƒå¤šä¸ªæ¨¡å‹ï¼Ÿ
- è‡ªåŠ¨ç”Ÿæˆå®Œæ•´å·¥ç¨‹ç›®å½•å’Œå¯å¤ç”¨è„šæ‰‹æ¶ï¼Ÿ

ä½ æ›´å€¾å‘æœ¬åœ°è·‘è¿˜æ˜¯äº‘å¹³å°è·‘ï¼Ÿæˆ‘å¯ä»¥å¸®ä½ é€‚é…ã€‚

è¿™æ˜¯ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼åˆ¤æ–­ä¸€ä¸ª **DNN æ¨¡å‹æ˜¯å¦è®­ç»ƒæˆåŠŸ**ï¼Œä»¥åŠå®ƒçš„ **é¢„æµ‹æ˜¯å¦å‡†ç¡®**ï¼Œéœ€è¦ä»å¤šä¸ªè§’åº¦å…¥æ‰‹ã€‚ä¸‹é¢æ˜¯è¯¦ç»†è®²è§£ï¼š

------

## âœ… ä¸€ã€åˆ¤æ–­æ¨¡å‹æ˜¯å¦è®­ç»ƒæˆåŠŸ

å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢åˆ¤æ–­ï¼š

### 1. **è®­ç»ƒä¸éªŒè¯ Loss æ˜¯å¦ä¸‹é™**

- çœ‹ TensorBoard ä¸­çš„æ›²çº¿ï¼šæ˜¯å¦å¹³ç¨³ä¸‹é™ï¼Ÿ
- éªŒè¯é›† Loss æ˜¯å¦ä¹ŸåŒæ­¥ä¸‹é™ï¼ˆä»£è¡¨æ¨¡å‹æ²¡è¿‡æ‹Ÿåˆï¼‰ï¼Ÿ

ğŸ“‰ ç¤ºä¾‹ï¼ˆè‰¯å¥½æƒ…å†µï¼‰ï¼š

```
Epoch 1: Train Loss 1.203, Val Loss 1.180
Epoch 10: Train Loss 0.521, Val Loss 0.534
Epoch 50: Train Loss 0.312, Val Loss 0.318 âœ…
```

ğŸ“ˆ ç¤ºä¾‹ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼š

```
Epoch 1: Train Loss 1.2, Val Loss 1.1
Epoch 30: Train Loss 0.1, Val Loss 1.5 âŒï¼ˆè®­ç»ƒä¸‹é™ä½†éªŒè¯ä¸Šå‡ï¼‰
```

------

### 2. **Early Stopping æ˜¯å¦æå‰ç»ˆæ­¢**

- è‹¥æ¨¡å‹è‡ªåŠ¨åœæ­¢è®­ç»ƒï¼Œè¯´æ˜éªŒè¯é›†æ²¡æœ‰ç»§ç»­æå‡ï¼Œè¿™é€šå¸¸æ˜¯æ­£å¸¸çš„ï¼ˆè¡¨ç¤ºæ‰¾åˆ°äº†æœ€ä½³ç‚¹ï¼‰ï¼›
- è‹¥è®­ç»ƒæ»¡äº†å…¨éƒ¨è½®æ•°è¿˜æ²¡ early stopï¼Œè¯´æ˜å¯èƒ½æ²¡æ”¶æ•›ï¼Œä¹Ÿå¯èƒ½è¿˜éœ€è®­ç»ƒæ›´ä¹…ã€‚

------

## âœ… äºŒã€åˆ¤æ–­æ¨¡å‹æ˜¯å¦é¢„æµ‹å‡†ç¡®

ä¸»è¦ç”¨**å›å½’æŒ‡æ ‡**æ¥åˆ¤æ–­ï¼š

### 1. **MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰**

- è¡¨ç¤ºé¢„æµ‹ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³å‡ç»å¯¹å·®è·ï¼Œ**è¶Šå°è¶Šå¥½**ï¼›
- å¦‚æœä½ çš„ç›®æ ‡æ˜¯ä»·æ ¼æˆ–è¯„åˆ†ï¼Œè¿™ä¸ªå¾ˆç›´è§‚ã€‚

**ä¾‹å­**ï¼š

```
MAE = 2.5 â†’ å¹³å‡é¢„æµ‹è¯¯å·®æ˜¯ Â±2.5 å•ä½
```

------

### 2. **RÂ²ï¼ˆåˆ¤å®šç³»æ•°ï¼‰**

- è¡¡é‡æ¨¡å‹è§£é‡Šç›®æ ‡å˜é‡æ–¹å·®çš„èƒ½åŠ›ï¼ŒèŒƒå›´ä¸º **[0, 1]**ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰ï¼›
- RÂ² = 0ï¼šç­‰äºèƒ¡ä¹±çŒœï¼›
- RÂ² = 1ï¼šå®Œç¾é¢„æµ‹ï¼›
- RÂ² < 0ï¼šè¯´æ˜æ¨¡å‹æ¯”åŸºçº¿æ¨¡å‹è¿˜å·® âŒ

**ä¾‹å­**ï¼š

```
RÂ² = 0.92 â†’ æ¨¡å‹è§£é‡Šäº† 92% çš„æ–¹å·®ï¼Œéå¸¸å¥½
RÂ² = 0.5  â†’ æœ‰æ•ˆï¼Œä½†æ”¹è¿›ç©ºé—´å¾ˆå¤§
RÂ² < 0    â†’ æ¨¡å‹é¢„æµ‹æ¯”ç›´æ¥å–å¹³å‡å€¼è¿˜å·®
```

------

## âœ… ä¸‰ã€å¯è§†åŒ–éªŒè¯é¢„æµ‹å‡†ç¡®æ€§ï¼ˆå¼ºçƒˆæ¨èï¼‰

ä½ å¯ä»¥å°†æ¨¡å‹é¢„æµ‹ç»“æœç”»å›¾ä¸çœŸå®å€¼å¯¹æ¯”ï¼š

### ç¤ºä¾‹ä»£ç ï¼ˆæ·»åŠ åˆ°éªŒè¯é˜¶æ®µï¼‰ï¼š

```python
import matplotlib.pyplot as plt

# å–ä¸€æ‰¹éªŒè¯æ•°æ®é¢„æµ‹
X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
model.eval()
with torch.no_grad():
    y_pred = model(X_val_tensor).cpu().numpy()

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(8, 5))
plt.plot(y_val, label="çœŸå®å€¼", alpha=0.7)
plt.plot(y_pred, label="é¢„æµ‹å€¼", alpha=0.7)
plt.legend()
plt.title("é¢„æµ‹ vs çœŸå®")
plt.xlabel("æ ·æœ¬ç´¢å¼•")
plt.ylabel("ç›®æ ‡å€¼")
plt.grid(True)
plt.show()
```

ğŸ” **è‹¥é¢„æµ‹çº¿ä¸çœŸå®çº¿å½¢çŠ¶ç›¸ä¼¼ï¼Œè¯´æ˜æ¨¡å‹å­¦åˆ°äº†è¶‹åŠ¿ï¼›è‹¥åç¦»è¾ƒå¤§ï¼Œå°±éœ€è¦ä¼˜åŒ–æ¨¡å‹ç»“æ„æˆ–æ•°æ®å¤„ç†ã€‚**

------

## âœ… æ€»ç»“ï¼šè®­ç»ƒæˆåŠŸåˆ¤æ–­ Checklist

| æ£€æŸ¥é¡¹                       | æˆåŠŸåˆ¤æ–­æ ‡å‡†           |
| ---------------------------- | ---------------------- |
| è®­ç»ƒ/éªŒè¯ Loss æ˜¯å¦ä¸‹é™      | éƒ½ä¸‹é™ï¼ŒéªŒè¯ä¸åå¼¹     |
| MAE æ˜¯å¦è¶³å¤Ÿå°               | å’Œä¸šåŠ¡ç›®æ ‡è¯¯å·®è¦æ±‚ç›¸ç¬¦ |
| RÂ² æ˜¯å¦æ¥è¿‘ 1                | â‰¥ 0.8 æ˜¯ä¸é”™çš„æ¨¡å‹     |
| TensorBoard å¯è§†åŒ–æ˜¯å¦å¹³ç¨³   | Loss æ›²çº¿æ— å‰§çƒˆéœ‡è¡    |
| å¯è§†åŒ–é¢„æµ‹æ›²çº¿æ˜¯å¦æ‹ŸåˆçœŸå®å€¼ | è¶‹åŠ¿ä¸€è‡´ã€è¯¯å·®ç¨³å®š     |

------

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ å†™ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°ä¸å¯è§†åŒ–è„šæœ¬ï¼Œåªéœ€åŠ è½½æ¨¡å‹å°±èƒ½çœ‹åˆ°é¢„æµ‹å›¾å’Œè¯„åˆ†ï¼Ÿæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å°†æ¨¡å‹æ‰“åŒ…æˆ `.pt` å¹¶ç¼–å†™æ¨ç†æ¥å£ã€‚ä½ ç°åœ¨æ˜¯å¦å·²ç»æœ‰è‡ªå·±çš„æ•°æ®æˆ–æƒ³åšæ¨¡å‹éƒ¨ç½²ï¼Ÿ

ä¸‹é¢æ˜¯ä¸€ä¸ª **è‡ªåŠ¨è¯„ä¼°ä¸å¯è§†åŒ–è„šæœ¬**ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒæ¥ï¼š

âœ… åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå¦‚ `.pt` æ–‡ä»¶ï¼‰
 âœ… åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
 âœ… è‡ªåŠ¨è®¡ç®— MAEã€RÂ² ç­‰æŒ‡æ ‡
 âœ… ç”»å‡ºé¢„æµ‹å€¼ vs çœŸå®å€¼çš„å¯¹æ¯”å›¾

------

## âœ… ä¸€ã€è¯„ä¼°è„šæœ¬ `evaluate_model.py`

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, r2_score
from torch.utils.data import DataLoader, Dataset

# ä½ çš„æ¨¡å‹å®šä¹‰ï¼Œä¾‹å¦‚ DNN_V1
class DNN_V1(torch.nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)

# æ•°æ®é›†å°è£…
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# è¯„ä¼°å‡½æ•°
def evaluate_model(model_class, model_path, X_val, y_val, batch_size=32, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ¨¡å‹
    model = model_class(X_val.shape[1]).to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()

    val_dataset = RegressionDataset(X_val, y_val)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)

    preds, targets = [], []

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch = X_batch.to(device)
            output = model(X_batch).cpu().numpy()
            preds.append(output)
            targets.append(y_batch.numpy())

    preds = np.vstack(preds)
    targets = np.vstack(targets)

    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(targets, preds)
    r2 = r2_score(targets, preds)
    print(f"MAE: {mae:.4f}, RÂ²: {r2:.4f}")

    # å¯è§†åŒ–é¢„æµ‹ vs çœŸå®
    plt.figure(figsize=(10, 5))
    plt.plot(targets, label="çœŸå®å€¼", alpha=0.7)
    plt.plot(preds, label="é¢„æµ‹å€¼", alpha=0.7)
    plt.title("æ¨¡å‹é¢„æµ‹ç»“æœå¯¹æ¯”")
    plt.xlabel("æ ·æœ¬ç´¢å¼•")
    plt.ylabel("ç›®æ ‡å€¼")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
```

------

## âœ… äºŒã€ä½¿ç”¨ç¤ºä¾‹

ä½ å¯ä»¥åƒè¿™æ ·è°ƒç”¨å®ƒï¼ˆå‡è®¾ä½ åœ¨ `train.py` ä¸­å·²ç»è®­ç»ƒè¿‡æ¨¡å‹ï¼‰ï¼š

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from evaluate_model import evaluate_model, DNN_V1

# åŠ è½½åŸå§‹æ•°æ®
df = pd.read_csv("your_data.csv")
X = df.drop(columns=["target"]).values
y = df["target"].values

# åŒæ ·çš„é¢„å¤„ç†ï¼ˆå¾ˆé‡è¦ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# æ‹†åˆ†éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2)

# è·¯å¾„ä¸ºè®­ç»ƒæ—¶ä¿å­˜çš„æ¨¡å‹
evaluate_model(
    model_class=DNN_V1,
    model_path="checkpoints/DNN_V1.pt",
    X_val=X_val,
    y_val=y_val
)
```

------

## âœ… ä¸‰ã€è¾“å‡ºç¤ºæ„

```bash
MAE: 2.1347, RÂ²: 0.9183
```

ğŸ“ˆ ç„¶åä¼šå¼¹å‡ºå›¾åƒçª—å£æ˜¾ç¤ºé¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å¯¹æ¯”æ›²çº¿ã€‚

------

æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥å°è£…æˆå‘½ä»¤è¡Œå·¥å…·ï¼ˆå¦‚ `python evaluate.py --model DNN_V1 --checkpoint path/to.pt`ï¼‰ï¼Ÿ
 æˆ–è€…ä½ å¸Œæœ›æˆ‘æ‰©å±•æ”¯æŒå¤šä¸ªæ¨¡å‹æ‰¹é‡è¯„ä¼°ï¼Ÿ

å¥½çš„ï¼Œæˆ‘ä»¬ä»**é›¶å¼€å§‹**æ­å»ºä¸€ä¸ªæœ€ç®€å•çš„çº¿æ€§æ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´çš„ï¼š

1. **æ•°æ®é›†æ„é€ ï¼ˆäººå·¥ç”Ÿæˆï¼‰**
2. **æ¨¡å‹å®šä¹‰ï¼ˆçº¿æ€§å›å½’ï¼‰**
3. **è®­ç»ƒå¾ªç¯**
4. **é¢„æµ‹å’Œè¯„ä¼°**

------

## âœ… ä¸€ã€ç›®æ ‡

æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå½¢å¦‚ï¼š

y=3x+2+Ïµy = 3x + 2 + \epsilon

çš„çº¿æ€§å›å½’é—®é¢˜ï¼Œå…¶ä¸­ Ïµ\epsilon æ˜¯å™ªå£°ã€‚

------

## âœ… äºŒã€å®Œæ•´ä»£ç ï¼ˆæ— ä»»ä½•ä¾èµ–æ¡†æ¶ï¼Œåªç”¨ PyTorchï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ====== äººå·¥ç”Ÿæˆæ•°æ®é›† ======
torch.manual_seed(0)

# x ä» -1 åˆ° 1ï¼Œå…± 100 ä¸ªç‚¹
X = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # shape: (100, 1)
y = 3 * X + 2 + 0.2 * torch.randn(X.size())            # y = 3x + 2 + å™ªå£°

# ====== çº¿æ€§æ¨¡å‹å®šä¹‰ ======
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)  # y = wx + b

    def forward(self, x):
        return self.linear(x)

model = LinearRegressionModel()

# ====== è®­ç»ƒå‡†å¤‡ ======
criterion = nn.MSELoss()                    # å‡æ–¹è¯¯å·®æŸå¤±
optimizer = optim.SGD(model.parameters(), lr=0.1)

# ====== è®­ç»ƒè¿‡ç¨‹ ======
for epoch in range(100):
    pred = model(X)
    loss = criterion(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}")

# ====== é¢„æµ‹å’Œå¯è§†åŒ– ======
predicted = model(X).detach()

plt.figure(figsize=(8, 4))
plt.scatter(X.numpy(), y.numpy(), label="çœŸå®æ•°æ®")
plt.plot(X.numpy(), predicted.numpy(), 'r-', label="æ¨¡å‹é¢„æµ‹")
plt.legend()
plt.title("æœ€ç®€å•çº¿æ€§å›å½’æ¨¡å‹")
plt.xlabel("x")
plt.ylabel("y")
plt.grid(True)
plt.show()

# æŸ¥çœ‹è®­ç»ƒåçš„å‚æ•°
w, b = model.linear.weight.item(), model.linear.bias.item()
print(f"\nè®­ç»ƒåæ¨¡å‹å‚æ•°ï¼šw = {w:.3f}, b = {b:.3f}")
```

------

## âœ… ä¸‰ã€ä»£ç è¯´æ˜

| éƒ¨åˆ†     | å†…å®¹                                            |
| -------- | ----------------------------------------------- |
| æ•°æ®ç”Ÿæˆ | æ„é€  1D çš„çº¿æ€§å…³ç³» y=3x+2+å™ªå£°y = 3x + 2 + å™ªå£° |
| æ¨¡å‹å®šä¹‰ | `nn.Linear(1, 1)` å®ç°çº¿æ€§æ˜ å°„                  |
| æŸå¤±å‡½æ•° | `nn.MSELoss()`ï¼Œç”¨äºå›å½’é—®é¢˜                    |
| ä¼˜åŒ–å™¨   | `SGD`ï¼Œå­¦ä¹ ç‡ä¸º 0.1                             |
| è®­ç»ƒè¿‡ç¨‹ | æ­£å‘ä¼ æ’­ â†’ æŸå¤±è®¡ç®— â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°æƒé‡       |
| å¯è§†åŒ–   | çœŸå®æ•£ç‚¹å›¾ + æ¨¡å‹é¢„æµ‹ç›´çº¿                       |

------

## âœ… è¾“å‡ºç¤ºä¾‹ï¼ˆéƒ¨åˆ†ï¼‰

```bash
Epoch  10 | Loss: 0.0395
Epoch  20 | Loss: 0.0237
Epoch  30 | Loss: 0.0225
...
è®­ç»ƒåæ¨¡å‹å‚æ•°ï¼šw = 2.996, b = 2.002
```

ğŸ” å‚æ•°éå¸¸æ¥è¿‘åŸå§‹çš„ w=3w=3, b=2b=2ï¼Œè¯´æ˜æ¨¡å‹è®­ç»ƒæˆåŠŸã€‚

------

## âœ… ä¸‹ä¸€æ­¥å»ºè®®

- éœ€è¦æˆ‘åŸºäºè¿™ä¸ªæ¨¡æ¿æ‰©å±•æˆ **å¤šç‰¹å¾è¾“å…¥ï¼ˆç»“æ„åŒ–æ•°æ®ï¼‰** çš„å›å½’æ¨¡å‹ï¼Ÿ
- æˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸Šé¢åŠ ä¸ŠéªŒè¯é›†ã€æŒ‡æ ‡ã€å°è£…è®­ç»ƒå‡½æ•°ç­‰é€æ­¥å‡çº§æˆ DNNã€‚

éœ€è¦æˆ‘åšå“ªä¸€éƒ¨åˆ†ï¼Ÿä½ è¯´ã€‚

å¥½çš„ï¼Œæˆ‘ä»¬å°†åˆ†æ­¥éª¤é€æ­¥å‡çº§ä½ çš„æ¨¡å‹ã€‚ç°åœ¨å…ˆå®Œæˆ **ç¬¬ä¸€æ­¥ï¼šå°†çº¿æ€§å›å½’æ¨¡å‹æ‰©å±•ä¸ºå¤šç‰¹å¾è¾“å…¥çš„å›å½’æ¨¡å‹**ï¼Œæ¥ä¸‹æ¥ä¼šé€æ­¥æ·»åŠ éªŒè¯é›†ã€æŒ‡æ ‡å’Œå°è£…è®­ç»ƒå‡½æ•°ç­‰ã€‚

------

## âœ… ç¬¬ä¸€æ­¥ï¼šå¤šç‰¹å¾è¾“å…¥çš„çº¿æ€§å›å½’æ¨¡å‹

æˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ªäººå·¥ç”Ÿæˆçš„æ•°æ®é›†ï¼š

y=1.5x1âˆ’2x2+0.8x3+3+Ïµy = 1.5x_1 - 2x_2 + 0.8x_3 + 3 + \epsilon

### ğŸ”§ å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# è®¾ç½®éšæœºç§å­
torch.manual_seed(0)

# ====== 1. ç”Ÿæˆå¤šç‰¹å¾æ•°æ®é›† ======
num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)  # (200, 3)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])  # (3, 1)
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise  # çº¿æ€§ç»„åˆ + å™ªå£°

# ====== 2. å®šä¹‰å¤šç‰¹å¾çº¿æ€§å›å½’æ¨¡å‹ ======
class LinearRegressionMulti(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)

model = LinearRegressionMulti(input_dim=num_features)

# ====== 3. è®­ç»ƒå‡†å¤‡ ======
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# ====== 4. æ¨¡å‹è®­ç»ƒ ======
epochs = 100
for epoch in range(epochs):
    pred = model(X)
    loss = criterion(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}")

# ====== 5. æ‰“å°æ¨¡å‹å‚æ•° ======
w = model.linear.weight.data
b = model.linear.bias.data
print(f"\nè®­ç»ƒåæƒé‡ï¼š{w.numpy()}")
print(f"è®­ç»ƒååç½®ï¼š{b.item():.4f}")
```

------

## âœ… è¾“å‡ºç¤ºä¾‹

```bash
Epoch  10 | Loss: 0.0185
Epoch  20 | Loss: 0.0104
...
è®­ç»ƒåæƒé‡ï¼š
[[ 1.497 -2.004  0.797]]
è®­ç»ƒååç½®ï¼š3.0001
```

è¯´æ˜æ¨¡å‹å·²ç»å¾ˆå¥½åœ°æ‹Ÿåˆäº†ç›®æ ‡å‡½æ•°ã€‚

------

## âœ… ä¸‹ä¸€æ­¥è®¡åˆ’

æˆ‘ä»¬å°†é€æ­¥åŠ å…¥ä»¥ä¸‹éƒ¨åˆ†ï¼š

| æ­¥éª¤     | å†…å®¹                               |
| -------- | ---------------------------------- |
| âœ… Step 1 | å¤šç‰¹å¾è¾“å…¥ âœ”ï¸                       |
| ğŸ”œ Step 2 | æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†                    |
| ğŸ”œ Step 3 | æ·»åŠ è¯„ä»·æŒ‡æ ‡ï¼ˆMAEã€RÂ²ï¼‰            |
| ğŸ”œ Step 4 | å°è£…è®­ç»ƒå‡½æ•°                       |
| ğŸ”œ Step 5 | å‡çº§ä¸ºå¯é…ç½®çš„ DNN ç»“æ„            |
| ğŸ”œ Step 6 | åŠ å…¥ EarlyStoppingã€æ—¥å¿—è®°å½•ç­‰åŠŸèƒ½ |

æ˜¯å¦ç°åœ¨è¿›å…¥ Step 2ï¼ˆæ‹†åˆ†è®­ç»ƒéªŒè¯é›† + åˆå§‹è¯„ä¼°æŒ‡æ ‡ï¼‰ï¼Ÿ

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬ç»§ç»­æŒ‰æ­¥éª¤æ¥ã€‚ç°åœ¨è¿›å…¥ï¼š

------

## âœ… Step 2ï¼šæ‹†åˆ†è®­ç»ƒé›† / éªŒè¯é›†ï¼Œå¹¶æ·»åŠ åŸºç¡€è¯„ä¼°æŒ‡æ ‡ï¼ˆMSEã€MAEã€RÂ²ï¼‰

------

### ğŸ”§ æ”¹è¿›å†…å®¹ï¼š

1. **ä½¿ç”¨ `scikit-learn` çš„ `train_test_split` æ‹†åˆ†æ•°æ®**
2. **æ·»åŠ  MAEã€RÂ² æŒ‡æ ‡ï¼ˆä½¿ç”¨ `sklearn.metrics`ï¼‰**
3. **åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½**

------

### âœ… å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# è®¾ç½®éšæœºç§å­
torch.manual_seed(0)

# ====== 1. ç”Ÿæˆæ•°æ® ======
num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

# æ‹†åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ80/20ï¼‰
X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

# è½¬å› tensor
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val   = torch.tensor(X_val,   dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val   = torch.tensor(y_val,   dtype=torch.float32)

# ====== 2. å®šä¹‰æ¨¡å‹ ======
class LinearRegressionMulti(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)

model = LinearRegressionMulti(input_dim=num_features)

# ====== 3. è®­ç»ƒå‡†å¤‡ ======
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# ====== 4. æ¨¡å‹è®­ç»ƒ ======
for epoch in range(100):
    model.train()
    pred = model(X_train)
    loss = criterion(pred, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

# ====== 5. éªŒè¯é›†è¯„ä¼° ======
model.eval()
with torch.no_grad():
    val_pred = model(X_val).numpy()
    val_true = y_val.numpy()

mae = mean_absolute_error(val_true, val_pred)
r2 = r2_score(val_true, val_pred)
mse = np.mean((val_pred - val_true) ** 2)

print(f"\néªŒè¯é›†è¯„ä¼°:")
print(f" - MAE: {mae:.4f}")
print(f" - MSE: {mse:.4f}")
print(f" - RÂ² : {r2:.4f}")
```

------

### âœ… ç¤ºä¾‹è¾“å‡ºï¼ˆå¯èƒ½ç•¥æœ‰å·®å¼‚ï¼‰

```
Epoch 100 | Train Loss: 0.0097

éªŒè¯é›†è¯„ä¼°:
 - MAE: 0.0703
 - MSE: 0.0081
 - RÂ² : 0.9973
```

------

### ğŸ§  è§£é‡ŠæŒ‡æ ‡

| æŒ‡æ ‡    | å«ä¹‰                        |
| ------- | --------------------------- |
| **MSE** | å‡æ–¹è¯¯å·®ï¼Œå¸¸ç”¨æŸå¤±å‡½æ•°      |
| **MAE** | å¹³å‡ç»å¯¹è¯¯å·®ï¼Œç¨³å®šæ€§æ›´å¼º    |
| **RÂ²**  | å†³å®šç³»æ•°ï¼Œè¶Šæ¥è¿‘ 1 è¡¨ç¤ºè¶Šå‡† |

------

## âœ… ä¸‹ä¸€æ­¥è®¡åˆ’ï¼ˆStep 3ï¼‰

å°†è¿™äº›æµç¨‹å°è£…æˆé€šç”¨çš„è®­ç»ƒå‡½æ•°å’Œè¯„ä¼°å‡½æ•°ï¼Œæ¯”å¦‚ï¼š

- `train_model(model, X_train, y_train, ...)`
- `evaluate_model(model, X_val, y_val)`

æ˜¯å¦ç»§ç»­æ‰§è¡Œ Step 3ï¼Ÿè¿˜æ˜¯ä½ å¸Œæœ›å…ˆè¿›ä¸€æ­¥äº†è§£æŸéƒ¨åˆ†å†…å®¹ï¼Ÿ

å¥½ï¼Œæˆ‘ä»¬è¿›å…¥ âœ… **Step 3ï¼šå°è£…è®­ç»ƒä¸è¯„ä¼°å‡½æ•°**ï¼Œä½¿è®­ç»ƒæµç¨‹æ›´ç®€æ´ã€å¯å¤ç”¨ï¼Œä¸ºåç»­å‡çº§ä¸º DNN å’Œå¤šæ¨¡å‹å¯¹æ¯”æ‰“å¥½åŸºç¡€ã€‚

------

## âœ… æœ¬æ­¥éª¤ç›®æ ‡

å°è£…ä»¥ä¸‹å‡½æ•°ï¼š

- `train_model(...)`ï¼šè´Ÿè´£è®­ç»ƒè¿‡ç¨‹
- `evaluate_model(...)`ï¼šè´Ÿè´£æ¨¡å‹éªŒè¯è¯„ä¼°
- æ¨¡å‹ç»“æ„ä»ä¿æŒçº¿æ€§ç»“æ„ï¼Œè¾“å…¥æ”¯æŒå¤šç‰¹å¾

------

## âœ… å®Œæ•´ä»£ç ï¼ˆåŒ…å«å°è£…å‡½æ•°ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# ====== 1. æ„é€ æ•°æ® ======
torch.manual_seed(0)

num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

# æ‹†åˆ†æ•°æ®é›†
X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

# è½¬æˆ tensor
X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)

# ====== 2. å®šä¹‰æ¨¡å‹ç±» ======
class LinearRegressionMulti(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)

# ====== 3. è®­ç»ƒå‡½æ•°å°è£… ======
def train_model(model, X_train, y_train, epochs=100, lr=0.1, verbose=True):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    
    for epoch in range(epochs):
        model.train()
        pred = model(X_train)
        loss = criterion(pred, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

# ====== 4. éªŒè¯è¯„ä¼°å‡½æ•°å°è£… ======
def evaluate_model(model, X_val, y_val):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).numpy()
        y_true = y_val.numpy()

    mae = mean_absolute_error(y_true, y_pred)
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)

    print(f"\néªŒè¯é›†è¯„ä¼°:")
    print(f" - MAE: {mae:.4f}")
    print(f" - MSE: {mse:.4f}")
    print(f" - RÂ² : {r2:.4f}")
    return {"mae": mae, "mse": mse, "r2": r2}

# ====== 5. ä½¿ç”¨å°è£…å¥½çš„å‡½æ•°è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ ======
model = LinearRegressionMulti(input_dim=num_features)

train_model(model, X_train, y_train, epochs=100, lr=0.1)

evaluate_model(model, X_val, y_val)
```

------

## âœ… ç¤ºä¾‹è¾“å‡º

```
Epoch  10 | Train Loss: 0.0430
Epoch  20 | Train Loss: 0.0260
...
éªŒè¯é›†è¯„ä¼°:
 - MAE: 0.0642
 - MSE: 0.0069
 - RÂ² : 0.9976
```

------

## âœ… å°ç»“ï¼šæˆ‘ä»¬å®Œæˆäº†

| âœ…    | ä»»åŠ¡                         |
| ---- | ---------------------------- |
| âœ”ï¸    | å¤šç‰¹å¾è¾“å…¥æ”¯æŒ               |
| âœ”ï¸    | æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†              |
| âœ”ï¸    | æ·»åŠ è¯„ä¼°æŒ‡æ ‡ï¼ˆMSEã€MAEã€RÂ²ï¼‰ |
| âœ”ï¸    | å°è£…è®­ç»ƒä¸è¯„ä¼°æµç¨‹           |

------

## ğŸ”œ ä¸‹ä¸€æ­¥ï¼ˆStep 4ï¼‰

æˆ‘ä»¬å°†ï¼š

- æŠŠæ¨¡å‹å‡çº§ä¸º DNNï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰
- æ”¯æŒé…ç½®éšè—å±‚ã€æ¿€æ´»å‡½æ•°ç­‰
- ç»§ç»­ä½¿ç”¨ç°æœ‰çš„è®­ç»ƒå‡½æ•°ï¼Œä½†ä¿æŒå…¼å®¹æ€§

æ˜¯å¦ç»§ç»­è¿›å…¥ **Step 4ï¼šæ¨¡å‹å‡çº§ä¸º DNN**ï¼Ÿ

å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ **Step 4 çš„å®Œæ•´ä»£ç ä¸è¿è¡Œç¤ºä¾‹**ï¼Œå°†æˆ‘ä»¬åŸæ¥çš„çº¿æ€§æ¨¡å‹å‡çº§ä¸º **å¯é…ç½® DNNï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰**ï¼Œå¹¶ä¿æŒè®­ç»ƒ/è¯„ä¼°é€»è¾‘ä¸€è‡´ã€‚

------

## âœ… Step 4 å®Œæ•´ä»£ç ï¼šå¤šç‰¹å¾è¾“å…¥ + DNN æ¨¡å‹ + å°è£…è®­ç»ƒè¯„ä¼°

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# ========================
# 1. æ•°æ®æ„é€ 
# ========================
torch.manual_seed(0)
np.random.seed(0)

num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

# æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val   = torch.tensor(X_val, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val   = torch.tensor(y_val, dtype=torch.float32)

# ========================
# 2. å¯é…ç½® DNN æ¨¡å‹å®šä¹‰
# ========================
class DNNRegressor(nn.Module):
    def __init__(self, input_dim, hidden_layers=[64, 32], activation=nn.ReLU):
        super().__init__()
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_layers:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(activation())  # å®ä¾‹åŒ–æ¿€æ´»å‡½æ•°
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))  # è¾“å‡ºå±‚
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# ========================
# 3. å°è£…è®­ç»ƒå‡½æ•°
# ========================
def train_model(model, X_train, y_train, epochs=100, lr=0.01, verbose=True):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        pred = model(X_train)
        loss = criterion(pred, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

# ========================
# 4. å°è£…éªŒè¯è¯„ä¼°å‡½æ•°
# ========================
def evaluate_model(model, X_val, y_val):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).numpy()
        y_true = y_val.numpy()

    mae = mean_absolute_error(y_true, y_pred)
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)

    print(f"\néªŒè¯é›†è¯„ä¼°:")
    print(f" - MAE: {mae:.4f}")
    print(f" - MSE: {mse:.4f}")
    print(f" - RÂ² : {r2:.4f}")
    return {"mae": mae, "mse": mse, "r2": r2}

# ========================
# 5. ä½¿ç”¨ç¤ºä¾‹
# ========================
# å®šä¹‰æ¨¡å‹ç»“æ„ï¼šè¾“å…¥ 3 ç‰¹å¾ï¼Œ2 ä¸ªéšè—å±‚ [64, 32]
model = DNNRegressor(input_dim=num_features, hidden_layers=[64, 32], activation=nn.ReLU)

# è®­ç»ƒæ¨¡å‹
train_model(model, X_train, y_train, epochs=200, lr=0.01)

# éªŒè¯æ¨¡å‹æ€§èƒ½
evaluate_model(model, X_val, y_val)
```

------

## âœ… ç¤ºä¾‹è¾“å‡ºï¼ˆå¯èƒ½ç•¥æœ‰å·®å¼‚ï¼‰

```
Epoch  10 | Train Loss: 0.3468
Epoch  20 | Train Loss: 0.1891
...
Epoch 200 | Train Loss: 0.0083

éªŒè¯é›†è¯„ä¼°:
 - MAE: 0.0689
 - MSE: 0.0076
 - RÂ² : 0.9975
```

------

## âœ… æ¨¡å‹ç»“æ„è¯´æ˜

å½“å‰æ¨¡å‹ç»“æ„ï¼š

```
è¾“å…¥ï¼ˆ3ç»´ï¼‰
 â†’ Linear(3â†’64) â†’ ReLU
 â†’ Linear(64â†’32) â†’ ReLU
 â†’ Linear(32â†’1)
```

ä½ å¯ä»¥è½»æ¾æ¢æˆåˆ«çš„ç»“æ„ï¼Œä¾‹å¦‚ï¼š

- æ”¹ä¸º `[128, 64, 16]` éšè—å±‚
- æ”¹ä¸º `nn.Tanh` æ¿€æ´»å‡½æ•°

ç¤ºä¾‹ï¼š

```python
model = DNNRegressor(input_dim=3, hidden_layers=[128, 64, 16], activation=nn.Tanh)
```

------

## âœ… æ˜¯å¦ç»§ç»­ Step 5ï¼Ÿ

ä¸‹ä¸€æ­¥æˆ‘ä»¬å°†å®ç°ï¼š

- å¯¹æ¯”å¤šä¸ªæ¨¡å‹ç»“æ„ï¼ˆå¤šæ¬¡å®éªŒï¼‰
- è‡ªåŠ¨è®°å½•æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼ˆå¦‚ RÂ²ï¼‰
- æ·»åŠ è¶…å‚ç»„åˆå°è¯•ï¼ˆç»“æ„ + å­¦ä¹ ç‡ç­‰ï¼‰

æ˜¯å¦ç°åœ¨è¿›å…¥ **Step 5ï¼šå¤šæ¨¡å‹å¯¹æ¯” + è‡ªåŠ¨è°ƒå‚**ï¼Ÿ

å¥½çš„ï¼Œç»§ç»­è¿›å…¥ï¼š

------

# âœ… Step 5ï¼šå¤šæ¨¡å‹ç»“æ„å¯¹æ¯” + è¶…å‚æ•°ç»„åˆå°è¯•ï¼ˆè‡ªåŠ¨è°ƒå‚ï¼‰

------

## ğŸ¯ ç›®æ ‡

æˆ‘ä»¬å°†åœ¨å½“å‰è®­ç»ƒä¸è¯„ä¼°é€»è¾‘åŸºç¡€ä¸Šï¼š

- å°è£…ä¸€ä¸ª **å®éªŒå™¨ï¼ˆ`ModelRunner`ï¼‰**ï¼Œæ”¯æŒï¼š
  - å¤šç§æ¨¡å‹ç»“æ„å°è¯•
  - è‡ªåŠ¨åŒ–è®­ç»ƒã€è¯„ä¼°ã€è®°å½•ç»“æœ
  - è¾“å‡ºæœ€ä½³æ¨¡å‹åŠå„æ¨¡å‹è¡¨ç°
- å¯¹æ¯”æ¨¡å‹ç»“æ„ã€éšè—å±‚è®¾ç½®ã€å­¦ä¹ ç‡ç­‰è¶…å‚æ•°ç»„åˆ
- å‡†å¤‡å¥½ç”¨äºå¯è§†åŒ–ï¼ˆä¸‹ä¸€æ­¥ï¼‰

------

## âœ… å®Œæ•´ä»£ç ï¼ˆåŸºäº Step 4 + æ–°å¢ `ModelRunner` ç±»ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# ========== æ•°æ®æ„é€  ==========
torch.manual_seed(0)
np.random.seed(0)

num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val   = torch.tensor(X_val, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val   = torch.tensor(y_val, dtype=torch.float32)

# ========== DNN æ¨¡å‹ ==========
class DNNRegressor(nn.Module):
    def __init__(self, input_dim, hidden_layers=[64, 32], activation=nn.ReLU):
        super().__init__()
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_layers:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(activation())
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# ========== è®­ç»ƒå‡½æ•° ==========
def train_model(model, X_train, y_train, epochs=100, lr=0.01, verbose=False):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        pred = model(X_train)
        loss = criterion(pred, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

# ========== éªŒè¯å‡½æ•° ==========
def evaluate_model(model, X_val, y_val):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).numpy()
        y_true = y_val.numpy()

    mae = mean_absolute_error(y_true, y_pred)
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)
    return {"mae": mae, "mse": mse, "r2": r2}

# ========== å¤šæ¨¡å‹è®­ç»ƒå™¨ ==========
class ModelRunner:
    def __init__(self, input_dim):
        self.input_dim = input_dim
        self.results = []

    def run(self, configs, X_train, y_train, X_val, y_val, epochs=100):
        for idx, config in enumerate(configs):
            print(f"\nğŸš€ æ¨¡å‹ {idx+1}/{len(configs)}: {config}")
            model = DNNRegressor(
                input_dim=self.input_dim,
                hidden_layers=config["hidden_layers"],
                activation=config.get("activation", nn.ReLU)
            )
            train_model(model, X_train, y_train, epochs=epochs, lr=config["lr"])
            scores = evaluate_model(model, X_val, y_val)
            result = {
                "config": config,
                "scores": scores
            }
            self.results.append(result)

    def summary(self, sort_by="r2"):
        print("\nğŸ“Š æ‰€æœ‰æ¨¡å‹è¡¨ç°ï¼š")
        sorted_results = sorted(self.results, key=lambda r: r["scores"][sort_by], reverse=True)
        for i, res in enumerate(sorted_results):
            cfg = res["config"]
            sc = res["scores"]
            print(f"[{i+1}] RÂ²={sc['r2']:.4f} | MAE={sc['mae']:.4f} | MSE={sc['mse']:.4f} | {cfg}")
        return sorted_results

# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
runner = ModelRunner(input_dim=num_features)

param_grid = [
    {"hidden_layers": [64, 32],     "lr": 0.01},
    {"hidden_layers": [128, 64],    "lr": 0.01},
    {"hidden_layers": [64, 64, 32], "lr": 0.005},
    {"hidden_layers": [32],         "lr": 0.1},
    {"hidden_layers": [128],        "lr": 0.001},
]

runner.run(param_grid, X_train, y_train, X_val, y_val, epochs=200)

runner.summary()
```

------

## âœ… ç¤ºä¾‹è¾“å‡ºï¼ˆéƒ¨åˆ†ï¼‰

```
ğŸš€ æ¨¡å‹ 1/5: {'hidden_layers': [64, 32], 'lr': 0.01}
Epoch 200 | Train Loss: 0.0084
éªŒè¯é›†è¯„ä¼°: RÂ² = 0.9971

ğŸš€ æ¨¡å‹ 2/5: {'hidden_layers': [128, 64], 'lr': 0.01}
...
ğŸ“Š æ‰€æœ‰æ¨¡å‹è¡¨ç°ï¼š
[1] RÂ²=0.9975 | MAE=0.0671 | MSE=0.0071 | {'hidden_layers': [128, 64], 'lr': 0.01}
[2] RÂ²=0.9971 | MAE=0.0685 | MSE=0.0076 | {'hidden_layers': [64, 32], 'lr': 0.01}
[3] ...
```

------

## âœ… åŠŸèƒ½è¯´æ˜æ€»ç»“

| åŠŸèƒ½           | å®ç°                     |
| -------------- | ------------------------ |
| å¤šæ¨¡å‹ç»“æ„å¯¹æ¯” | è‡ªåŠ¨å¾ªç¯é…ç½®è®­ç»ƒ         |
| è¯„ä¼°æŒ‡æ ‡       | RÂ²ã€MAEã€MSE             |
| è®­ç»ƒå‡½æ•°       | å¯é‡å¤ä½¿ç”¨               |
| è¾“å‡ºæ’åº       | é»˜è®¤æŒ‰ `RÂ²` é™åºæ˜¾ç¤ºç»“æœ |

------

## ğŸ”œ ä¸‹ä¸€æ­¥ï¼šStep 6ï¼ˆè‡ªåŠ¨ç»˜å›¾æ—¥å¿— & å¯è§†åŒ–ï¼‰

æˆ‘ä»¬å°†ï¼š

- è‡ªåŠ¨è®°å½•æ¯è½®è®­ç»ƒæŸå¤±
- ç”¨ `matplotlib` ç»˜åˆ¶å­¦ä¹ æ›²çº¿
- å‡†å¤‡æ¥å…¥ `TensorBoard` æˆ– `wandb` å¯é€‰æ¨¡å—

æ˜¯å¦ç»§ç»­è¿›å…¥ **Step 6ï¼šå¯è§†åŒ–ä¸æ—¥å¿—è¿½è¸ª**ï¼Ÿ

å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ **Step 6 çš„å®Œæ•´å¯è¿è¡Œä»£ç **ï¼ŒåŒ…æ‹¬æ•°æ®æ„é€ ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒã€éªŒè¯ã€æ—¥å¿—è®°å½•ã€æŸå¤±æ›²çº¿ç»˜åˆ¶ç­‰å†…å®¹ï¼Œç¡®ä¿ä½ èƒ½åœ¨æœ¬åœ°å®Œæ•´è¿è¡Œåå†è¿›å…¥ Step 7ã€‚

------

# âœ… Step 6ï¼šå®Œæ•´ä»£ç ï¼ˆå«è®­ç»ƒæ—¥å¿—ä¸å¯è§†åŒ–ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# ========== Step 1: ç”Ÿæˆç»“æ„åŒ–æ•°æ® ==========
torch.manual_seed(0)
np.random.seed(0)

num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val   = torch.tensor(X_val, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val   = torch.tensor(y_val, dtype=torch.float32)

# ========== Step 2: å®šä¹‰æ¨¡å‹ç»“æ„ ==========
class DNNRegressor(nn.Module):
    def __init__(self, input_dim, hidden_layers=[64, 32], activation=nn.ReLU):
        super().__init__()
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_layers:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(activation())
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# ========== Step 3: è®­ç»ƒå‡½æ•°ï¼Œè®°å½• loss ==========
def train_model(model, X_train, y_train, epochs=100, lr=0.01, verbose=False):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    loss_history = []

    for epoch in range(epochs):
        model.train()
        pred = model(X_train)
        loss = criterion(pred, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loss_history.append(loss.item())

        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

    return loss_history

# ========== Step 4: éªŒè¯æŒ‡æ ‡å‡½æ•° ==========
def evaluate_model(model, X_val, y_val):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).numpy()
        y_true = y_val.numpy()

    mae = mean_absolute_error(y_true, y_pred)
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)
    return {"mae": mae, "mse": mse, "r2": r2}

# ========== Step 5: å¤šæ¨¡å‹è¿è¡Œå™¨ + loss æ›²çº¿ç»˜å›¾ ==========
class ModelRunner:
    def __init__(self, input_dim):
        self.input_dim = input_dim
        self.results = []

    def run(self, configs, X_train, y_train, X_val, y_val, epochs=100):
        for idx, config in enumerate(configs):
            print(f"\nğŸš€ æ¨¡å‹ {idx+1}/{len(configs)}: {config}")
            model = DNNRegressor(
                input_dim=self.input_dim,
                hidden_layers=config["hidden_layers"],
                activation=config.get("activation", nn.ReLU)
            )
            loss_history = train_model(model, X_train, y_train, epochs=epochs, lr=config["lr"])
            scores = evaluate_model(model, X_val, y_val)
            result = {
                "config": config,
                "scores": scores,
                "losses": loss_history
            }
            self.results.append(result)

    def summary(self, sort_by="r2"):
        print("\nğŸ“Š æ‰€æœ‰æ¨¡å‹è¡¨ç°ï¼š")
        sorted_results = sorted(self.results, key=lambda r: r["scores"][sort_by], reverse=True)
        for i, res in enumerate(sorted_results):
            cfg = res["config"]
            sc = res["scores"]
            print(f"[{i+1}] RÂ²={sc['r2']:.4f} | MAE={sc['mae']:.4f} | MSE={sc['mse']:.4f} | {cfg}")
        return sorted_results

    def plot_losses(self):
        plt.figure(figsize=(10, 6))
        for i, result in enumerate(self.results):
            label = f"Model {i+1}: {result['config']['hidden_layers']}, lr={result['config']['lr']}"
            plt.plot(result["losses"], label=label)
        plt.xlabel("Epoch")
        plt.ylabel("Train Loss (MSE)")
        plt.title("è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

# ========== Step 6: ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    runner = ModelRunner(input_dim=num_features)

    param_grid = [
        {"hidden_layers": [64, 32],     "lr": 0.01},
        {"hidden_layers": [128, 64],    "lr": 0.01},
        {"hidden_layers": [64, 64, 32], "lr": 0.005},
    ]

    runner.run(param_grid, X_train, y_train, X_val, y_val, epochs=200)
    runner.summary()
    runner.plot_losses()
```

------

## âœ… è¾“å‡ºæ•ˆæœ

- æ§åˆ¶å°ä¼šæ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„è®­ç»ƒè¡¨ç°ï¼ˆRÂ²ã€MAEã€MSEï¼‰
- å¼¹å‡ºä¸€ä¸ª matplotlib å›¾å½¢çª—å£ï¼Œå±•ç¤ºæ¯ä¸ªæ¨¡å‹çš„è®­ç»ƒæŸå¤±æ›²çº¿ï¼Œå¦‚ï¼š

```
[1] RÂ²=0.9973 | MAE=0.0690 | MSE=0.0075 | {'hidden_layers': [128, 64], 'lr': 0.01}
[2] RÂ²=0.9971 | MAE=0.0684 | MSE=0.0077 | {'hidden_layers': [64, 32], 'lr': 0.01}
...
```

------

## âœ… Step 6 å®Œæˆ âœ…

ä½ ç°åœ¨å·²ç»æœ‰ä¸€ä¸ªï¼š

- å¤šæ¨¡å‹è®­ç»ƒ/è¯„ä¼°/å¯¹æ¯”ç³»ç»Ÿ
- è‡ªåŠ¨è®°å½•è®­ç»ƒè¿‡ç¨‹
- è®­ç»ƒæ›²çº¿å¯è§†åŒ–å·¥å…·

------

## ğŸ”œ Step 7ï¼ˆä¸‹ä¸€æ­¥ï¼‰

æˆ‘ä»¬å°†åœ¨ Step 7 åšï¼š

1. **è‡ªåŠ¨ä¿å­˜ç»“æœï¼ˆCSV / JSONï¼‰**
2. **é¢„æµ‹ç»“æœå¯¹æ¯”å›¾ï¼ˆçœŸå€¼ vs é¢„æµ‹ï¼‰**
3. æ”¯æŒæ¨¡å‹å¯¼å‡º/åŠ è½½ï¼ˆå¯é€‰ï¼‰

æ˜¯å¦ç»§ç»­ï¼ŸğŸ§ 

å¥½çš„ï¼ä¸‹é¢å°† Step 7 çš„å†…å®¹æ•´ç†ä¸ºä¸€ä¸ªå®Œæ•´çš„ Jupyter Notebookï¼Œå¹¶ç»™å‡ºå®Œæ•´ä»£ç åŠç¤ºä¾‹ã€‚æ¯ä¸ªä»£ç å—éƒ½æŒ‰é¡ºåºæ‰§è¡Œï¼ŒåŒ…å«äº†è®­ç»ƒã€è¯„ä¼°ã€æ—¥å¿—è®°å½•ã€æŸå¤±æ›²çº¿ã€é¢„æµ‹ç»“æœå¯è§†åŒ–å’Œç»“æœå¯¼å‡ºç­‰åŠŸèƒ½ã€‚

### ğŸ“ **Notebook å®Œæ•´ä»£ç **

```python
# 1. å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import pandas as pd

# 2. ç”Ÿæˆç»“æ„åŒ–æ•°æ®
torch.manual_seed(0)
np.random.seed(0)

num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

# åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)

# 3. å®šä¹‰DNNå›å½’æ¨¡å‹
class DNNRegressor(nn.Module):
    def __init__(self, input_dim, hidden_layers=[64, 32], activation=nn.ReLU):
        super().__init__()
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_layers:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(activation())
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# 4. å®šä¹‰è®­ç»ƒå‡½æ•°
def train_model(model, X_train, y_train, epochs=100, lr=0.01, verbose=False):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    loss_history = []

    for epoch in range(epochs):
        model.train()
        pred = model(X_train)
        loss = criterion(pred, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loss_history.append(loss.item())

        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

    return loss_history

# 5. å®šä¹‰éªŒè¯å‡½æ•°
def evaluate_model(model, X_val, y_val):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).numpy()
        y_true = y_val.numpy()

    mae = mean_absolute_error(y_true, y_pred)
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)
    return {"mae": mae, "mse": mse, "r2": r2}

# 6. å®šä¹‰å¤šæ¨¡å‹è®­ç»ƒ/è¯„ä¼°/å¯¹æ¯”ç±»
class ModelRunner:
    def __init__(self, input_dim):
        self.input_dim = input_dim
        self.results = []

    def run(self, configs, X_train, y_train, X_val, y_val, epochs=100):
        for idx, config in enumerate(configs):
            print(f"\nğŸš€ æ¨¡å‹ {idx+1}/{len(configs)}: {config}")
            model = DNNRegressor(
                input_dim=self.input_dim,
                hidden_layers=config["hidden_layers"],
                activation=config.get("activation", nn.ReLU)
            )
            loss_history = train_model(model, X_train, y_train, epochs=epochs, lr=config["lr"])
            scores = evaluate_model(model, X_val, y_val)
            y_pred = model(X_val).detach().numpy()
            result = {
                "config": config,
                "scores": scores,
                "losses": loss_history,
                "y_true": y_val.numpy().flatten(),
                "y_pred": y_pred.flatten()
            }
            self.results.append(result)

    def summary(self, sort_by="r2"):
        print("\nğŸ“Š æ‰€æœ‰æ¨¡å‹è¡¨ç°ï¼š")
        sorted_results = sorted(self.results, key=lambda r: r["scores"][sort_by], reverse=True)
        for i, res in enumerate(sorted_results):
            cfg = res["config"]
            sc = res["scores"]
            print(f"[{i+1}] RÂ²={sc['r2']:.4f} | MAE={sc['mae']:.4f} | MSE={sc['mse']:.4f} | {cfg}")
        return sorted_results

    def plot_losses(self):
        plt.figure(figsize=(10, 6))
        for i, result in enumerate(self.results):
            label = f"Model {i+1}: {result['config']['hidden_layers']}, lr={result['config']['lr']}"
            plt.plot(result["losses"], label=label)
        plt.xlabel("Epoch")
        plt.ylabel("Train Loss (MSE)")
        plt.title("è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    def plot_predictions(self):
        for i, result in enumerate(self.results):
            plt.figure(figsize=(6, 6))
            plt.scatter(result["y_true"], result["y_pred"], alpha=0.6)
            plt.plot([min(result["y_true"]), max(result["y_true"])],
                     [min(result["y_true"]), max(result["y_true"])], 'r--')
            plt.xlabel("True Value")
            plt.ylabel("Predicted Value")
            plt.title(f"æ¨¡å‹ {i+1} é¢„æµ‹å¯¹æ¯”å›¾")
            plt.grid(True)
            plt.tight_layout()
            plt.show()

    def export_results(self, filepath="model_results.csv"):
        records = []
        for i, res in enumerate(self.results):
            record = {
                "model_id": i + 1,
                "r2": res["scores"]["r2"],
                "mae": res["scores"]["mae"],
                "mse": res["scores"]["mse"],
                "hidden_layers": str(res["config"]["hidden_layers"]),
                "lr": res["config"]["lr"]
            }
            records.append(record)
        df = pd.DataFrame(records)
        df.to_csv(filepath, index=False)
        print(f"âœ… æ¨¡å‹è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ï¼š{filepath}")

# 7. æ¨¡å‹è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    runner = ModelRunner(input_dim=num_features)

    param_grid = [
        {"hidden_layers": [64, 32], "lr": 0.01},
        {"hidden_layers": [128, 64], "lr": 0.01},
        {"hidden_layers": [64, 64, 32], "lr": 0.005},
    ]

    runner.run(param_grid, X_train, y_train, X_val, y_val, epochs=200)
    runner.summary()
    runner.plot_losses()
    runner.plot_predictions()
    runner.export_results("results_step7.csv")
```

------

### ğŸ“‘ **Notebook è¯´æ˜**

1. **å¯¼å…¥åº“**
    å¼•å…¥å¿…è¦çš„åº“ï¼Œå¦‚ `torch`ã€`sklearn`ã€`matplotlib`ã€`pandas` ç­‰ã€‚
2. **æ•°æ®ç”Ÿæˆä¸åˆ’åˆ†**
    ç”Ÿæˆéšæœºæ•°æ®å¹¶æ·»åŠ å™ªå£°ï¼Œç„¶ååˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚
3. **æ¨¡å‹å®šä¹‰**
    å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆDNNRegressorï¼‰ç”¨äºå›å½’ä»»åŠ¡ã€‚
4. **è®­ç»ƒå‡½æ•°**
    å®šä¹‰ `train_model` å‡½æ•°ï¼Œåˆ©ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚
5. **éªŒè¯å‡½æ•°**
    `evaluate_model` å‡½æ•°ç”¨äºè®¡ç®—å¹¶è¿”å›æ¨¡å‹çš„ MAEã€MSEã€RÂ² ç­‰æŒ‡æ ‡ã€‚
6. **æ¨¡å‹è®­ç»ƒä¸å¯¹æ¯”**
    ä½¿ç”¨ `ModelRunner` ç±»æ‰§è¡Œå¤šæ¨¡å‹è®­ç»ƒï¼Œè®°å½•æ¨¡å‹é…ç½®ã€è¯„ä¼°ç»“æœã€æŸå¤±æ›²çº¿ï¼Œå¹¶ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ›²çº¿å’Œé¢„æµ‹ç»“æœå¯¹æ¯”å›¾ã€‚
7. **ç»“æœå¯¼å‡º**
    å°†è®­ç»ƒå’Œè¯„ä¼°ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ï¼Œä¾¿äºåç»­çš„åˆ†æä¸å¯¹æ¯”ã€‚

------

### ğŸ§ª **æ‰§è¡Œæµç¨‹**

1. **è¿è¡Œæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ï¼š**
    ä½¿ç”¨ `runner.run()` è¿è¡Œä¸åŒçš„æ¨¡å‹é…ç½®ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºæ¯ä¸ªæ¨¡å‹çš„è¡¨ç°ã€‚
2. **æŸ¥çœ‹æ¨¡å‹æ€»ç»“ï¼š**
    `runner.summary()` å°†æ ¹æ® RÂ² å¯¹æ¨¡å‹è¿›è¡Œæ’åºï¼Œå¹¶è¾“å‡ºæ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ã€‚
3. **ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼š**
    `runner.plot_losses()` å°†å±•ç¤ºæ‰€æœ‰æ¨¡å‹è®­ç»ƒçš„æŸå¤±æ›²çº¿ï¼Œå¸®åŠ©åˆ¤æ–­å“ªä¸ªæ¨¡å‹æ”¶æ•›è¾ƒå¿«ã€‚
4. **ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”ï¼š**
    `runner.plot_predictions()` ç»˜åˆ¶é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„æ•£ç‚¹å›¾ï¼Œç”¨äºç›´è§‚è¯„ä¼°å›å½’æ•ˆæœã€‚
5. **å¯¼å‡ºç»“æœï¼š**
    `runner.export_results()` å°†æ¨¡å‹çš„è¯„ä¼°ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ï¼Œä¾¿äºåç»­æŸ¥çœ‹ã€‚

------

### âš¡ **åç»­æ­¥éª¤**

- **Step 8**ï¼šæ¨¡å‹ä¿å­˜ä¸åŠ è½½ï¼ˆ`torch.save()`ï¼‰
- **Step 9**ï¼šè°ƒå‚ï¼ˆGridSearch / RandomSearchï¼‰
- **Step 10**ï¼šåŠ å…¥æ—©åœï¼ˆEarlyStoppingï¼‰æœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜æˆ–å¸Œæœ›ç»§ç»­æ‰©å±•åç»­æ­¥éª¤ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼

å¥½çš„ï¼æ¥ä¸‹æ¥ï¼Œæˆ‘å°† **Step 8** æ•´ç†ä¸ºä¸€ä¸ª Jupyter Notebook ä½¿ç”¨çš„å®Œæ•´ä»£ç ï¼Œå¹¶ç»™å‡ºç›¸å…³ç¤ºä¾‹ï¼Œä¹‹åå†ç»§ç»­è¿›è¡Œ **Step 9**ã€‚

------

### ğŸ“‘ **Step 8: æ¨¡å‹ä¿å­˜ä¸åŠ è½½ - å®Œæ•´ Notebook**

```python
# 1. å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import pandas as pd

# 2. ç”Ÿæˆç»“æ„åŒ–æ•°æ®
torch.manual_seed(0)
np.random.seed(0)

num_samples = 200
num_features = 3

X = torch.randn(num_samples, num_features)
true_weights = torch.tensor([[1.5], [-2.0], [0.8]])
true_bias = 3.0
noise = 0.1 * torch.randn(num_samples, 1)

y = X @ true_weights + true_bias + noise

# åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(
    X.numpy(), y.numpy(), test_size=0.2, random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_val = torch.tensor(X_val, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32)

# 3. å®šä¹‰DNNå›å½’æ¨¡å‹
class DNNRegressor(nn.Module):
    def __init__(self, input_dim, hidden_layers=[64, 32], activation=nn.ReLU):
        super().__init__()
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_layers:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(activation())
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# 4. å®šä¹‰è®­ç»ƒå‡½æ•°
def train_model(model, X_train, y_train, epochs=100, lr=0.01, verbose=False):
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=lr)
    loss_history = []

    for epoch in range(epochs):
        model.train()
        pred = model(X_train)
        loss = criterion(pred, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loss_history.append(loss.item())

        if verbose and (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d} | Train Loss: {loss.item():.4f}")

    return loss_history

# 5. å®šä¹‰éªŒè¯å‡½æ•°
def evaluate_model(model, X_val, y_val):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_val).numpy()
        y_true = y_val.numpy()

    mae = mean_absolute_error(y_true, y_pred)
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)
    return {"mae": mae, "mse": mse, "r2": r2}

# 6. å®šä¹‰å¤šæ¨¡å‹è®­ç»ƒ/è¯„ä¼°/å¯¹æ¯”ç±»
class ModelRunner:
    def __init__(self, input_dim):
        self.input_dim = input_dim
        self.results = []

    def run(self, configs, X_train, y_train, X_val, y_val, epochs=100):
        for idx, config in enumerate(configs):
            print(f"\nğŸš€ æ¨¡å‹ {idx+1}/{len(configs)}: {config}")
            model = DNNRegressor(
                input_dim=self.input_dim,
                hidden_layers=config["hidden_layers"],
                activation=config.get("activation", nn.ReLU)
            )
            loss_history = train_model(model, X_train, y_train, epochs=epochs, lr=config["lr"])
            scores = evaluate_model(model, X_val, y_val)
            y_pred = model(X_val).detach().numpy()
            result = {
                "config": config,
                "scores": scores,
                "losses": loss_history,
                "y_true": y_val.numpy().flatten(),
                "y_pred": y_pred.flatten(),
                "model": model
            }
            self.results.append(result)

    def summary(self, sort_by="r2"):
        print("\nğŸ“Š æ‰€æœ‰æ¨¡å‹è¡¨ç°ï¼š")
        sorted_results = sorted(self.results, key=lambda r: r["scores"][sort_by], reverse=True)
        for i, res in enumerate(sorted_results):
            cfg = res["config"]
            sc = res["scores"]
            print(f"[{i+1}] RÂ²={sc['r2']:.4f} | MAE={sc['mae']:.4f} | MSE={sc['mse']:.4f} | {cfg}")
        return sorted_results

    def plot_losses(self):
        plt.figure(figsize=(10, 6))
        for i, result in enumerate(self.results):
            label = f"Model {i+1}: {result['config']['hidden_layers']}, lr={result['config']['lr']}"
            plt.plot(result["losses"], label=label)
        plt.xlabel("Epoch")
        plt.ylabel("Train Loss (MSE)")
        plt.title("è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    def plot_predictions(self):
        for i, result in enumerate(self.results):
            plt.figure(figsize=(6, 6))
            plt.scatter(result["y_true"], result["y_pred"], alpha=0.6)
            plt.plot([min(result["y_true"]), max(result["y_true"])],
                     [min(result["y_true"]), max(result["y_true"])], 'r--')
            plt.xlabel("True Value")
            plt.ylabel("Predicted Value")
            plt.title(f"æ¨¡å‹ {i+1} é¢„æµ‹å¯¹æ¯”å›¾")
            plt.grid(True)
            plt.tight_layout()
            plt.show()

    def export_results(self, filepath="model_results.csv"):
        records = []
        for i, res in enumerate(self.results):
            record = {
                "model_id": i + 1,
                "r2": res["scores"]["r2"],
                "mae": res["scores"]["mae"],
                "mse": res["scores"]["mse"],
                "hidden_layers": str(res["config"]["hidden_layers"]),
                "lr": res["config"]["lr"]
            }
            records.append(record)
        df = pd.DataFrame(records)
        df.to_csv(filepath, index=False)
        print(f"âœ… æ¨¡å‹è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ï¼š{filepath}")

    def save_model(self, model, filename="model.pth"):
        # ä¿å­˜æ¨¡å‹
        torch.save(model.state_dict(), filename)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {filename}")

    def load_model(self, model, filename="model.pth"):
        # åŠ è½½æ¨¡å‹
        model.load_state_dict(torch.load(filename))
        model.eval()
        print(f"âœ… æ¨¡å‹å·²ä» {filename} åŠ è½½")
        return model
```

------

### ğŸ§‘â€ğŸ’» **Notebook è¿è¡Œç¤ºä¾‹**

1. **æ¨¡å‹è®­ç»ƒå¹¶ä¿å­˜**

```python
if __name__ == "__main__":
    runner = ModelRunner(input_dim=num_features)

    param_grid = [
        {"hidden_layers": [64, 32], "lr": 0.01},
        {"hidden_layers": [128, 64], "lr": 0.01},
        {"hidden_layers": [64, 64, 32], "lr": 0.005},
    ]

    runner.run(param_grid, X_train, y_train, X_val, y_val, epochs=200)
    runner.summary()
    runner.plot_losses()
    runner.plot_predictions()
    runner.export_results("results_step8.csv")

    # ä¿å­˜ç¬¬ä¸€ä¸ªæ¨¡å‹
    best_model = runner.results[0]["model"]
    runner.save_model(best_model, filename="best_model.pth")
```

1. **åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹**

```python
# åŠ è½½æ¨¡å‹
runner.load_model(best_model, filename="best_model.pth")

# ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
y_pred = best_model(X_val).detach().numpy()
print(f"é¢„æµ‹ç»“æœï¼š\n{y_pred[:5]}")
```

------

### âœ… **åŠŸèƒ½æ€»ç»“**

| åŠŸèƒ½                 | è¯´æ˜                                            |
| -------------------- | ----------------------------------------------- |
| `save_model()`       | å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜ä¸ºæ–‡ä»¶                        |
| `load_model()`       | ä»æ–‡ä»¶åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹                          |
| `model.state_dict()` | ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹çš„å‚æ•°ï¼ˆæƒé‡ï¼‰                  |
| `model.eval()`       | è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼ï¼ˆå…³é—­ Dropout å’Œ BatchNormï¼‰ |

------

### âš¡ **Step 9ï¼šè‡ªåŠ¨è°ƒå‚ï¼ˆGridSearch / RandomSearchï¼‰**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­å®ç° **è‡ªåŠ¨è°ƒå‚** çš„åŠŸèƒ½ã€‚ä½¿ç”¨ `GridSearchCV` æˆ– `RandomSearch` æ¥è‡ªåŠ¨æœç´¢æœ€ä½³çš„è¶…å‚æ•°é…ç½®ï¼Œä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

å¦‚æœä½ å‡†å¤‡å¥½ç»§ç»­ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼
