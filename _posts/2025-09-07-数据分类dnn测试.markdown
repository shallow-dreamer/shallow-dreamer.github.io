---
layout:     post
title:      "æ•°æ®åˆ†ç±»dnnæµ‹è¯•"
subtitle:   ""
date:       2025-09-07
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - å­¦ä¹ æ¨¡å‹
---

å¥½äº† â€” æˆ‘å·²ç»ä¸ºä½ å†™å¥½å¹¶è¿è¡Œäº†ä¸€ä¸ª**å¯ç«‹å³è¿è¡Œã€ä»…ä¾èµ– NumPy çš„åŠ¨æ€åˆ†ç±»ç¥ç»ç½‘ç»œç¤ºä¾‹**ã€‚ç‰¹ç‚¹ä¸è¯´æ˜ï¼š

- æ”¯æŒç”¨æˆ·é€šè¿‡ `config` åŠ¨æ€ä¼ å…¥ç½‘ç»œå±‚çº§ï¼ˆæ¯å±‚çš„ units ä¸ activationï¼‰ã€è®­ç»ƒå‚æ•°ï¼ˆepochsã€batch_sizeã€learning_rate ç­‰ï¼‰æ¥æ„å»ºå¹¶è®­ç»ƒç½‘ç»œã€‚
- æ²¡æœ‰ä¾èµ– PyTorch / TensorFlowï¼Œçº¯ NumPy å®ç°ï¼Œä¿è¯åœ¨å¤§å¤šæ•° Python ç¯å¢ƒä¸­ç«‹å³å¯è¿è¡Œã€‚
- é™„å¸¦åˆæˆçš„å¤šç±»é«˜æ–¯æ•°æ®é›†ï¼ˆå¯è°ƒæ•´æ ·æœ¬æ•°ä¸ç±»åˆ«æ•°ï¼‰ï¼Œå¹¶ç»™å‡ºè®­ç»ƒæ›²çº¿ï¼ˆLoss / Accuracyï¼‰ã€‚
- æ¨¡å‹å‚æ•°ä¿å­˜åˆ° `/mnt/data/dynamic_nn_demo_params.npy`ï¼ˆå·²ä¿å­˜ï¼‰ã€‚ä½ å¯ä»¥ä¸‹è½½å¹¶åœ¨éœ€è¦æ—¶åŠ è½½ã€‚

æˆ‘å·²åœ¨è¿è¡Œç¯å¢ƒä¸­è®­ç»ƒå¹¶ä¿å­˜äº†æ¨¡å‹ â€”â€” ä¸‹è½½è·¯å¾„ï¼š
 [ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼ˆdynamic_nn_demo_params.npyï¼‰](sandbox:/mnt/data/dynamic_nn_demo_params.npy)

ä¸‹é¢æ˜¯ä½ å¯ä»¥ç›´æ¥å¤åˆ¶è¿è¡Œï¼ˆæˆ–åœ¨ notebook ä¸­æ‰§è¡Œï¼‰çš„å®Œæ•´è„šæœ¬ï¼ˆä¸æˆ‘è¿è¡Œçš„ä»£ç ç›¸åŒï¼‰ã€‚ä½ å¯ä»¥ç›´æ¥ä¿®æ”¹ `config` ä¸­çš„é¡¹ä»¥è‡ªå®šä¹‰ç½‘ç»œä¸è®­ç»ƒè¶…å‚æ•°ã€‚

```python
# Dynamic neural network implemented with NumPy (no external DL libs required).
# Copy & run this whole file in a Python environment or notebook.

import numpy as np
import matplotlib.pyplot as plt
import os

def one_hot(labels, num_classes):
    oh = np.zeros((labels.size, num_classes))
    oh[np.arange(labels.size), labels] = 1
    return oh

def accuracy(pred_probs, labels):
    preds = np.argmax(pred_probs, axis=1)
    return np.mean(preds == labels)

class Activations:
    @staticmethod
    def relu(x):
        return np.maximum(0, x)
    @staticmethod
    def drelu(x):
        return (x > 0).astype(float)
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
    @staticmethod
    def dsigmoid(x):
        s = Activations.sigmoid(x)
        return s * (1 - s)
    @staticmethod
    def tanh(x):
        return np.tanh(x)
    @staticmethod
    def dtanh(x):
        return 1 - np.tanh(x)**2
    @staticmethod
    def softmax(x):
        x = x - np.max(x, axis=1, keepdims=True)
        e = np.exp(x)
        return e / np.sum(e, axis=1, keepdims=True)

class Layer:
    def __init__(self, in_dim, out_dim, activation='relu'):
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.activation = activation
        # init
        if activation == 'relu':
            scale = np.sqrt(2.0 / in_dim)
        elif activation in ('tanh', 'sigmoid'):
            scale = np.sqrt(1.0 / in_dim)
        else:
            scale = 0.01
        self.W = np.random.randn(in_dim, out_dim) * scale
        self.b = np.zeros((1, out_dim))
        self.x = None
        self.z = None

    def forward(self, x):
        self.x = x
        self.z = x.dot(self.W) + self.b
        if self.activation == 'relu':
            return Activations.relu(self.z)
        elif self.activation == 'sigmoid':
            return Activations.sigmoid(self.z)
        elif self.activation == 'tanh':
            return Activations.tanh(self.z)
        elif self.activation == 'linear':
            return self.z
        else:
            raise ValueError(f"Unsupported activation: {self.activation}")

    def backward(self, grad_out):
        if self.activation == 'relu':
            grad_z = grad_out * Activations.drelu(self.z)
        elif self.activation == 'sigmoid':
            grad_z = grad_out * Activations.dsigmoid(self.z)
        elif self.activation == 'tanh':
            grad_z = grad_out * Activations.dtanh(self.z)
        elif self.activation == 'linear':
            grad_z = grad_out
        else:
            raise ValueError(f"Unsupported activation: {self.activation}")
        m = self.x.shape[0]
        dW = self.x.T.dot(grad_z) / m
        db = np.sum(grad_z, axis=0, keepdims=True) / m
        dX = grad_z.dot(self.W.T)
        return dX, dW, db

class DynamicNN:
    def __init__(self, input_dim, layer_specs, num_classes):
        self.layers = []
        in_dim = input_dim
        for spec in layer_specs:
            self.layers.append(Layer(in_dim, spec['units'], spec.get('activation', 'relu')))
            in_dim = spec['units']
        self.out_layer = Layer(in_dim, num_classes, activation='linear')
        self.num_classes = num_classes

    def forward(self, x):
        out = x
        for layer in self.layers:
            out = layer.forward(out)
        logits = self.out_layer.forward(out)
        probs = Activations.softmax(logits)
        return logits, probs

    def compute_loss_and_grad(self, x, y_onehot):
        logits, probs = self.forward(x)
        m = x.shape[0]
        eps = 1e-12
        loss = -np.sum(y_onehot * np.log(probs + eps)) / m
        grad_logits = (probs - y_onehot) / m
        grad, dW, db = self.out_layer.backward(grad_logits)
        grads = [{'layer': 'out', 'dW': dW, 'db': db}]
        for layer in reversed(self.layers):
            grad, dW, db = layer.backward(grad)
            grads.append({'layer': layer, 'dW': dW, 'db': db})
        return loss, probs, grads

    def step(self, grads, lr):
        out_grad = grads[0]
        self.out_layer.W -= lr * out_grad['dW']
        self.out_layer.b -= lr * out_grad['db']
        for g, layer in zip(grads[1:], reversed(self.layers)):
            layer.W -= lr * g['dW']
            layer.b -= lr * g['db']

    def predict(self, x):
        _, probs = self.forward(x)
        return np.argmax(probs, axis=1), probs

    def save(self, path):
        params = {
            'out_W': self.out_layer.W,
            'out_b': self.out_layer.b,
            'layers': [(l.W, l.b, l.activation) for l in self.layers],
            'num_classes': self.num_classes
        }
        np.save(path, params, allow_pickle=True)

    def load(self, path):
        d = np.load(path, allow_pickle=True).item()
        self.out_layer.W = d['out_W']
        self.out_layer.b = d['out_b']
        layers_data = d['layers']
        for l, data in zip(self.layers, layers_data):
            W, b, activation = data
            l.W = W
            l.b = b
            l.activation = activation

def generate_gaussian_blobs(n_samples=1500, centers=3, dim=2, seed=42, std=1.2):
    rng = np.random.RandomState(seed)
    samples_per = n_samples // centers
    X = []
    y = []
    angles = np.linspace(0, 2*np.pi, centers, endpoint=False)
    for i, a in enumerate(angles):
        center = np.array([4*np.cos(a), 4*np.sin(a)])
        X.append(rng.randn(samples_per, dim) * std + center)
        y.append(np.full(samples_per, i))
    X = np.vstack(X)
    y = np.concatenate(y)
    perm = rng.permutation(X.shape[0])
    return X[perm], y[perm]

def train_model(config):
    input_dim = config['input_dim']
    layer_specs = config['layer_specs']
    num_classes = config['num_classes']
    epochs = config.get('epochs', 30)
    batch_size = config.get('batch_size', 64)
    lr = config.get('learning_rate', 0.01)
    seed = config.get('seed', 42)
    verbose = config.get('verbose', True)

    X, y = generate_gaussian_blobs(n_samples=config.get('n_samples', 1200), centers=num_classes, dim=input_dim, seed=seed)
    split = int(0.8 * X.shape[0])
    X_train, y_train = X[:split], y[:split]
    X_test, y_test = X[split:], y[split:]
    y_train_oh = one_hot(y_train, num_classes)
    y_test_oh = one_hot(y_test, num_classes)

    mu = X_train.mean(axis=0, keepdims=True)
    sigma = X_train.std(axis=0, keepdims=True) + 1e-8
    X_train = (X_train - mu) / sigma
    X_test = (X_test - mu) / sigma

    model = DynamicNN(input_dim, layer_specs, num_classes)
    steps_per_epoch = int(np.ceil(X_train.shape[0] / batch_size))
    history = {'loss': [], 'train_acc': [], 'test_acc': []}
    rng = np.random.RandomState(seed)
    for ep in range(1, epochs+1):
        perm = rng.permutation(X_train.shape[0])
        X_train_sh = X_train[perm]
        y_train_oh_sh = y_train_oh[perm]
        epoch_loss = 0.0
        for step in range(steps_per_epoch):
            start = step * batch_size
            end = start + batch_size
            xb = X_train_sh[start:end]
            yb = y_train_oh_sh[start:end]
            loss, probs, grads = model.compute_loss_and_grad(xb, yb)
            model.step(grads, lr)
            epoch_loss += loss * xb.shape[0]
        epoch_loss /= X_train.shape[0]
        _, train_probs = model.forward(X_train)
        train_acc = accuracy(train_probs, y_train)
        _, test_probs = model.forward(X_test)
        test_acc = accuracy(test_probs, y_test)
        history['loss'].append(epoch_loss)
        history['train_acc'].append(train_acc)
        history['test_acc'].append(test_acc)
        if verbose and (ep % max(1, epochs//6) == 0 or ep == 1 or ep == epochs):
            print(f"Epoch {ep}/{epochs} - loss: {epoch_loss:.4f} - train_acc: {train_acc:.4f} - test_acc: {test_acc:.4f}")

    save_path = "/mnt/data/dynamic_nn_demo_params.npy"
    model.save(save_path)
    if verbose:
        print(f"\nModel saved to: {save_path}")

    plt.figure(figsize=(8,4))
    plt.subplot(1,2,1)
    plt.plot(history['loss'])
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.subplot(1,2,2)
    plt.plot(history['train_acc'], label='train')
    plt.plot(history['test_acc'], label='test')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.tight_layout()
    plt.show()

    preds, probs = model.predict(X_test)
    final_acc = accuracy(probs, y_test)
    print(f"Final test accuracy: {final_acc:.4f} on {X_test.shape[0]} samples")
    return model, history, save_path, (X_test, y_test, preds, probs)

# Example config â€” change this to experiment:
config = {
    'input_dim': 2,
    'layer_specs': [
        {'units': 64, 'activation': 'relu'},
        {'units': 32, 'activation': 'relu'},
    ],
    'num_classes': 3,
    'epochs': 30,
    'batch_size': 64,
    'learning_rate': 0.05,
    'n_samples': 1200,
    'seed': 123,
    'verbose': True
}

model, history, save_path, test_info = train_model(config)
```

å¥½çš„ ğŸ‘
 æˆ‘å¸®ä½ æŠŠä¸Šé¢ NumPy å®ç°çš„ **åŠ¨æ€åˆ†ç±»æ¨¡å‹** æ”¹å†™æˆ **PyTorch** ç‰ˆæœ¬ï¼Œä¿æŒåŸæ¥â€œç”¨æˆ·å¯é€šè¿‡é…ç½®åŠ¨æ€æŒ‡å®šå±‚çº§å’Œè®­ç»ƒå‚æ•°â€çš„è®¾è®¡ã€‚è¿™ä¸ªç‰ˆæœ¬æ”¯æŒ GPUï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œå¹¶ä¸”ä»£ç æ›´åŠ ç®€æ´ï¼ˆè‡ªåŠ¨æ±‚å¯¼å’Œä¼˜åŒ–å™¨ï¼‰ã€‚

ä¸‹é¢æ˜¯å®Œæ•´ä»£ç ï¼ˆå¯ç«‹å³è¿è¡Œï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# ---------- æ•°æ®ç”Ÿæˆ ----------
def generate_gaussian_blobs(n_samples=1500, centers=3, dim=2, seed=42, std=1.2):
    rng = np.random.RandomState(seed)
    samples_per = n_samples // centers
    X = []
    y = []
    angles = np.linspace(0, 2*np.pi, centers, endpoint=False)
    for i, a in enumerate(angles):
        center = np.array([4*np.cos(a), 4*np.sin(a)])
        X.append(rng.randn(samples_per, dim) * std + center)
        y.append(np.full(samples_per, i))
    X = np.vstack(X)
    y = np.concatenate(y)
    perm = rng.permutation(X.shape[0])
    return X[perm], y[perm]

# ---------- åŠ¨æ€æ¨¡å‹ ----------
class DynamicNN(nn.Module):
    def __init__(self, input_dim, layer_specs, num_classes):
        super().__init__()
        layers = []
        in_dim = input_dim
        for spec in layer_specs:
            layers.append(nn.Linear(in_dim, spec["units"]))
            if spec["activation"] == "relu":
                layers.append(nn.ReLU())
            elif spec["activation"] == "tanh":
                layers.append(nn.Tanh())
            elif spec["activation"] == "sigmoid":
                layers.append(nn.Sigmoid())
            # linear åˆ™ä¸åŠ æ¿€æ´»
            in_dim = spec["units"]
        layers.append(nn.Linear(in_dim, num_classes))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# ---------- è®­ç»ƒå‡½æ•° ----------
def train_model(config):
    # æ•°æ®
    X, y = generate_gaussian_blobs(
        n_samples=config.get("n_samples", 1200),
        centers=config["num_classes"],
        dim=config["input_dim"],
        seed=config.get("seed", 42),
    )
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=config.get("seed", 42))

    # è½¬ tensor
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train = torch.tensor(y_train, dtype=torch.long).to(device)
    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test = torch.tensor(y_test, dtype=torch.long).to(device)

    # æ¨¡å‹
    model = DynamicNN(config["input_dim"], config["layer_specs"], config["num_classes"]).to(device)
    optimizer = optim.Adam(model.parameters(), lr=config.get("learning_rate", 0.01))
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒ
    epochs = config.get("epochs", 30)
    batch_size = config.get("batch_size", 64)
    history = {"loss": [], "train_acc": [], "test_acc": []}

    for ep in range(1, epochs + 1):
        model.train()
        perm = torch.randperm(X_train.size(0))
        epoch_loss = 0
        correct = 0

        for i in range(0, X_train.size(0), batch_size):
            idx = perm[i : i + batch_size]
            xb, yb = X_train[idx], y_train[idx]

            optimizer.zero_grad()
            out = model(xb)
            loss = criterion(out, yb)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item() * xb.size(0)
            correct += (out.argmax(1) == yb).sum().item()

        train_acc = correct / X_train.size(0)
        epoch_loss /= X_train.size(0)

        # æµ‹è¯•é›†
        model.eval()
        with torch.no_grad():
            out_test = model(X_test)
            test_acc = (out_test.argmax(1) == y_test).float().mean().item()

        history["loss"].append(epoch_loss)
        history["train_acc"].append(train_acc)
        history["test_acc"].append(test_acc)

        if ep % max(1, epochs // 6) == 0 or ep == 1 or ep == epochs:
            print(f"Epoch {ep}/{epochs} - loss: {epoch_loss:.4f} - train_acc: {train_acc:.4f} - test_acc: {test_acc:.4f}")

    # ç”»å›¾
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history["loss"])
    plt.title("Loss")
    plt.subplot(1, 2, 2)
    plt.plot(history["train_acc"], label="train")
    plt.plot(history["test_acc"], label="test")
    plt.legend()
    plt.title("Accuracy")
    plt.show()

    return model, history, (X_test, y_test)

# ---------- æµ‹è¯•è¿è¡Œ ----------
if __name__ == "__main__":
    config = {
        "input_dim": 2,
        "layer_specs": [
            {"units": 64, "activation": "relu"},
            {"units": 32, "activation": "tanh"},
        ],
        "num_classes": 3,
        "epochs": 30,
        "batch_size": 64,
        "learning_rate": 0.01,
        "n_samples": 1200,
        "seed": 123,
    }
    model, history, test_data = train_model(config)
```

è¿è¡Œåä½ ä¼šçœ‹åˆ° **loss æ›²çº¿**å’Œ**è®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡æ›²çº¿**ï¼Œå‡†ç¡®ç‡ä¸€èˆ¬å¯ä»¥åˆ° 95% ä»¥ä¸Šã€‚
 ä½ å¯ä»¥è‡ªç”±ä¿®æ”¹ `config["layer_specs"]` æ¥æ­å»ºä¸åŒæ·±åº¦/æ¿€æ´»å‡½æ•°çš„ç½‘ç»œã€‚

æ˜ç™½ âœ…
 ä½ å¸Œæœ›åšçš„æ˜¯ï¼š

- ç”¨ **Excel ç±»ä¼¼çš„è¡¨æ ¼æ•°æ®** åšåˆ†ç±»ä»»åŠ¡ï¼›
- æ•°æ®ç”¨ **pandas DataFrame** æ¥ç”Ÿæˆï¼ˆæ¨¡æ‹Ÿ Excelï¼‰ï¼Œè€Œä¸æ˜¯å®é™…è¯» `.xlsx` æ–‡ä»¶ï¼›
- è®­ç»ƒå’Œæµ‹è¯•æ—¶èƒ½è·‘é€šï¼Œå¹¶ä¸”èƒ½è¾“å‡ºé¢„æµ‹ç»“æœã€‚

æˆ‘ç»™ä½ ä¸€ä¸ª **PyTorch åˆ†ç±»æ¨¡å‹å®Œæ•´ç¤ºä¾‹**ï¼ŒåŒ…æ‹¬ï¼š

1. ç”¨ `pandas` ç”Ÿæˆæ¨¡æ‹Ÿè¡¨æ ¼æ•°æ®ï¼ˆæ··åˆæ•°å€¼å‹å’Œç±»åˆ«å‹ï¼‰ï¼›
2. å¯¹ç±»åˆ«å‹ç‰¹å¾åš **One-Hot ç¼–ç **ï¼›
3. ç”¨ `train_test_split` åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼›
4. ç”¨ PyTorch å®šä¹‰ä¸€ä¸ªåŠ¨æ€çš„ MLP åˆ†ç±»æ¨¡å‹ï¼›
5. è®­ç»ƒå¹¶è¾“å‡º **é¢„æµ‹ç»“æœ**ã€‚

------

## ğŸ”¥ ä»£ç ç¤ºä¾‹

```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. ç”Ÿæˆæ¨¡æ‹ŸExcelæ•°æ®ï¼ˆDataFrameï¼‰
np.random.seed(42)
N = 300
df = pd.DataFrame({
    "feature_num1": np.random.randn(N),             # æ•°å€¼ç‰¹å¾1
    "feature_num2": np.random.rand(N) * 10,         # æ•°å€¼ç‰¹å¾2
    "feature_cat": np.random.choice(["A", "B", "C"], size=N),  # ç±»åˆ«ç‰¹å¾
    "label": np.random.choice([0, 1, 2], size=N)    # åˆ†ç±»æ ‡ç­¾ï¼ˆ3ç±»ï¼‰
})

print("ç¤ºä¾‹æ•°æ®ï¼š")
print(df.head())

# 2. å¤„ç†ç±»åˆ«å‹ç‰¹å¾ï¼ˆOne-Hotï¼‰
df = pd.get_dummies(df, columns=["feature_cat"], drop_first=True)

# ç‰¹å¾å’Œæ ‡ç­¾
X = df.drop("label", axis=1).values
y = df["label"].values

# æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾ï¼ˆæ›´åˆ©äºè®­ç»ƒï¼‰
scaler = StandardScaler()
X = scaler.fit_transform(X)

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è½¬ä¸ºTensor
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# 3. å®šä¹‰åŠ¨æ€ç¥ç»ç½‘ç»œ
class DynamicNN(nn.Module):
    def __init__(self, input_dim, layer_specs, num_classes):
        super().__init__()
        layers = []
        in_dim = input_dim
        for spec in layer_specs:
            layers.append(nn.Linear(in_dim, spec["units"]))
            if spec["activation"] == "relu":
                layers.append(nn.ReLU())
            elif spec["activation"] == "tanh":
                layers.append(nn.Tanh())
            elif spec["activation"] == "sigmoid":
                layers.append(nn.Sigmoid())
            in_dim = spec["units"]
        layers.append(nn.Linear(in_dim, num_classes))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# é…ç½®ç½‘ç»œ
config = {
    "input_dim": X_train.shape[1],
    "layer_specs": [
        {"units": 32, "activation": "relu"},
        {"units": 16, "activation": "tanh"},
    ],
    "num_classes": len(np.unique(y)),
    "epochs": 30,
    "batch_size": 32,
    "learning_rate": 0.01
}

# 4. æ¨¡å‹è®­ç»ƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DynamicNN(config["input_dim"], config["layer_specs"], config["num_classes"]).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=config["learning_rate"])

X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

for epoch in range(1, config["epochs"] + 1):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    if epoch % 5 == 0 or epoch == 1 or epoch == config["epochs"]:
        _, preds_train = torch.max(outputs, 1)
        acc_train = (preds_train == y_train).float().mean().item()

        model.eval()
        with torch.no_grad():
            outputs_test = model(X_test)
            _, preds_test = torch.max(outputs_test, 1)
            acc_test = (preds_test == y_test).float().mean().item()

        print(f"Epoch {epoch}/{config['epochs']} | Loss: {loss.item():.4f} | Train Acc: {acc_train:.4f} | Test Acc: {acc_test:.4f}")

# 5. é¢„æµ‹ç»“æœ
model.eval()
with torch.no_grad():
    probs = torch.softmax(model(X_test), dim=1)
    preds = probs.argmax(dim=1)

print("\n=== æµ‹è¯•é›†é¢„æµ‹ç»“æœç¤ºä¾‹ ===")
for i in range(5):
    print(f"çœŸå®: {y_test[i].item()} | é¢„æµ‹: {preds[i].item()} | æ¦‚ç‡åˆ†å¸ƒ: {probs[i].cpu().numpy()}")
```

------

## ğŸš€ è¯´æ˜

1. **æ•°æ®ç”Ÿæˆ**
   - æ•°å€¼ç‰¹å¾ï¼š`feature_num1, feature_num2`
   - ç±»åˆ«ç‰¹å¾ï¼š`feature_cat`ï¼ˆA/B/Cï¼Œè½¬ä¸º One-Hotï¼‰
   - æ ‡ç­¾ï¼š`label`ï¼ˆä¸‰åˆ†ç±»ï¼š0/1/2ï¼‰
2. **é¢„å¤„ç†**
   - `pd.get_dummies` è‡ªåŠ¨æŠŠç±»åˆ«ç‰¹å¾è½¬æˆæ•°å€¼åˆ—
   - `StandardScaler` è®©æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
3. **æ¨¡å‹**
   - `layer_specs` å†³å®šéšè—å±‚ç»“æ„ï¼ˆä½ å¯ä»¥éšæ„æ”¹ï¼‰
   - è‡ªåŠ¨æ”¯æŒäºŒåˆ†ç±»ã€å¤šåˆ†ç±»
4. **è¾“å‡º**
   - æ‰“å°è®­ç»ƒ & æµ‹è¯•é›†å‡†ç¡®ç‡
   - è¾“å‡ºæµ‹è¯•é›†å‰ 5 ä¸ªæ ·æœ¬çš„ **çœŸå®ç±»åˆ«ã€é¢„æµ‹ç±»åˆ«ã€é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ**

æ˜ç™½å•¦ ğŸ‘
 ä½ å¸Œæœ›é€šè¿‡ä¸€ä¸ª **åˆ—è¡¨ï¼ˆå±‚çº§é…ç½®ï¼‰** æ¥æ§åˆ¶ç¥ç»ç½‘ç»œçš„éšè—å±‚ç»“æ„ï¼Œæ¯”å¦‚ï¼š

```python
layers = [64, 32, 16]
```

è¡¨ç¤ºï¼š

- è¾“å…¥å±‚ â†’ éšè—å±‚1ï¼ˆ64ä¸ªç¥ç»å…ƒï¼‰
- éšè—å±‚1 â†’ éšè—å±‚2ï¼ˆ32ä¸ªç¥ç»å…ƒï¼‰
- éšè—å±‚2 â†’ éšè—å±‚3ï¼ˆ16ä¸ªç¥ç»å…ƒï¼‰
- æœ€ç»ˆè¿æ¥åˆ°è¾“å‡ºå±‚ï¼ˆåˆ†ç±»æ•°ï¼‰

ä¸‹é¢æˆ‘ç»™ä½ ä¸€ä¸ª **å®Œæ•´çš„ PyTorch åˆ†ç±»æ¨¡å‹è®­ç»ƒä»£ç **ï¼Œæ”¯æŒç”¨ `layers=[]` åŠ¨æ€æ„å»ºç½‘ç»œï¼Œç”¨ `pandas` ç”Ÿæˆæ¨¡æ‹Ÿ Excel æ•°æ®ï¼ˆä¸ç”¨çœŸå® Excel æ–‡ä»¶ï¼‰ï¼Œå¹¶å±•ç¤ºè®­ç»ƒ+é¢„æµ‹ç»“æœã€‚

------

## ğŸ“Œ åŠ¨æ€åˆ†ç±»æ¨¡å‹ä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np

# ===== 1. æ¨¡æ‹ŸExcelæ•°æ®ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰ =====
np.random.seed(42)
num_samples = 500
num_features = 10
num_classes = 3

# éšæœºç”Ÿæˆç‰¹å¾
X = np.random.randn(num_samples, num_features)

# æ„é€ ä¸€ä¸ªç›®æ ‡åˆ— y ï¼ˆ0,1,2 ä¸‰åˆ†ç±»ï¼‰
y = np.random.randint(0, num_classes, size=num_samples)

# è½¬æˆ DataFrameï¼Œæ¨¡æ‹Ÿ Excel è¯»è¿›æ¥çš„æ ·å­
df = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(num_features)])
df["label"] = y
print("ğŸ“Š æ¨¡æ‹ŸExcelæ•°æ®ç¤ºä¾‹ï¼š")
print(df.head())

# ===== 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† =====
X_train, X_test, y_train, y_test = train_test_split(
    df.drop("label", axis=1).values,
    df["label"].values,
    test_size=0.2,
    random_state=42
)

# è½¬æˆå¼ é‡
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# ===== 3. åŠ¨æ€ç¥ç»ç½‘ç»œæ¨¡å‹ =====
class DynamicNN(nn.Module):
    def __init__(self, input_dim, hidden_layers, num_classes):
        super(DynamicNN, self).__init__()
        layers = []
        in_dim = input_dim

        # åŠ¨æ€ç”Ÿæˆéšè—å±‚
        for h in hidden_layers:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(in_dim, num_classes))

        # ä½¿ç”¨Sequentialå®¹å™¨
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ===== 4. è®­ç»ƒé…ç½® =====
hidden_layers = [64, 32]  # ä½ å¯ä»¥æ”¹æˆ []
model = DynamicNN(input_dim=num_features, hidden_layers=hidden_layers, num_classes=num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ===== 5. è®­ç»ƒ =====
epochs = 50
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# ===== 6. æµ‹è¯• & é¢„æµ‹ =====
with torch.no_grad():
    y_pred = model(X_test).argmax(dim=1)

print("\nğŸ“ˆ åˆ†ç±»ç»“æœæŠ¥å‘Šï¼š")
print(classification_report(y_test, y_pred))

# éšæœºå–å‰10æ¡é¢„æµ‹ç»“æœ
df_results = pd.DataFrame({
    "çœŸå®å€¼": y_test.numpy()[:10],
    "é¢„æµ‹å€¼": y_pred.numpy()[:10]
})
print("\nğŸ”® å‰10æ¡é¢„æµ‹ç»“æœï¼š")
print(df_results)
```

------

## ğŸ“Œ ä»£ç è¯´æ˜

1. **`hidden_layers`**

   - ä½ å¯ä»¥ä¼ ä¸€ä¸ªåˆ—è¡¨ï¼Œå®šä¹‰éšè—å±‚çš„ç»“æ„ï¼Œæ¯”å¦‚ï¼š

     ```python
     hidden_layers = [128, 64, 32]
     ```

     æˆ–è€…ä¸éœ€è¦éšè—å±‚ï¼š

     ```python
     hidden_layers = []
     ```

2. **è¾“å…¥æ•°æ®**

   - ç”¨ `pandas` æ¨¡æ‹Ÿäº†ä¸€ä¸ª Excel è¡¨æ ¼ï¼ˆ10ä¸ªç‰¹å¾ + 1ä¸ªåˆ†ç±»æ ‡ç­¾ï¼‰ã€‚
   - ä½ ä»¥åå¯ä»¥ç›´æ¥ç”¨ `pd.read_excel("yourfile.xlsx")` æ›¿æ¢ã€‚

3. **è¾“å‡ºç»“æœ**

   - è®­ç»ƒè¿‡ç¨‹æ‰“å° `Loss`ã€‚
   - æœ€ç»ˆæ‰“å°åˆ†ç±»æŒ‡æ ‡ï¼ˆprecisionã€recallã€f1ï¼‰ã€‚
   - å¹¶å±•ç¤ºå‰10æ¡çš„é¢„æµ‹ç»“æœã€‚

------

å¥½ğŸ‘Œï¼Œæˆ‘æ¥å¸®ä½ æŠŠè¿™ä¸ªåŠ¨æ€åˆ†ç±»æ¨¡å‹å‡çº§ï¼Œè®©å®ƒèƒ½å¤„ç† **å­—ç¬¦å‹æ•°æ®ï¼ˆç±»åˆ«å­—æ®µï¼‰**ï¼ŒåŒæ—¶ä¿æŒå¯¹ **å¤šåˆ†ç±»ä»»åŠ¡** çš„é€‚åº”æ€§ã€‚

æ ¸å¿ƒæ€è·¯æ˜¯ï¼š

1. **å­—ç¬¦ç‰¹å¾ â†’ æ•°å€¼åŒ–**ï¼šç”¨ `LabelEncoder` æˆ– `OneHotEncoder` è½¬æ¢æˆæ•°å­—ã€‚
2. **åŠ¨æ€æ„å»ºç½‘ç»œ**ï¼š`hidden_layers = []` æˆ– `[64, 32, ...]` è‡ªç”±é€‰æ‹©ã€‚
3. **è‡ªåŠ¨è¯†åˆ«åˆ†ç±»æ•°é‡**ï¼šæ¨¡å‹æ ¹æ® `y` çš„ä¸åŒç±»åˆ«æ•°è‡ªåŠ¨è°ƒæ•´è¾“å‡ºå±‚ã€‚

------

## ğŸ“Œ ä»£ç ç¤ºä¾‹ï¼ˆå­—ç¬¦åˆ†ç±» + å¤šåˆ†ç±»åœºæ™¯ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import pandas as pd
import numpy as np

# ===== 1. æ¨¡æ‹Ÿå­—ç¬¦å‹åˆ†ç±»æ•°æ® =====
np.random.seed(42)
num_samples = 300

# å‡è®¾æœ‰ 3 ä¸ªç±»åˆ«ç‰¹å¾ï¼ˆé¢œè‰², å½¢çŠ¶, å¤§å°ï¼‰
colors = np.random.choice(["red", "blue", "green"], size=num_samples)
shapes = np.random.choice(["circle", "square", "triangle"], size=num_samples)
sizes = np.random.choice(["small", "medium", "large"], size=num_samples)

# æ ‡ç­¾ï¼šä¸‰åˆ†ç±»
labels = np.random.choice(["A", "B", "C"], size=num_samples)

# è½¬æˆ DataFrameï¼ˆæ¨¡æ‹Ÿ Excelï¼‰
df = pd.DataFrame({
    "color": colors,
    "shape": shapes,
    "size": sizes,
    "label": labels
})
print("ğŸ“Š æ¨¡æ‹ŸExcelæ•°æ®ç¤ºä¾‹ï¼š")
print(df.head())

# ===== 2. ç‰¹å¾ç¼–ç ï¼ˆå­—ç¬¦è½¬æ•°å­—ï¼‰ =====
# LabelEncoderï¼šæ¯åˆ—ç‹¬ç«‹ç¼–ç 
encoders = {}
for col in ["color", "shape", "size"]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoders[col] = le  # ä¿å­˜ç¼–ç å™¨ï¼Œæ–¹ä¾¿ä¹‹åé¢„æµ‹æ–°æ•°æ®

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["label"])
num_classes = len(label_encoder.classes_)

# ===== 3. åˆ’åˆ†æ•°æ® =====
X_train, X_test, y_train, y_test = train_test_split(
    df.drop("label", axis=1).values,
    df["label"].values,
    test_size=0.2,
    random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# ===== 4. åŠ¨æ€ç¥ç»ç½‘ç»œæ¨¡å‹ =====
class DynamicNN(nn.Module):
    def __init__(self, input_dim, hidden_layers, num_classes):
        super(DynamicNN, self).__init__()
        layers = []
        in_dim = input_dim

        for h in hidden_layers:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h

        layers.append(nn.Linear(in_dim, num_classes))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ===== 5. åˆå§‹åŒ–æ¨¡å‹ =====
hidden_layers = [32, 16]   # åŠ¨æ€è®¾ç½®ï¼Œæˆ–è€… []
model = DynamicNN(input_dim=X_train.shape[1], hidden_layers=hidden_layers, num_classes=num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ===== 6. è®­ç»ƒ =====
epochs = 50
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# ===== 7. æµ‹è¯• & é¢„æµ‹ =====
with torch.no_grad():
    y_pred = model(X_test).argmax(dim=1)

print("\nğŸ“ˆ åˆ†ç±»ç»“æœæŠ¥å‘Šï¼š")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# å‰10æ¡é¢„æµ‹ç»“æœ
df_results = pd.DataFrame({
    "çœŸå®å€¼": label_encoder.inverse_transform(y_test.numpy()[:10]),
    "é¢„æµ‹å€¼": label_encoder.inverse_transform(y_pred.numpy()[:10])
})
print("\nğŸ”® å‰10æ¡é¢„æµ‹ç»“æœï¼š")
print(df_results)
```

------

## ğŸ“Œ ä»£ç è¦ç‚¹

1. **æ”¯æŒå­—ç¬¦ç‰¹å¾**
   - `LabelEncoder` æŠŠ `["red", "blue", "green"] â†’ [0,1,2]`ã€‚
   - æ¯ä¸ªå­—æ®µç‹¬ç«‹ç¼–ç ï¼Œä¿è¯è®­ç»ƒå’Œé¢„æµ‹ä¸€è‡´ã€‚
2. **æ”¯æŒå¤šåˆ†ç±»**
   - æ ‡ç­¾ `"A", "B", "C"` è‡ªåŠ¨ç¼–ç æˆ `0,1,2`ã€‚
   - è¾“å‡ºå±‚ `num_classes` æ ¹æ®æ•°æ®åŠ¨æ€ç¡®å®šã€‚
3. **åŠ¨æ€å±‚çº§**
   - `hidden_layers=[]` â†’ åªæœ‰è¾“å…¥å±‚ â†’ è¾“å‡ºå±‚ã€‚
   - `hidden_layers=[64,32]` â†’ è¾“å…¥ â†’ 64 â†’ 32 â†’ è¾“å‡ºã€‚

------

âš¡ è¿™æ ·ä¸€æ¥ï¼Œä½ å°±å¯ä»¥ç›´æ¥ç”¨ Excel é‡Œçš„å­—ç¬¦å­—æ®µè¿›è¡Œåˆ†ç±»è®­ç»ƒäº†ï¼Œåªè¦æ¢æˆï¼š

```python
df = pd.read_excel("your_data.xlsx")
```

å°±èƒ½æ— ç¼å¯¹æ¥ã€‚

æ˜ç™½ âœ…
 ä½ ç°åœ¨çš„éœ€æ±‚ä¸»è¦æœ‰ä¸‰ç‚¹ï¼š

1. **çœ‹åˆ°è®­ç»ƒçš„æƒ…å†µ** â†’ æŸå¤±ï¼ˆlossï¼‰ã€å‡†ç¡®ç‡éš epoch å˜åŒ–çš„è¶‹åŠ¿ï¼ˆåˆ¤æ–­æ˜¯å¦æ”¶æ•›ï¼‰ã€‚
2. **é¢„æµ‹æƒ…å†µ** â†’ è¾“å‡ºåˆ†ç±»æŠ¥å‘Šï¼ˆprecisionã€recallã€f1ï¼‰å’Œéƒ¨åˆ†é¢„æµ‹ç»“æœå¯¹æ¯”ã€‚
3. **æµ‹è¯•ä¸åŒçš„åˆ†ç±»åœºæ™¯** â†’ ä¾‹å¦‚äºŒåˆ†ç±»ã€å¤šåˆ†ç±»ï¼ˆ3ç±»ã€5ç±»ç­‰ï¼‰ï¼Œçœ‹çœ‹æ¨¡å‹é€‚åº”æ€§ã€‚

------

## ğŸ“Œ æ”¹è¿›ç‰ˆä»£ç ï¼ˆå¸¦è®­ç»ƒæ›²çº¿ & å¤šåœºæ™¯æµ‹è¯•ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ===== åŠ¨æ€ç¥ç»ç½‘ç»œæ¨¡å‹ =====
class DynamicNN(nn.Module):
    def __init__(self, input_dim, hidden_layers, num_classes):
        super(DynamicNN, self).__init__()
        layers = []
        in_dim = input_dim
        for h in hidden_layers:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h
        layers.append(nn.Linear(in_dim, num_classes))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ===== è®­ç»ƒ & æµ‹è¯•å‡½æ•° =====
def train_and_evaluate(df, feature_cols, label_col, hidden_layers=[32, 16], epochs=50):
    # ç¼–ç ç‰¹å¾
    encoders = {}
    for col in feature_cols:
        if df[col].dtype == object:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            encoders[col] = le

    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    df[label_col] = label_encoder.fit_transform(df[label_col])
    num_classes = len(label_encoder.classes_)

    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        df[feature_cols].values,
        df[label_col].values,
        test_size=0.2,
        random_state=42
    )
    X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)
    y_train, y_test = torch.tensor(y_train, dtype=torch.long), torch.tensor(y_test, dtype=torch.long)

    # æ¨¡å‹
    model = DynamicNN(input_dim=len(feature_cols), hidden_layers=hidden_layers, num_classes=num_classes)
    criterion, optimizer = nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr=0.01)

    # è®­ç»ƒ
    train_losses, train_accs = [], []
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        # è®¡ç®—å‡†ç¡®ç‡
        preds = outputs.argmax(dim=1)
        acc = accuracy_score(y_train.numpy(), preds.numpy())

        train_losses.append(loss.item())
        train_accs.append(acc)

        if (epoch+1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}, Train Acc={acc:.4f}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10,4))
    plt.subplot(1,2,1)
    plt.plot(train_losses, label="Loss")
    plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Training Loss")
    plt.legend()
    plt.subplot(1,2,2)
    plt.plot(train_accs, label="Accuracy", color="orange")
    plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Training Accuracy")
    plt.legend()
    plt.show()

    # æµ‹è¯•
    with torch.no_grad():
        y_pred = model(X_test).argmax(dim=1)

    print("\nğŸ“ˆ åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

    df_results = pd.DataFrame({
        "çœŸå®å€¼": label_encoder.inverse_transform(y_test.numpy()[:15]),
        "é¢„æµ‹å€¼": label_encoder.inverse_transform(y_pred.numpy()[:15])
    })
    print("\nğŸ”® éƒ¨åˆ†é¢„æµ‹ç»“æœï¼š")
    print(df_results)

# ===== åœºæ™¯1ï¼šä¸‰åˆ†ç±»ï¼Œå­—ç¬¦ç‰¹å¾ =====
np.random.seed(42)
df1 = pd.DataFrame({
    "color": np.random.choice(["red", "blue", "green"], size=300),
    "shape": np.random.choice(["circle", "square", "triangle"], size=300),
    "label": np.random.choice(["A", "B", "C"], size=300)
})
print("\n=== åœºæ™¯1ï¼šä¸‰åˆ†ç±»ï¼ˆå­—ç¬¦ç‰¹å¾ï¼‰ ===")
train_and_evaluate(df1, feature_cols=["color","shape"], label_col="label", hidden_layers=[32,16])

# ===== åœºæ™¯2ï¼šäºŒåˆ†ç±»ï¼Œæ··åˆç‰¹å¾ =====
df2 = pd.DataFrame({
    "color": np.random.choice(["red", "blue"], size=200),
    "size": np.random.choice(["small","large"], size=200),
    "num_feature": np.random.randn(200),
    "label": np.random.choice(["yes","no"], size=200)
})
print("\n=== åœºæ™¯2ï¼šäºŒåˆ†ç±»ï¼ˆå­—ç¬¦+æ•°å€¼ç‰¹å¾ï¼‰ ===")
train_and_evaluate(df2, feature_cols=["color","size","num_feature"], label_col="label", hidden_layers=[16])

# ===== åœºæ™¯3ï¼šäº”åˆ†ç±»ï¼Œæ•°å€¼ç‰¹å¾ =====
X = np.random.randn(400, 5)
y = np.random.choice(list("ABCDE"), size=400)
df3 = pd.DataFrame(X, columns=[f"f{i}" for i in range(5)])
df3["label"] = y
print("\n=== åœºæ™¯3ï¼šäº”åˆ†ç±»ï¼ˆæ•°å€¼ç‰¹å¾ï¼‰ ===")
train_and_evaluate(df3, feature_cols=[f"f{i}" for i in range(5)], label_col="label", hidden_layers=[64,32])
```

------

## ğŸ“Œ è¿è¡Œæ•ˆæœ

1. **è®­ç»ƒæƒ…å†µ**
   - æ¯10ä¸ª epoch æ‰“å°ä¸€æ¬¡ Loss å’Œ è®­ç»ƒé›†å‡†ç¡®ç‡ã€‚
   - åŒæ—¶ç»˜åˆ¶ Loss æ›²çº¿ & Accuracy æ›²çº¿ï¼Œèƒ½ç›´è§‚çœ‹åˆ°æ˜¯å¦æ”¶æ•›ã€‚
2. **é¢„æµ‹æƒ…å†µ**
   - è¾“å‡º `classification_report`ï¼ˆprecision, recall, f1, accuracyï¼‰ã€‚
   - å±•ç¤ºå‰ 15 æ¡é¢„æµ‹ vs çœŸå®æ ‡ç­¾ã€‚
3. **å¤šåˆ†ç±»åœºæ™¯**
   - åœºæ™¯1ï¼š3åˆ†ç±»ï¼Œå…¨éƒ¨æ˜¯å­—ç¬¦ç‰¹å¾ã€‚
   - åœºæ™¯2ï¼š2åˆ†ç±»ï¼Œå­—ç¬¦+æ•°å€¼æ··åˆã€‚
   - åœºæ™¯3ï¼š5åˆ†ç±»ï¼Œçº¯æ•°å€¼ç‰¹å¾ã€‚

------

è¿™æ ·ä½ å°±å¯ä»¥ç›´è§‚çœ‹åˆ°ï¼š

- è®­ç»ƒæ˜¯å¦æ”¶æ•›ï¼ˆLoss æ˜¯å¦ä¸‹é™ã€Accuracy æ˜¯å¦ä¸Šå‡ï¼‰ã€‚
- æ¨¡å‹é¢„æµ‹æ•ˆæœæ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚
- ä¸åŒåˆ†ç±»åœºæ™¯çš„æ³›åŒ–æƒ…å†µã€‚

éå¸¸å¥½çš„é—®é¢˜ ğŸ‘
 è®­ç»ƒå‡†ç¡®ç‡ä½ â†’ è¯´æ˜æ¨¡å‹è¿˜æ²¡å­¦å¥½ã€‚åŸå› å¯èƒ½æœ‰ä¸‰ç±»ï¼š

------

## ğŸš© 1. æ¨¡å‹ç»“æ„é—®é¢˜

- éšè—å±‚å¤ªå°‘ / ç¥ç»å…ƒå¤ªå°‘ â†’ å­¦ä¹ èƒ½åŠ›ä¸è¶³ã€‚
- æ¿€æ´»å‡½æ•°å•ä¸€ï¼ˆåªæœ‰ ReLUï¼‰ï¼Œå¯èƒ½å¯¼è‡´ç‰¹å¾å­¦ä¹ ä¸å……åˆ†ã€‚

âœ… è§£å†³åŠæ³•ï¼š

- å°è¯•å¢åŠ éšè—å±‚å’Œç¥ç»å…ƒï¼šæ¯”å¦‚ `[128, 64, 32]`ã€‚
- åœ¨å±‚ä¹‹é—´åŠ å…¥ `Dropout`ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰å’Œ `BatchNorm`ï¼ˆåŠ å¿«æ”¶æ•›ï¼‰ã€‚

------

## ğŸš© 2. è®­ç»ƒè¶…å‚æ•°é—®é¢˜

- å­¦ä¹ ç‡è¿‡å¤§/è¿‡å° â†’ ä¼˜åŒ–æ•ˆæœä¸å¥½ã€‚
- è®­ç»ƒè½®æ•°å¤ªå°‘ â†’ æ¨¡å‹è¿˜æ²¡æ”¶æ•›ã€‚

âœ… è§£å†³åŠæ³•ï¼š

- è°ƒæ•´å­¦ä¹ ç‡ `lr=0.001 ~ 0.01`ï¼Œå°è¯• `torch.optim.Adam` æˆ– `torch.optim.SGD`ã€‚
- å¢åŠ è®­ç»ƒè½®æ•°ï¼Œæ¯”å¦‚ä» `50` æå‡åˆ° `200`ã€‚

------

## ğŸš© 3. æ•°æ®é—®é¢˜

- ç‰¹å¾éšæœºæ€§å¤ªå¼ºï¼ˆæ¯”å¦‚æˆ‘ç»™ä½ çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œæœ¬èº«ç±»åˆ«å’Œç‰¹å¾å‡ ä¹éšæœºï¼‰ â†’ å¤©èŠ±æ¿å‡†ç¡®ç‡æœ¬èº«ä¸é«˜ã€‚
- ç‰¹å¾éœ€è¦æ›´å¥½çš„é¢„å¤„ç†ï¼Œæ¯”å¦‚æ ‡å‡†åŒ–ã€ç¼–ç æ–¹å¼ä¼˜åŒ–ã€‚

âœ… è§£å†³åŠæ³•ï¼š

- å¯¹æ•°å€¼ç‰¹å¾åšæ ‡å‡†åŒ– `StandardScaler`ã€‚
- å¯¹ç±»åˆ«ç‰¹å¾ä½¿ç”¨ **Embedding** æ›¿ä»£ç®€å•çš„ LabelEncoderï¼Œå¯ä»¥æ›´å¥½åœ°è¡¨è¾¾ä¿¡æ¯ã€‚

------

## ğŸ“Œ æ”¹è¿›ç‰ˆæ¨¡å‹ï¼ˆå¸¦ Dropout + BatchNormï¼‰

æˆ‘ç»™ä½ ä¸€ä¸ªå‡çº§ç‰ˆçš„æ¨¡å‹ï¼Œæ”¯æŒæ›´æ·±å±‚ç»“æ„ï¼Œå¹¶ä¸”åŠ å…¥ `BatchNorm` å’Œ `Dropout`ï¼Œæ•ˆæœä¼šæ›´ç¨³å®šï¼š

```python
class ImprovedNN(nn.Module):
    def __init__(self, input_dim, hidden_layers, num_classes, dropout=0.3):
        super(ImprovedNN, self).__init__()
        layers = []
        in_dim = input_dim
        for h in hidden_layers:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.BatchNorm1d(h))   # æ‰¹å½’ä¸€åŒ–
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout)) # Dropout
            in_dim = h
        layers.append(nn.Linear(in_dim, num_classes))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)
```

ç„¶åä½ å¯ä»¥è¿™æ ·æ›¿æ¢ï¼š

```python
model = ImprovedNN(input_dim=X_train.shape[1], hidden_layers=[128, 64, 32], num_classes=num_classes)
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

------

## ğŸ“ˆ å»ºè®®çš„ä¼˜åŒ–æ­¥éª¤

1. **å…ˆåŠ æ·±ç½‘ç»œ** â†’ `[128, 64, 32]`ã€‚
2. **ç”¨ BatchNorm + Dropout** æå‡æ³›åŒ–ã€‚
3. **è°ƒå­¦ä¹ ç‡**ï¼šå…ˆè¯• `0.001`ï¼Œä¸è¡Œå†è°ƒã€‚
4. **å¢åŠ è®­ç»ƒè½®æ•°**ï¼š100 ~ 200 è½®ã€‚
5. **æ£€æŸ¥æ•°æ®æœ¬èº«**ï¼šå¦‚æœç‰¹å¾ä¸æ ‡ç­¾å…³ç³»å¼±ï¼Œæ¨¡å‹å‡†ç¡®ç‡çš„ä¸Šé™å°±ä½ã€‚

------

è¿™ä¸ªé—®é¢˜é—®å¾—å¾ˆå…³é”® ğŸ‘

------

## âœ… ç»“è®ºå…ˆè¯´

ç›®å‰æˆ‘ä»¬å†™çš„ **åŠ¨æ€å…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰**ï¼Œæ˜¯**é’ˆå¯¹è¡¨æ ¼æ•°æ® / å·²æ•°å€¼åŒ–ç‰¹å¾**çš„ã€‚
 ç›´æ¥ç”¨åœ¨**å›¾ç‰‡åˆ†ç±»**ä¸Šï¼Œå¯ä»¥å‹‰å¼ºè·‘ï¼Œä½†æ•ˆæœä¼šå¾ˆå·®ã€‚

åŸå› æ˜¯ï¼š

- å›¾ç‰‡åƒç´ æœ‰å¾ˆå¼ºçš„**ç©ºé—´ç»“æ„ï¼ˆå±€éƒ¨ç›¸å…³æ€§ï¼‰**ï¼ŒMLP åªèƒ½â€œå¹³é“ºâ€æˆä¸€ç»´å‘é‡ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨è¿™äº›ç©ºé—´ä¿¡æ¯ã€‚
- å›¾ç‰‡åˆ†ç±»é€šå¸¸éœ€è¦ **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰** æˆ– **é¢„è®­ç»ƒæ¨¡å‹ï¼ˆResNetã€VGGã€EfficientNet ç­‰ï¼‰**ã€‚

------

## ğŸš© å¦‚æœä¸€å®šè¦ç”¨å½“å‰æ¨¡å‹å¤„ç†å›¾ç‰‡

- ä½ å¯ä»¥æŠŠå›¾ç‰‡å±•å¹³ä¸ºä¸€ç»´å‘é‡ï¼ˆæ¯”å¦‚ `28x28` çš„ç°åº¦å›¾å±•å¹³æˆ 784 ç»´ï¼‰ã€‚
- è¾“å…¥åˆ° MLPï¼Œä¾ç„¶èƒ½åˆ†ç±»ï¼ˆæ¯”å¦‚ MNIST æ‰‹å†™æ•°å­—ï¼‰ï¼Œä½†æ˜¯å‡†ç¡®ç‡è¿œä¸å¦‚ CNNã€‚

ç¤ºä¾‹ï¼ˆMNIST ç”¨ MLPï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# æ•°æ®é›†ï¼ˆMNISTï¼‰
transform = transforms.Compose([transforms.ToTensor(), lambda x: x.view(-1)])  # å±•å¹³
train_dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

# åŠ¨æ€ MLP æ¨¡å‹
class DynamicNN(nn.Module):
    def __init__(self, input_dim, hidden_layers, num_classes):
        super().__init__()
        layers, in_dim = [], input_dim
        for h in hidden_layers:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h
        layers.append(nn.Linear(in_dim, num_classes))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

model = DynamicNN(28*28, [256,128], 10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒ
for epoch in range(5):
    for X, y in train_loader:
        optimizer.zero_grad()
        out = model(X)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}: Loss={loss.item():.4f}")

# æµ‹è¯•
correct, total = 0, 0
with torch.no_grad():
    for X, y in test_loader:
        pred = model(X).argmax(1)
        correct += (pred == y).sum().item()
        total += y.size(0)
print(f"Test Accuracy: {correct/total:.4f}")
```

------

## ğŸš© æ›´é€‚åˆå›¾ç‰‡åˆ†ç±»çš„æ¨¡å‹

ğŸ‘‰ æ¨èç›´æ¥ç”¨ **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰**ï¼š

```python
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),  # (B,1,28,28) -> (B,32,28,28)
            nn.ReLU(),
            nn.MaxPool2d(2),                 # -> (B,32,14,14)
            nn.Conv2d(32, 64, 3, padding=1), # -> (B,64,14,14)
            nn.ReLU(),
            nn.MaxPool2d(2)                  # -> (B,64,7,7)
        )
        self.fc = nn.Sequential(
            nn.Linear(64*7*7, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.fc(x)
```

è¿™ç§ CNN ç»“æ„æ¯” MLP æ›´èƒ½æŠ“ä½å›¾ç‰‡ç‰¹å¾ï¼Œåœ¨ MNIST ä¸Šå¯ä»¥è½»æ¾è¶…è¿‡ **98% å‡†ç¡®ç‡**ã€‚

------

âœ… æ€»ç»“ï¼š

- ä½ çš„ **åŠ¨æ€ MLP** æ¨¡å‹èƒ½å¤„ç†å›¾ç‰‡ï¼Œä½†åªæ˜¯â€œå¹³é“ºåƒç´ â€ â†’ ç²¾åº¦ä¸é«˜ã€‚
- çœŸæ­£è¦åšå›¾ç‰‡åˆ†ç±»ï¼Œå»ºè®®ç”¨ **CNN** æˆ–è¿ç§»å­¦ä¹ ï¼ˆåŠ è½½ ResNetã€EfficientNet ç­‰ï¼‰ã€‚

------

æ˜ç™½ âœ…
 æ—¢ç„¶ä½ æåˆ°è¦ç»§ç»­ä¼˜åŒ– **RNN æ¨¡å‹**ï¼Œæˆ‘è¿™é‡Œç»™ä½ ä¸€ä¸ªå®Œæ•´çš„ **å¯è¿è¡Œ PyTorch RNN åˆ†ç±»ä»£ç **ï¼Œæ”¯æŒå­—ç¬¦/æ–‡æœ¬åºåˆ—åˆ†ç±»ï¼Œå¹¶ä¸”ç»“æ„å¯æ‰©å±•ã€‚æˆ‘ä»¬ä¼šä¸€æ­¥æ­¥ä¼˜åŒ–ï¼š

1. **æ¨¡å‹ç»“æ„**
   - è¾“å…¥å±‚ï¼šåµŒå…¥å±‚ï¼ˆEmbeddingï¼‰
   - éšè—å±‚ï¼šRNN / LSTM / GRUï¼ˆå¯é…ç½®ï¼‰
   - è¾“å‡ºå±‚ï¼šå…¨è¿æ¥å±‚
2. **ä¼˜åŒ–ç‚¹**
   - ä½¿ç”¨ `nn.LSTM`ï¼ˆæ¯”å•çº¯ RNN æ›´ç¨³å®šï¼‰
   - æ”¯æŒå¯å˜é•¿åº¦åºåˆ—ï¼ˆpack_padded_sequenceï¼‰
   - å¢åŠ  `dropout` é˜²æ­¢è¿‡æ‹Ÿåˆ
   - æ”¯æŒå¤šåˆ†ç±»ä»»åŠ¡
3. **æµ‹è¯•æ•°æ®**
   - å…ˆç”¨ Pandas ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿæ•°æ®é›†ï¼ˆå­—ç¬¦åˆ†ç±»ä»»åŠ¡ï¼‰
   - ä¾‹å¦‚ï¼šå¥å­ â†’ ç±»åˆ«

------

### ğŸŸ¢ å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ====================
# 1. æ„é€ ç¤ºä¾‹æ•°æ®
# ====================
data = {
    "text": [
        "hello world", "hi there", "good morning", "good night",
        "machine learning", "deep learning", "neural network",
        "cat", "dog", "fish"
    ],
    "label": ["greeting", "greeting", "greeting", "greeting",
              "ai", "ai", "ai",
              "animal", "animal", "animal"]
}
df = pd.DataFrame(data)

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
df["label_encoded"] = label_encoder.fit_transform(df["label"])

# å­—ç¬¦å­—å…¸
all_text = " ".join(df["text"].values)
vocab = sorted(set(all_text))
char2idx = {c: i + 1 for i, c in enumerate(vocab)}  # 0 ç•™ç»™ padding
idx2char = {i: c for c, i in char2idx.items()}

def text_to_seq(text, max_len=15):
    seq = [char2idx.get(c, 0) for c in text]
    return seq[:max_len] + [0] * (max_len - len(seq))

df["seq"] = df["text"].apply(lambda x: text_to_seq(x))

# åˆ’åˆ†æ•°æ®é›†
X = np.stack(df["seq"].values)
y = df["label_encoded"].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ====================
# 2. Dataset & DataLoader
# ====================
class TextDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=4, shuffle=True)
test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=4)

# ====================
# 3. RNN æ¨¡å‹
# ====================
class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, rnn_type="LSTM", num_layers=1, dropout=0.2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        if rnn_type == "RNN":
            self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_layers,
                              batch_first=True, dropout=dropout if num_layers > 1 else 0)
        elif rnn_type == "GRU":
            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,
                              batch_first=True, dropout=dropout if num_layers > 1 else 0)
        else:  # é»˜è®¤ LSTM
            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,
                               batch_first=True, dropout=dropout if num_layers > 1 else 0)

        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        embed = self.embedding(x)  # [B, T, E]
        _, hidden = self.rnn(embed)

        if isinstance(hidden, tuple):  # LSTM
            hidden = hidden[0]

        out = self.fc(hidden[-1])  # å–æœ€åä¸€å±‚ hidden
        return out

# ====================
# 4. è®­ç»ƒ
# ====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = RNNClassifier(
    vocab_size=len(char2idx) + 1,
    embed_dim=32,
    hidden_dim=64,
    num_classes=len(label_encoder.classes_),
    rnn_type="LSTM",
    num_layers=2,
    dropout=0.3
).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

def train_model(model, loader, epochs=20):
    for epoch in range(epochs):
        model.train()
        total_loss, correct, total = 0, 0, 0

        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y_batch).sum().item()
            total += y_batch.size(0)

        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}, "
              f"Acc: {correct/total:.4f}")

train_model(model, train_loader, epochs=15)

# ====================
# 5. æµ‹è¯•ä¸é¢„æµ‹
# ====================
model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(y_batch.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

print("\n=== Classification Report ===")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

# ====================
# 6. é¢„æµ‹æ–°æ•°æ®
# ====================
def predict(text):
    seq = torch.tensor([text_to_seq(text)], dtype=torch.long).to(device)
    model.eval()
    with torch.no_grad():
        output = model(seq)
        pred = torch.argmax(output, dim=1).item()
    return label_encoder.inverse_transform([pred])[0]

print("\n=== æµ‹è¯•é¢„æµ‹ ===")
print("Input: 'deep learning' -> Pred:", predict("deep learning"))
print("Input: 'dog' -> Pred:", predict("dog"))
print("Input: 'good evening' -> Pred:", predict("good evening"))
```

------

### ğŸŸ¢ ä»£ç è¯´æ˜

- æ•°æ®ï¼šç”¨ Pandas æ¨¡æ‹Ÿ Excel æ•°æ®ï¼ˆå­—ç¬¦ä¸² â†’ åˆ†ç±»æ ‡ç­¾ï¼‰
- æ¨¡å‹ï¼šå¯é€‰ `RNN / LSTM / GRU`ï¼Œæ”¯æŒå¤šå±‚ã€dropout
- è¾“å‡ºï¼šè®­ç»ƒè¿‡ç¨‹å‡†ç¡®ç‡ã€æµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Šã€é¢„æµ‹æ–°æ ·æœ¬

------

å¥½çš„ âœ… æˆ‘æ˜ç™½äº†ã€‚æˆ‘ä»¬é‡æ–°ä»é›¶å¼€å§‹ï¼Œç»™ä½ ä¸€ä¸ª **åŠ¨æ€æ„å»ºéšè—å±‚çš„ DNN åˆ†ç±»æ¨¡å‹**ï¼Œå¹¶ä¸”æ”¯æŒï¼š

- **åŠ¨æ€éšè—å±‚**ï¼šæ ¹æ® `layer_list = [64, 32, 16]` è¿™æ ·çš„åˆ—è¡¨æ¥è‡ªåŠ¨æ­å»ºç½‘ç»œã€‚
- **å¤šè¾“å…¥ç‰¹å¾ã€å¤šè¾“å‡ºç±»åˆ«**ï¼šè¾“å…¥å’Œè¾“å‡ºæ•°æ®éƒ½æ¥è‡ª `pandas.DataFrame`ï¼Œæ”¯æŒå¤šåˆ—è¾“å…¥å’Œå¤šåˆ—è¾“å‡ºã€‚
- **è®­ç»ƒä¸é¢„æµ‹å®Œæ•´æµç¨‹**ï¼š
  1. ç”Ÿæˆ DataFrame æ•°æ®ï¼ˆæ¨¡æ‹Ÿ Excelï¼‰
  2. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
  3. è®­ç»ƒæ¨¡å‹å¹¶è¾“å‡ºå‡†ç¡®ç‡
  4. è¿›è¡Œé¢„æµ‹å¹¶è¾“å‡ºé¢„æµ‹ç»“æœ

------

## ğŸŸ¢ å®Œæ•´å¯è¿è¡Œä»£ç ï¼ˆPyTorch + Pandasï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# ============================
# 1. æ„é€ ç¤ºä¾‹ DataFrame æ•°æ®
# ============================
# å‡è®¾æœ‰å¤šä¸ªè¾“å…¥ç‰¹å¾ (x1, x2, x3)ï¼Œè¾“å‡ºä¸ºåˆ†ç±» (y)
np.random.seed(42)
df = pd.DataFrame({
    "x1": np.random.randn(100),
    "x2": np.random.randn(100),
    "x3": np.random.randn(100),
    "y": np.random.choice(["A", "B", "C"], size=100)  # å¤šåˆ†ç±»
})

# è¾“å…¥è¾“å‡ºåˆ—
input_cols = ["x1", "x2", "x3"]
output_col = "y"

# ç¼–ç è¾“å‡ºæ ‡ç­¾
label_encoder = LabelEncoder()
df[output_col] = label_encoder.fit_transform(df[output_col])

# å½’ä¸€åŒ–è¾“å…¥
scaler = StandardScaler()
df[input_cols] = scaler.fit_transform(df[input_cols])

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    df[input_cols].values,
    df[output_col].values,
    test_size=0.2,
    random_state=42
)

# ============================
# 2. Dataset & DataLoader
# ============================
class DataFrameDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(DataFrameDataset(X_train, y_train), batch_size=8, shuffle=True)
test_loader = DataLoader(DataFrameDataset(X_test, y_test), batch_size=8)

# ============================
# 3. åŠ¨æ€æ„å»º DNN æ¨¡å‹
# ============================
class DynamicDNN(nn.Module):
    def __init__(self, input_dim, layer_list, output_dim, dropout=0.2):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for hidden_dim in layer_list:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ============================
# 4. è®­ç»ƒä¸æµ‹è¯•
# ============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
layer_list = [64, 32]  # åŠ¨æ€éšè—å±‚ç»“æ„
model = DynamicDNN(input_dim=len(input_cols),
                   layer_list=layer_list,
                   output_dim=len(label_encoder.classes_)).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

def train_model(model, loader, epochs=30):
    for epoch in range(epochs):
        model.train()
        total_loss, correct, total = 0, 0, 0
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y_batch).sum().item()
            total += y_batch.size(0)

        acc = correct / total
        print(f"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(loader):.4f} | Acc: {acc:.4f}")

train_model(model, train_loader, epochs=20)

# ============================
# 5. æµ‹è¯•ä¸é¢„æµ‹
# ============================
model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(y_batch.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

print("\n=== æµ‹è¯•é›†ç»“æœ ===")
print("Accuracy:", accuracy_score(y_true, y_pred))
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

# ============================
# 6. é¢„æµ‹æ–°æ•°æ®ï¼ˆæ¨¡æ‹Ÿ Excel è¡Œï¼‰
# ============================
def predict(df_row):
    X_new = scaler.transform([df_row[input_cols].values])
    X_tensor = torch.tensor(X_new, dtype=torch.float32).to(device)
    with torch.no_grad():
        output = model(X_tensor)
        pred = torch.argmax(output, dim=1).item()
    return label_encoder.inverse_transform([pred])[0]

print("\n=== æ–°æ•°æ®é¢„æµ‹ ===")
print(df.head(3))
print("é¢„æµ‹ç»“æœ:", predict(df.iloc[0]))
```

------

## ğŸŸ¢ ä»£ç è¯´æ˜

1. `layer_list = [64, 32]` â†’ è‡ªåŠ¨ç”Ÿæˆ **ä¸¤å±‚éšè—å±‚** (64 å’Œ 32 ä¸ªç¥ç»å…ƒ)ã€‚
2. è¾“å…¥æ•°æ®æ¥è‡ª `pandas.DataFrame`ï¼Œæ”¯æŒå¤šåˆ—è¾“å…¥ã€‚
3. è¾“å‡ºä¸ºåˆ†ç±»ï¼ˆå¤šåˆ†ç±»ï¼‰ï¼Œä½¿ç”¨ `LabelEncoder` è‡ªåŠ¨å¤„ç†æ ‡ç­¾ã€‚
4. ä½¿ç”¨ `train_loader` è®­ç»ƒï¼Œæ‰“å° **Loss å’Œ Accuracy**ã€‚
5. æµ‹è¯•é›†è¾“å‡ºå‡†ç¡®ç‡å’Œåˆ†ç±»æŠ¥å‘Šã€‚
6. æ–°æ•°æ®ï¼ˆDataFrame è¡Œï¼‰å¯ä»¥ç›´æ¥é¢„æµ‹ç±»åˆ«ã€‚

------

å¥½ ğŸ‘ é‚£æˆ‘ä»¬åœ¨ä¸Šé¢çš„ **åŠ¨æ€ DNN åˆ†ç±»æ¨¡å‹** åŸºç¡€ä¸Šï¼Œæ‰©å±•ä¸º **æ”¯æŒå¤šè¾“å‡ºåˆ—ï¼ˆå¤šæ ‡ç­¾ / å¤šä»»åŠ¡åˆ†ç±»ï¼‰** çš„ç‰ˆæœ¬ã€‚

è¿™æ ·ä½ çš„ DataFrame å³ä½¿æœ‰å¤šä¸ªè¾“å‡ºåˆ—ï¼ˆæ¯”å¦‚ `y1`, `y2`, `y3`ï¼‰ï¼Œä¹Ÿèƒ½åŒæ—¶è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚

------

# ğŸŸ¢ å¤šè¾“å‡º DNN åˆ†ç±»æ¨¡å‹

### ä¸»è¦æ”¹åŠ¨

1. **è¾“å‡ºæ”¯æŒå¤šåˆ—**ï¼š
   - æ¯ä¸ªè¾“å‡ºåˆ—éƒ½å¯ä»¥æ˜¯åˆ†ç±»ï¼ˆå•ä»»åŠ¡åˆ†ç±» â†’ å¤šä»»åŠ¡åˆ†ç±»ï¼‰ã€‚
   - æ¯ä¸ªè¾“å‡ºåˆ—å¯¹åº”ä¸€ä¸ªåˆ†ç±»å¤´ (`nn.Linear`)ã€‚
   - æ¯ä¸ªè¾“å‡ºåˆ—æœ‰è‡ªå·±çš„æŸå¤±å‡½æ•°ï¼Œæœ€åæ±‚å¹³å‡ã€‚
2. **æ•°æ®å¤„ç†**ï¼š
   - è¾“å…¥åˆ— `input_cols`
   - è¾“å‡ºåˆ— `output_cols = ["y1", "y2"]`
   - æ¯ä¸ªè¾“å‡ºåˆ—åˆ†åˆ« `LabelEncoder` ç¼–ç ã€‚
3. **é¢„æµ‹ç»“æœ**ï¼š
   - æ¯ä¸ªè¾“å‡ºåˆ—éƒ½å¯ä»¥é¢„æµ‹ï¼Œè¾“å‡ºå­—å…¸å½¢å¼ `{col: pred}`ã€‚

------

## ğŸŸ¢ å®Œæ•´ä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report

# ============================
# 1. æ„é€ ç¤ºä¾‹ DataFrame æ•°æ®
# ============================
np.random.seed(42)
df = pd.DataFrame({
    "x1": np.random.randn(200),
    "x2": np.random.randn(200),
    "x3": np.random.randn(200),
    "y1": np.random.choice(["A", "B", "C"], size=200),
    "y2": np.random.choice(["Yes", "No"], size=200),
})

# è¾“å…¥è¾“å‡ºåˆ—
input_cols = ["x1", "x2", "x3"]
output_cols = ["y1", "y2"]

# å¯¹æ¯ä¸ªè¾“å‡ºåˆ—è¿›è¡Œ LabelEncoder
label_encoders = {}
for col in output_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# å½’ä¸€åŒ–è¾“å…¥
scaler = StandardScaler()
df[input_cols] = scaler.fit_transform(df[input_cols])

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    df[input_cols].values,
    df[output_cols].values,
    test_size=0.2,
    random_state=42
)

# ============================
# 2. Dataset & DataLoader
# ============================
class MultiOutputDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(MultiOutputDataset(X_train, y_train), batch_size=16, shuffle=True)
test_loader = DataLoader(MultiOutputDataset(X_test, y_test), batch_size=16)

# ============================
# 3. å¤šè¾“å‡º DNN æ¨¡å‹
# ============================
class MultiOutputDNN(nn.Module):
    def __init__(self, input_dim, layer_list, output_dims, dropout=0.2):
        super().__init__()
        # å…¬å…±éƒ¨åˆ†
        layers = []
        prev_dim = input_dim
        for hidden_dim in layer_list:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        self.shared = nn.Sequential(*layers)

        # æ¯ä¸ªè¾“å‡ºä¸€ä¸ªåˆ†ç±»å¤´
        self.output_heads = nn.ModuleList([nn.Linear(prev_dim, out_dim) for out_dim in output_dims])

    def forward(self, x):
        shared_out = self.shared(x)
        outputs = [head(shared_out) for head in self.output_heads]
        return outputs

# ============================
# 4. è®­ç»ƒ
# ============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ¯ä¸ªè¾“å‡ºåˆ—çš„ç±»åˆ«æ•°
output_dims = [len(label_encoders[col].classes_) for col in output_cols]

model = MultiOutputDNN(
    input_dim=len(input_cols),
    layer_list=[64, 32],
    output_dims=output_dims
).to(device)

criterions = [nn.CrossEntropyLoss() for _ in output_cols]
optimizer = optim.Adam(model.parameters(), lr=0.01)

def train_model(model, loader, epochs=20):
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)

            # æ¯ä¸ªè¾“å‡ºåˆ—åˆ†åˆ«è®¡ç®— loss
            losses = [criterions[i](outputs[i], y_batch[:, i]) for i in range(len(output_cols))]
            loss = sum(losses) / len(losses)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(loader):.4f}")

train_model(model, train_loader, epochs=15)

# ============================
# 5. æµ‹è¯•ä¸é¢„æµ‹
# ============================
model.eval()
all_true, all_pred = [[] for _ in output_cols], [[] for _ in output_cols]

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        for i in range(len(output_cols)):
            _, predicted = torch.max(outputs[i], 1)
            all_true[i].extend(y_batch[:, i].cpu().numpy())
            all_pred[i].extend(predicted.cpu().numpy())

print("\n=== æµ‹è¯•é›†ç»“æœ ===")
for i, col in enumerate(output_cols):
    print(f"\nåˆ— {col}:")
    print(classification_report(all_true[i], all_pred[i], target_names=label_encoders[col].classes_))

# ============================
# 6. æ–°æ•°æ®é¢„æµ‹
# ============================
def predict(df_row):
    X_new = scaler.transform([df_row[input_cols].values])
    X_tensor = torch.tensor(X_new, dtype=torch.float32).to(device)
    with torch.no_grad():
        outputs = model(X_tensor)
        preds = {}
        for i, col in enumerate(output_cols):
            pred_idx = torch.argmax(outputs[i], dim=1).item()
            preds[col] = label_encoders[col].inverse_transform([pred_idx])[0]
    return preds

print("\n=== æ–°æ•°æ®é¢„æµ‹ ===")
print(df.head(3))
print("é¢„æµ‹ç»“æœ:", predict(df.iloc[0]))
```

------

# ğŸŸ¢ è¾“å‡ºæ•ˆæœ

1. **è®­ç»ƒè¿‡ç¨‹**ï¼šæ‰“å° `Loss`

2. **æµ‹è¯•ç»“æœ**ï¼šæ¯ä¸ªè¾“å‡ºåˆ—éƒ½æœ‰ `classification_report`ï¼ˆç²¾åº¦/å¬å›ç‡/F1ï¼‰

3. **é¢„æµ‹æ–°æ•°æ®**ï¼šè¿”å›å­—å…¸ï¼Œæ¯”å¦‚ï¼š

   ```python
   {'y1': 'B', 'y2': 'Yes'}
   ```

------

å¤ªå¥½äº† âœ… é‚£æˆ‘ä»¬æ¥åšä¸€ä¸ª **é€šç”¨å¤šä»»åŠ¡ DNN**ï¼š

æ”¯æŒ **åˆ†ç±»ä»»åŠ¡ + å›å½’ä»»åŠ¡** æ··åˆï¼š

- å¦‚æœ **è¾“å‡ºåˆ—æ˜¯ç±»åˆ«å‹ï¼ˆå­—ç¬¦ä¸² / åˆ†ç±»ï¼‰** â†’ è‡ªåŠ¨åšåˆ†ç±»ï¼ˆCrossEntropyLossï¼‰
- å¦‚æœ **è¾“å‡ºåˆ—æ˜¯æ•°å€¼å‹** â†’ è‡ªåŠ¨åšå›å½’ï¼ˆMSELossï¼‰
- æœ€ç»ˆè®­ç»ƒæ—¶ä¼šå¯¹æ‰€æœ‰ä»»åŠ¡çš„ loss å–å¹³å‡

è¿™æ ·ä½ çš„ DataFrame é‡Œï¼Œ**æŸäº›åˆ—æ˜¯åˆ†ç±»ä»»åŠ¡ï¼ŒæŸäº›åˆ—æ˜¯å›å½’ä»»åŠ¡**ï¼Œéƒ½èƒ½åŒæ—¶è®­ç»ƒã€åŒæ—¶é¢„æµ‹ã€‚

------

# ğŸŸ¢ é€šç”¨å¤šä»»åŠ¡ DNNï¼ˆåˆ†ç±» + å›å½’ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, mean_squared_error

# ============================
# 1. æ„é€ ç¤ºä¾‹ DataFrame æ•°æ®
# ============================
np.random.seed(42)
df = pd.DataFrame({
    "x1": np.random.randn(300),
    "x2": np.random.randn(300),
    "x3": np.random.randn(300),
    "y_class": np.random.choice(["A", "B", "C"], size=300),   # åˆ†ç±»ä»»åŠ¡
    "y_reg": np.random.randn(300) * 10 + 50                   # å›å½’ä»»åŠ¡
})

# è¾“å…¥è¾“å‡ºåˆ—
input_cols = ["x1", "x2", "x3"]
output_cols = ["y_class", "y_reg"]

# ä¿å­˜ä»»åŠ¡ç±»å‹ï¼ˆåˆ†ç±»/å›å½’ï¼‰
task_types = {}
label_encoders = {}

for col in output_cols:
    if df[col].dtype == "object" or df[col].dtype.name == "category":
        task_types[col] = "classification"
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le
    else:
        task_types[col] = "regression"

# å½’ä¸€åŒ–è¾“å…¥
scaler = StandardScaler()
df[input_cols] = scaler.fit_transform(df[input_cols])

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    df[input_cols].values,
    df[output_cols].values,
    test_size=0.2,
    random_state=42
)

# ============================
# 2. Dataset & DataLoader
# ============================
class MultiTaskDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)  # æ··åˆå­˜å‚¨
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(MultiTaskDataset(X_train, y_train), batch_size=16, shuffle=True)
test_loader = DataLoader(MultiTaskDataset(X_test, y_test), batch_size=16)

# ============================
# 3. é€šç”¨å¤šä»»åŠ¡ DNN æ¨¡å‹
# ============================
class MultiTaskDNN(nn.Module):
    def __init__(self, input_dim, layer_list, output_info, dropout=0.2):
        super().__init__()
        # å…±äº«å±‚
        layers = []
        prev_dim = input_dim
        for hidden_dim in layer_list:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        self.shared = nn.Sequential(*layers)

        # æ¯ä¸ªè¾“å‡ºä»»åŠ¡å¯¹åº”ä¸€ä¸ª head
        self.output_heads = nn.ModuleDict()
        for col, info in output_info.items():
            if info["task"] == "classification":
                self.output_heads[col] = nn.Linear(prev_dim, info["num_classes"])
            elif info["task"] == "regression":
                self.output_heads[col] = nn.Linear(prev_dim, 1)

    def forward(self, x):
        shared_out = self.shared(x)
        outputs = {}
        for col, head in self.output_heads.items():
            outputs[col] = head(shared_out)
        return outputs

# ============================
# 4. å‡†å¤‡ä»»åŠ¡ä¿¡æ¯
# ============================
output_info = {}
for i, col in enumerate(output_cols):
    if task_types[col] == "classification":
        output_info[col] = {"task": "classification", "num_classes": len(label_encoders[col].classes_)}
    else:
        output_info[col] = {"task": "regression"}

# ============================
# 5. æ¨¡å‹ & è®­ç»ƒ
# ============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = MultiTaskDNN(
    input_dim=len(input_cols),
    layer_list=[64, 32],
    output_info=output_info
).to(device)

criterions = {}
for col, info in output_info.items():
    if info["task"] == "classification":
        criterions[col] = nn.CrossEntropyLoss()
    else:
        criterions[col] = nn.MSELoss()

optimizer = optim.Adam(model.parameters(), lr=0.01)

def train_model(model, loader, epochs=20):
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)

            losses = []
            for i, col in enumerate(output_cols):
                if task_types[col] == "classification":
                    target = y_batch[:, i].long()
                    loss = criterions[col](outputs[col], target)
                else:  # å›å½’
                    target = y_batch[:, i].unsqueeze(1)
                    loss = criterions[col](outputs[col], target)
                losses.append(loss)
            loss = sum(losses) / len(losses)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(loader):.4f}")

train_model(model, train_loader, epochs=15)

# ============================
# 6. æµ‹è¯•ä¸é¢„æµ‹
# ============================
model.eval()
y_true, y_pred = {col: [] for col in output_cols}, {col: [] for col in output_cols}

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        for i, col in enumerate(output_cols):
            if task_types[col] == "classification":
                _, predicted = torch.max(outputs[col], 1)
                y_true[col].extend(y_batch[:, i].cpu().numpy())
                y_pred[col].extend(predicted.cpu().numpy())
            else:  # å›å½’
                y_true[col].extend(y_batch[:, i].cpu().numpy())
                y_pred[col].extend(outputs[col].squeeze().cpu().numpy())

print("\n=== æµ‹è¯•é›†ç»“æœ ===")
for col in output_cols:
    if task_types[col] == "classification":
        print(f"\nåˆ†ç±»ä»»åŠ¡ {col}:")
        print(classification_report(y_true[col], y_pred[col], target_names=label_encoders[col].classes_))
    else:
        print(f"\nå›å½’ä»»åŠ¡ {col}:")
        print("MSE:", mean_squared_error(y_true[col], y_pred[col]))

# ============================
# 7. æ–°æ•°æ®é¢„æµ‹
# ============================
def predict(df_row):
    X_new = scaler.transform([df_row[input_cols].values])
    X_tensor = torch.tensor(X_new, dtype=torch.float32).to(device)
    with torch.no_grad():
        outputs = model(X_tensor)
        preds = {}
        for col in output_cols:
            if task_types[col] == "classification":
                pred_idx = torch.argmax(outputs[col], dim=1).item()
                preds[col] = label_encoders[col].inverse_transform([pred_idx])[0]
            else:
                preds[col] = outputs[col].item()
        return preds

print("\n=== æ–°æ•°æ®é¢„æµ‹ ===")
print(df.head(2))
print("é¢„æµ‹ç»“æœ:", predict(df.iloc[0]))
```

------

# ğŸŸ¢ åŠŸèƒ½æ€»ç»“

1. **åŠ¨æ€éšè—å±‚**ï¼šç”¨ `layer_list` æ§åˆ¶ç½‘ç»œç»“æ„ã€‚
2. **å¤šä»»åŠ¡**ï¼š
   - è‡ªåŠ¨åŒºåˆ†åˆ†ç±»ä»»åŠ¡ / å›å½’ä»»åŠ¡ã€‚
   - å„è‡ªç”¨ä¸åŒçš„ `loss`ï¼ˆäº¤å‰ç†µ / MSEï¼‰ã€‚
   - åŒæ—¶è®­ç»ƒã€‚
3. **è¾“å‡º**ï¼š
   - åˆ†ç±»ä»»åŠ¡ï¼š`classification_report`ï¼ˆç²¾åº¦/å¬å›ç‡/F1ï¼‰ã€‚
   - å›å½’ä»»åŠ¡ï¼šMSEã€‚
4. **é¢„æµ‹**ï¼š
   - è¾“å‡ºå­—å…¸ï¼Œè‡ªåŠ¨åŒºåˆ†åˆ†ç±»ç»“æœ / å›å½’ç»“æœã€‚

------

ğŸ‘Œ æ˜ç™½äº†ï¼Œæˆ‘ç»™ä½ æ•´ç†ä¸€ä¸ªå®Œæ•´çš„ **åŠ¨æ€éšè—å±‚ DNN åˆ†ç±»æ¨¡å‹** ç¤ºä¾‹ï¼Œæ»¡è¶³ä»¥ä¸‹éœ€æ±‚ï¼š

- ä½¿ç”¨ **PyTorch** å®ç°
- éšè—å±‚ç”± `layer_list` æ§åˆ¶ï¼Œæ¯ä¸€å±‚ç¥ç»å…ƒæ•°åŠ¨æ€é…ç½®
- è¾“å…¥ / è¾“å‡º æ”¯æŒ **å¤šä¸ªåˆ—**ï¼ˆå¤šç‰¹å¾è¾“å…¥ï¼Œå¤šç±»åˆ«è¾“å‡ºï¼‰
- æ•°æ®ä½¿ç”¨ **pandas DataFrame** æ„å»ºï¼Œå¸¦åˆ—å
- åŒ…å«å®Œæ•´æµç¨‹ï¼š
  1. æ•°æ®å‡†å¤‡ï¼ˆDataFrameï¼‰
  2. é¢„å¤„ç†ï¼ˆæ•°å€¼åŒ–ã€å¼ é‡åŒ–ï¼‰
  3. æ¨¡å‹å®šä¹‰ï¼ˆåŠ¨æ€ DNNï¼‰
  4. æ¨¡å‹è®­ç»ƒ
  5. é¢„æµ‹ä¸ç»“æœå±•ç¤º

------

### ğŸ”¹ å®Œæ•´ä»£ç ç¤ºä¾‹

```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# ========== 1. æ„é€ æµ‹è¯•æ•°æ® ==========
# å‡è®¾æˆ‘ä»¬æœ‰ 3 ä¸ªè¾“å…¥ç‰¹å¾ï¼Œè¾“å‡ºæ˜¯åˆ†ç±»æ ‡ç­¾
df = pd.DataFrame({
    "feature1": np.random.randn(200),
    "feature2": np.random.randn(200),
    "feature3": np.random.randn(200),
    "label": np.random.choice(["A", "B", "C"], size=200)  # å¤šåˆ†ç±»è¾“å‡º
})

print("åŸå§‹æ•°æ®ï¼š")
print(df.head())

# ========== 2. æ•°æ®é¢„å¤„ç† ==========
X = df[["feature1", "feature2", "feature3"]].values
y = df["label"].values

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# æ ‡å‡†åŒ–è¾“å…¥
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ‡åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42
)

# è½¬æ¢ä¸º tensor
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

input_dim = X_train.shape[1]
output_dim = len(np.unique(y_encoded))

# ========== 3. å®šä¹‰åŠ¨æ€ DNN æ¨¡å‹ ==========
class DynamicDNN(nn.Module):
    def __init__(self, input_dim, layer_list, output_dim):
        super().__init__()
        layers = []
        in_dim = input_dim
        for hidden_dim in layer_list:
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.ReLU())
            in_dim = hidden_dim
        layers.append(nn.Linear(in_dim, output_dim))  # è¾“å‡ºå±‚
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ç¤ºä¾‹ï¼šéšè—å±‚ [64, 32]
layer_list = [64, 32]
model = DynamicDNN(input_dim, layer_list, output_dim)

# ========== 4. è®­ç»ƒ ==========
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

epochs = 30
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()

    # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
    _, predicted = torch.max(outputs, 1)
    acc = (predicted == y_train).float().mean().item()
    print(f"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}, Train Acc={acc:.4f}")

# ========== 5. é¢„æµ‹ ==========
model.eval()
with torch.no_grad():
    outputs = model(X_test)
    _, predicted = torch.max(outputs, 1)

print("\né¢„æµ‹ç»“æœç¤ºä¾‹ï¼š")
results = pd.DataFrame({
    "çœŸå®æ ‡ç­¾": label_encoder.inverse_transform(y_test.numpy()),
    "é¢„æµ‹æ ‡ç­¾": label_encoder.inverse_transform(predicted.numpy())
})
print(results.head(10))
```

------

### ğŸ”¹ è¿è¡Œç»“æœï¼ˆè¯´æ˜ï¼‰

1. **è®­ç»ƒè¿‡ç¨‹**ï¼šä¼šæ‰“å° `Loss` å’Œ `Train Accuracy`ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹åœ¨é€æ­¥æ”¶æ•›
2. **é¢„æµ‹ç»“æœ**ï¼šä¼šè¾“å‡ºä¸€ä¸ª DataFrameï¼ŒåŒ…å«çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾å¯¹æ¯”ï¼Œæ–¹ä¾¿éªŒè¯

------

### ğŸ”¹ å¯æ‰©å±•æ€§

- å¦‚æœæœ‰ **å¤šä¸ªè¾“å‡ºåˆ—**ï¼ˆä¾‹å¦‚å¤šä»»åŠ¡åˆ†ç±» / å¤šæ ‡ç­¾åˆ†ç±»ï¼‰ï¼Œåªéœ€åœ¨ `y` éƒ¨åˆ†æ„å»ºæ—¶æŠŠå¤šä¸ªåˆ—åˆå¹¶ï¼Œå¹¶åœ¨æ¨¡å‹æœ€åè¾“å‡ºå¯¹åº”ç»´åº¦å³å¯
- `layer_list` å¯ä»¥éšæ„è°ƒæ•´ï¼Œæ¯”å¦‚ `[128, 64, 32]`
- å¯ä»¥æ”¯æŒ **å›å½’ä»»åŠ¡**ï¼Œåªéœ€æŠŠæŸå¤±å‡½æ•°æ”¹ä¸º `MSELoss`ï¼Œè¾“å‡ºå±‚ä¸åŠ  `Softmax`

------

å¥½ âœ… é‚£æˆ‘å¸®ä½ æ‰©å±•åˆ° **å¤šä»»åŠ¡åˆ†ç±»**ï¼ˆå³ DataFrame é‡Œæœ‰å¤šä¸ªè¾“å‡ºåˆ—ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼‰ï¼ŒDNN æ¨¡å‹ä¼šåŒæ—¶é¢„æµ‹å¤šä¸ªè¾“å‡ºã€‚

------

## ğŸ”¹ æ€è·¯

1. è¾“å…¥ç‰¹å¾ä»ç„¶æ˜¯å¤šä¸ªæ•°å€¼åˆ—
2. è¾“å‡ºå¯ä»¥æœ‰å¤šåˆ—ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼ˆä¾‹å¦‚ `label1`ã€`label2`ï¼‰
3. æ¯ä¸ªè¾“å‡ºåˆ—æœ‰è‡ªå·±çš„ç±»åˆ«æ•°é‡ï¼Œå› æ­¤éœ€è¦ **å¤šå¤´è¾“å‡º**ï¼ˆæ¯ä¸ªä»»åŠ¡ä¸€ä¸ª `Linear` å±‚ï¼‰
4. è®­ç»ƒæ—¶ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„äº¤å‰ç†µæŸå¤±ï¼Œç„¶åå–å¹³å‡

------

## ğŸ”¹ å®Œæ•´ä»£ç ç¤ºä¾‹

```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# ========== 1. æ„é€ æµ‹è¯•æ•°æ® ==========
# å‡è®¾æœ‰ 3 ä¸ªè¾“å…¥ç‰¹å¾ï¼Œ2 ä¸ªåˆ†ç±»è¾“å‡ºä»»åŠ¡
df = pd.DataFrame({
    "feature1": np.random.randn(300),
    "feature2": np.random.randn(300),
    "feature3": np.random.randn(300),
    "label1": np.random.choice(["A", "B", "C"], size=300),  # 3 åˆ†ç±»
    "label2": np.random.choice(["X", "Y"], size=300)        # 2 åˆ†ç±»
})

print("åŸå§‹æ•°æ®ï¼š")
print(df.head())

# ========== 2. æ•°æ®é¢„å¤„ç† ==========
X = df[["feature1", "feature2", "feature3"]].values
y1 = df["label1"].values
y2 = df["label2"].values

# æ ‡ç­¾ç¼–ç ï¼ˆæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹ç¼–ç å™¨ï¼‰
le1, le2 = LabelEncoder(), LabelEncoder()
y1_encoded = le1.fit_transform(y1)
y2_encoded = le2.fit_transform(y2)

# æ ‡å‡†åŒ–è¾“å…¥
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ‡åˆ†æ•°æ®é›†
X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(
    X_scaled, y1_encoded, y2_encoded, test_size=0.2, random_state=42
)

# è½¬ tensor
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y1_train = torch.tensor(y1_train, dtype=torch.long)
y1_test = torch.tensor(y1_test, dtype=torch.long)
y2_train = torch.tensor(y2_train, dtype=torch.long)
y2_test = torch.tensor(y2_test, dtype=torch.long)

input_dim = X_train.shape[1]
output_dims = [len(np.unique(y1_encoded)), len(np.unique(y2_encoded))]  # [3,2]

# ========== 3. å®šä¹‰åŠ¨æ€ DNN å¤šä»»åŠ¡æ¨¡å‹ ==========
class MultiTaskDNN(nn.Module):
    def __init__(self, input_dim, layer_list, output_dims):
        super().__init__()
        layers = []
        in_dim = input_dim
        for hidden_dim in layer_list:
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.ReLU())
            in_dim = hidden_dim
        self.shared = nn.Sequential(*layers)
        # æ¯ä¸ªä»»åŠ¡ä¸€ä¸ªè¾“å‡ºå¤´
        self.heads = nn.ModuleList([nn.Linear(in_dim, out_dim) for out_dim in output_dims])

    def forward(self, x):
        shared_out = self.shared(x)
        return [head(shared_out) for head in self.heads]

# ç¤ºä¾‹ï¼šéšè—å±‚ [64, 32]
layer_list = [64, 32]
model = MultiTaskDNN(input_dim, layer_list, output_dims)

# ========== 4. è®­ç»ƒ ==========
criterions = [nn.CrossEntropyLoss(), nn.CrossEntropyLoss()]
optimizer = optim.Adam(model.parameters(), lr=0.01)

epochs = 30
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    losses = [criterions[i](outputs[i], y) for i, y in enumerate([y1_train, y2_train])]
    loss = sum(losses) / len(losses)  # å¹³å‡æŸå¤±
    loss.backward()
    optimizer.step()

    # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
    accs = []
    for i, y in enumerate([y1_train, y2_train]):
        _, pred = torch.max(outputs[i], 1)
        accs.append((pred == y).float().mean().item())
    print(f"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}, Acc1={accs[0]:.4f}, Acc2={accs[1]:.4f}")

# ========== 5. é¢„æµ‹ ==========
model.eval()
with torch.no_grad():
    outputs = model(X_test)
    preds = [torch.argmax(out, 1) for out in outputs]

results = pd.DataFrame({
    "çœŸå®label1": le1.inverse_transform(y1_test.numpy()),
    "é¢„æµ‹label1": le1.inverse_transform(preds[0].numpy()),
    "çœŸå®label2": le2.inverse_transform(y2_test.numpy()),
    "é¢„æµ‹label2": le2.inverse_transform(preds[1].numpy())
})
print("\né¢„æµ‹ç»“æœç¤ºä¾‹ï¼š")
print(results.head(10))
```

------

## ğŸ”¹ è¾“å‡ºç¤ºä¾‹

```
Epoch 1/30, Loss=1.0352, Acc1=0.3600, Acc2=0.5100
Epoch 2/30, Loss=0.9801, Acc1=0.4600, Acc2=0.5500
...
é¢„æµ‹ç»“æœç¤ºä¾‹ï¼š
  çœŸå®label1 é¢„æµ‹label1 çœŸå®label2 é¢„æµ‹label2
0         C         B         X         X
1         B         B         Y         Y
2         A         A         X         X
```

------

## ğŸ”¹ ç‰¹ç‚¹

- `layer_list` åŠ¨æ€æ§åˆ¶éšè—å±‚ï¼ˆæ¯”å¦‚ `[128, 64, 32]`ï¼‰
- `output_dims` è‡ªåŠ¨æ ¹æ® **æ¯ä¸ªè¾“å‡ºåˆ—ç±»åˆ«æ•°** ç”Ÿæˆå¤šä¸ªè¾“å‡ºå¤´
- è®­ç»ƒæ—¶å¯¹æ¯ä¸ªä»»åŠ¡è®¡ç®— **ç‹¬ç«‹çš„ CrossEntropyLoss**ï¼Œå†æ±‚å¹³å‡
- è¾“å‡ºæ—¶å¯ä»¥åŒæ—¶é¢„æµ‹å¤šä¸ªåˆ†ç±»ä»»åŠ¡

------

å¥½ ğŸ‘Œï¼Œç°åœ¨æˆ‘ä»¬æ¥å‡çº§ç‰ˆæœ¬ï¼Œè®© **è¾“å…¥å¯ä»¥æ¥å—å­—ç¬¦å‹æ•°æ®**ã€‚
 é€šå¸¸è¡¨æ ¼æ•°æ®é‡Œæ—¢æœ‰ **æ•°å€¼ç‰¹å¾**ï¼Œä¹Ÿæœ‰ **å­—ç¬¦å‹ç‰¹å¾**ï¼ˆç±»åˆ«ç‰¹å¾ï¼‰ï¼Œè®­ç»ƒå‰éœ€è¦è¿›è¡Œ**ç¼–ç **ã€‚

------

## ğŸ”¹ æ€è·¯

1. **æ•°å€¼åˆ—**ï¼šç›´æ¥æ ‡å‡†åŒ–ï¼ˆ`StandardScaler`ï¼‰
2. **å­—ç¬¦åˆ—**ï¼šç”¨ `LabelEncoder` è½¬æ¢ä¸ºæ•´æ•°ï¼Œç„¶ååš **Embedding** å¤„ç†ï¼ˆç›¸æ¯” OneHot æ›´é«˜æ•ˆï¼‰
3. **æ¨¡å‹ç»“æ„**ï¼š
   - æ•°å€¼è¾“å…¥ â†’ ç›´æ¥é€å…¥ DNN
   - ç±»åˆ«è¾“å…¥ â†’ è½¬æ¢æˆ embeddingï¼Œå†ä¸æ•°å€¼ç‰¹å¾æ‹¼æ¥
   - æ‹¼æ¥åçš„å‘é‡é€å…¥å…±äº«çš„ DNN éšè—å±‚ï¼Œå†åˆ†åˆ°å¤šä»»åŠ¡è¾“å‡º

------

## ğŸ”¹ å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ˆå­—ç¬¦å‹è¾“å…¥ + å¤šä»»åŠ¡åˆ†ç±»ï¼‰

```python
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# ========== 1. æ„é€ æµ‹è¯•æ•°æ® ==========
df = pd.DataFrame({
    "num1": np.random.randn(300),
    "num2": np.random.randn(300),
    "cat1": np.random.choice(["red", "blue", "green"], size=300),  # ç±»åˆ«å‹è¾“å…¥
    "cat2": np.random.choice(["low", "medium", "high"], size=300), # ç±»åˆ«å‹è¾“å…¥
    "label1": np.random.choice(["A", "B", "C"], size=300),  # 3 åˆ†ç±»
    "label2": np.random.choice(["X", "Y"], size=300)       # 2 åˆ†ç±»
})

print("åŸå§‹æ•°æ®ï¼š")
print(df.head())

# ========== 2. æ•°æ®é¢„å¤„ç† ==========
num_cols = ["num1", "num2"]
cat_cols = ["cat1", "cat2"]
label_cols = ["label1", "label2"]

# æ•°å€¼ç‰¹å¾
scaler = StandardScaler()
X_num = scaler.fit_transform(df[num_cols].values)

# ç±»åˆ«ç‰¹å¾ç¼–ç 
cat_encoders = {col: LabelEncoder() for col in cat_cols}
X_cat = np.column_stack([cat_encoders[col].fit_transform(df[col]) for col in cat_cols])
cat_dims = [len(cat_encoders[col].classes_) for col in cat_cols]  # æ¯åˆ—ç±»åˆ«æ•°

# è¾“å‡ºæ ‡ç­¾
label_encoders = {col: LabelEncoder() for col in label_cols}
y_encoded = [label_encoders[col].fit_transform(df[col]) for col in label_cols]
output_dims = [len(label_encoders[col].classes_) for col in label_cols]

# åˆ’åˆ†æ•°æ®
X_num_train, X_num_test, X_cat_train, X_cat_test, y1_train, y1_test, y2_train, y2_test = train_test_split(
    X_num, X_cat, y_encoded[0], y_encoded[1], test_size=0.2, random_state=42
)

# è½¬ tensor
X_num_train = torch.tensor(X_num_train, dtype=torch.float32)
X_num_test = torch.tensor(X_num_test, dtype=torch.float32)
X_cat_train = torch.tensor(X_cat_train, dtype=torch.long)
X_cat_test = torch.tensor(X_cat_test, dtype=torch.long)
y1_train = torch.tensor(y1_train, dtype=torch.long)
y1_test = torch.tensor(y1_test, dtype=torch.long)
y2_train = torch.tensor(y2_train, dtype=torch.long)
y2_test = torch.tensor(y2_test, dtype=torch.long)

# ========== 3. å®šä¹‰æ¨¡å‹ ==========
class MultiInputDNN(nn.Module):
    def __init__(self, num_input_dim, cat_dims, layer_list, output_dims, embed_dim=4):
        super().__init__()
        # Embedding å±‚
        self.embeddings = nn.ModuleList([
            nn.Embedding(cat_dim, min(embed_dim, cat_dim)) for cat_dim in cat_dims
        ])
        embed_total_dim = sum([min(embed_dim, cat_dim) for cat_dim in cat_dims])

        # å…±äº« DNN
        in_dim = num_input_dim + embed_total_dim
        layers = []
        for hidden_dim in layer_list:
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.ReLU())
            in_dim = hidden_dim
        self.shared = nn.Sequential(*layers)

        # å¤šä»»åŠ¡è¾“å‡º
        self.heads = nn.ModuleList([nn.Linear(in_dim, out_dim) for out_dim in output_dims])

    def forward(self, x_num, x_cat):
        # ç±»åˆ«ç‰¹å¾ embedding
        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]
        x_cat_embed = torch.cat(embeds, dim=1)

        # æ‹¼æ¥æ•°å€¼ç‰¹å¾å’Œç±»åˆ« embedding
        x = torch.cat([x_num, x_cat_embed], dim=1)

        # å…±äº«å±‚
        shared_out = self.shared(x)

        # å¤šä»»åŠ¡è¾“å‡º
        return [head(shared_out) for head in self.heads]

# ========== 4. åˆå§‹åŒ– ==========
layer_list = [64, 32]
model = MultiInputDNN(num_input_dim=X_num_train.shape[1],
                      cat_dims=cat_dims,
                      layer_list=layer_list,
                      output_dims=output_dims)

criterions = [nn.CrossEntropyLoss(), nn.CrossEntropyLoss()]
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ========== 5. è®­ç»ƒ ==========
epochs = 30
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_num_train, X_cat_train)
    losses = [criterions[i](outputs[i], y) for i, y in enumerate([y1_train, y2_train])]
    loss = sum(losses) / len(losses)
    loss.backward()
    optimizer.step()

    # å‡†ç¡®ç‡
    accs = []
    for i, y in enumerate([y1_train, y2_train]):
        _, pred = torch.max(outputs[i], 1)
        accs.append((pred == y).float().mean().item())
    print(f"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}, Acc1={accs[0]:.4f}, Acc2={accs[1]:.4f}")

# ========== 6. æµ‹è¯•é¢„æµ‹ ==========
model.eval()
with torch.no_grad():
    outputs = model(X_num_test, X_cat_test)
    preds = [torch.argmax(out, 1) for out in outputs]

results = pd.DataFrame({
    "çœŸå®label1": label_encoders["label1"].inverse_transform(y1_test.numpy()),
    "é¢„æµ‹label1": label_encoders["label1"].inverse_transform(preds[0].numpy()),
    "çœŸå®label2": label_encoders["label2"].inverse_transform(y2_test.numpy()),
    "é¢„æµ‹label2": label_encoders["label2"].inverse_transform(preds[1].numpy())
})
print("\né¢„æµ‹ç»“æœç¤ºä¾‹ï¼š")
print(results.head(10))
```

------

## ğŸ”¹ ç‰¹ç‚¹

- **æ”¯æŒå­—ç¬¦å‹è¾“å…¥**ï¼šé€šè¿‡ `LabelEncoder + Embedding` å¤„ç†
- **æ”¯æŒæ•°å€¼å‹è¾“å…¥**ï¼šæ ‡å‡†åŒ–åç›´æ¥è¾“å…¥
- **æ”¯æŒå¤šä»»åŠ¡è¾“å‡º**ï¼šå¤šä¸ªåˆ†ç±»ç›®æ ‡åŒæ—¶è®­ç»ƒ
- **`layer_list`** å†³å®šéšè—å±‚ç¥ç»å…ƒæ•°ï¼ˆåŠ¨æ€ï¼‰

------

ğŸ‘Œ æ˜ç™½äº†ï¼Œä½ çš„é¢„å¤„ç†å·²ç»æœ‰äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç°åœ¨ä¸»è¦é’ˆå¯¹ **PyTorch åŠ¨æ€ DNN åˆ†ç±»æ¨¡å‹** æ¥åšä¼˜åŒ–ï¼Œæé«˜å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

ä¸‹é¢æˆ‘æ€»ç»“ä¸€äº›å¯æ“ä½œçš„ä¼˜åŒ–æ–¹å‘ï¼Œå¹¶åœ¨ä»£ç é‡ŒåŠ è¿›å»ï¼š

------

## ğŸ”¹ ä¼˜åŒ–æ€è·¯

1. **ç½‘ç»œç»“æ„ä¼˜åŒ–**
   - å¢åŠ  **BatchNorm**ï¼šç¼“è§£æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ï¼ŒåŠ å¿«æ”¶æ•›
   - å¢åŠ  **Dropout**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
   - ä½¿ç”¨ **æ›´æ·±/æ›´å®½çš„éšè—å±‚**ï¼ˆæ¯”å¦‚ `[128, 64, 32]`ï¼‰
2. **è®­ç»ƒè¿‡ç¨‹ä¼˜åŒ–**
   - ä½¿ç”¨ **å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼ˆå¦‚ `StepLR` æˆ– `ReduceLROnPlateau`ï¼‰
   - å¢åŠ  **æ—©åœæœºåˆ¶**ï¼ˆEarly Stoppingï¼‰
3. **æ­£åˆ™åŒ–ä¼˜åŒ–**
   - ä½¿ç”¨ `weight_decay`ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰
   - Dropout
4. **è¯„ä¼°æŒ‡æ ‡**
   - é™¤äº†å‡†ç¡®ç‡ï¼Œè¿˜æ‰“å° **éªŒè¯é›† Lossã€æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Š**

------

## ğŸ”¹ ä¼˜åŒ–åçš„æ¨¡å‹ä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report

# ========== ä¼˜åŒ–ç‰ˆ DNN æ¨¡å‹ ==========
class OptimizedDNN(nn.Module):
    def __init__(self, input_dim, layer_list, output_dim, dropout=0.3):
        super().__init__()
        layers = []
        in_dim = input_dim
        for hidden_dim in layer_list:
            layers.append(nn.Linear(in_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))  # åŠ  BatchNorm
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))  # åŠ  Dropout
            in_dim = hidden_dim
        layers.append(nn.Linear(in_dim, output_dim))  # è¾“å‡ºå±‚
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ========== è®­ç»ƒå‡½æ•° ==========
def train_model(model, X_train, y_train, X_val, y_val, epochs=50, lr=0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # åŠ  weight_decay
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)  # å­¦ä¹ ç‡è°ƒåº¦

    best_val_acc = 0
    patience, patience_counter = 5, 0  # Early stopping

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # è®­ç»ƒå‡†ç¡®ç‡
        _, pred_train = torch.max(outputs, 1)
        train_acc = (pred_train == y_train).float().mean().item()

        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)
            _, pred_val = torch.max(val_outputs, 1)
            val_acc = (pred_val == y_val).float().mean().item()

        print(f"Epoch {epoch+1}/{epochs}, Loss={loss.item():.4f}, "
              f"Train Acc={train_acc:.4f}, Val Loss={val_loss.item():.4f}, Val Acc={val_acc:.4f}")

        # Early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            best_model_state = model.state_dict()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping triggered")
                model.load_state_dict(best_model_state)
                break

    return model

# ========== é¢„æµ‹å‡½æ•° ==========
def evaluate_model(model, X_test, y_test, label_encoder):
    model.eval()
    with torch.no_grad():
        outputs = model(X_test)
        _, preds = torch.max(outputs, 1)

    print("\nåˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test.numpy(),
                                preds.numpy(),
                                target_names=label_encoder.classes_))

    return preds
```

------

## ğŸ”¹ æ”¹è¿›ç‚¹æ€»ç»“

- **BatchNorm + Dropout** â†’ æå‡æ³›åŒ–èƒ½åŠ›
- **Adam + weight_decay** â†’ æ­£åˆ™åŒ–
- **å­¦ä¹ ç‡è°ƒåº¦å™¨** â†’ è‡ªåŠ¨é™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢éœ‡è¡
- **Early Stopping** â†’ é˜²æ­¢è¿‡æ‹Ÿåˆ
- **åˆ†ç±»æŠ¥å‘Š** â†’ æ›´ç›´è§‚åœ°çœ‹ Precisionã€Recallã€F1

------

å¥½é—®é¢˜ ğŸ‘

åœ¨ **PyTorch çš„åˆ†ç±»æ¨¡å‹**é‡Œï¼Œ`softmax` ç¡®å®ç»å¸¸ä¼šè¢«â€œå¿½ç•¥â€ï¼ŒåŸå› æ˜¯ï¼š

1. **è®­ç»ƒæ—¶ä¸éœ€è¦æ˜¾å¼ softmax**

   - æˆ‘ä»¬ä¸€èˆ¬åœ¨æœ€åä¸€å±‚åªè¾“å‡º logitsï¼ˆåŸå§‹åˆ†æ•°ï¼‰ï¼Œä¸åš softmaxã€‚
   - å› ä¸º `nn.CrossEntropyLoss` å†…éƒ¨ä¼šè‡ªåŠ¨æ‰§è¡Œ `log_softmax`ï¼Œè¿™æ ·æ¯”è‡ªå·±æ‰‹åŠ¨åŠ  `softmax` æ•°å€¼æ›´ç¨³å®šã€‚

   ```python
   criterion = nn.CrossEntropyLoss()
   logits = model(X)  # shape: (batch_size, num_classes)
   loss = criterion(logits, y)
   ```

   ğŸ‘‰ æ‰€ä»¥è®­ç»ƒæ—¶ **ä¸è¦æ˜¾å¼åŠ  softmax**ã€‚

2. **é¢„æµ‹/æ¨ç†æ—¶æ‰éœ€è¦ softmax**

   - åœ¨é¢„æµ‹é˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦æ¦‚ç‡åˆ†å¸ƒï¼Œå°±è¦æ‰‹åŠ¨åŠ  `softmax`ï¼š

   ```python
   probs = torch.softmax(logits, dim=1)  # è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ
   preds = torch.argmax(probs, dim=1)    # å–æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
   ```

3. **æ˜¯å¦éœ€è¦æ˜¾å¼æ·»åŠ  softmaxï¼Ÿ**

   - **è®­ç»ƒä¸­ â†’ ä¸éœ€è¦**ï¼ˆé™¤éä½ è‡ªå®šä¹‰ lossï¼‰ã€‚
   - **é¢„æµ‹ä¸­ â†’ éœ€è¦**ï¼ˆå¦‚æœä½ å…³å¿ƒæ¦‚ç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ç±»åˆ«ï¼‰ã€‚

------

å¥½çš„ âœ… æˆ‘æ¥å¸®ä½ æ„å»ºä¸€ä¸ª **ä¼˜åŒ–åçš„åŠ¨æ€ DNN åˆ†ç±»æ¨¡å‹**ï¼Œç‰¹ç‚¹æ˜¯ï¼š

- **æ”¯æŒå¤šè¾“å…¥ã€å¤šè¾“å‡º**ï¼ˆDataFrame çš„åˆ—åå¯æ§ï¼‰ã€‚
- **åŠ¨æ€éšè—å±‚**ï¼ˆé€šè¿‡ `layer_list` æ§åˆ¶å±‚æ•°å’Œæ¯å±‚ç¥ç»å…ƒæ•°ï¼‰ã€‚
- **åŠ å…¥ä¼˜åŒ–ç‚¹**ï¼š
  - Dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
  - BatchNormï¼ˆåŠ å¿«æ”¶æ•›ï¼Œæå‡ç¨³å®šæ€§ï¼‰
  - å­¦ä¹ ç‡è°ƒåº¦å™¨
- **è®­ç»ƒæ—¶ä¸æ˜¾å¼ softmax**ï¼ˆç”¨ `CrossEntropyLoss`ï¼‰ï¼Œé¢„æµ‹æ—¶æ‰ç”¨ `softmax` è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚
- **è‡ªåŠ¨è®¡ç®—å‡†ç¡®ç‡**ï¼Œå¹¶è¾“å‡ºé¢„æµ‹ç»“æœã€‚

ä¸‹é¢æ˜¯ä¸€ä»½å®Œæ•´çš„ PyTorch ç¤ºä¾‹ï¼Œä½¿ç”¨ DataFrame ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import pandas as pd
import numpy as np

# ========== 1. ç”Ÿæˆæ¨¡æ‹Ÿ DataFrame æ•°æ® ==========
np.random.seed(42)
N = 500

df = pd.DataFrame({
    "feature1": np.random.randn(N),
    "feature2": np.random.randn(N) * 2,
    "feature3": np.random.choice(["A", "B", "C"], size=N),
    "label": np.random.choice(["cat", "dog", "mouse"], size=N)
})

# ç¼–ç å­—ç¬¦å‹è¾“å…¥ï¼ˆOne-Hot æˆ– Label Encodingï¼‰
df = pd.get_dummies(df, columns=["feature3"], drop_first=True)

# ç¼–ç æ ‡ç­¾
label_enc = LabelEncoder()
df["label"] = label_enc.fit_transform(df["label"])

X = df.drop(columns=["label"]).values
y = df["label"].values

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# ========== 2. å®šä¹‰åŠ¨æ€ DNN æ¨¡å‹ ==========
class DynamicDNN(nn.Module):
    def __init__(self, input_dim, output_dim, layer_list, dropout=0.3):
        super(DynamicDNN, self).__init__()
        layers = []
        prev_dim = input_dim

        for h in layer_list:
            layers.append(nn.Linear(prev_dim, h))
            layers.append(nn.BatchNorm1d(h))   # æ‰¹å½’ä¸€åŒ–
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout)) # dropout
            prev_dim = h

        layers.append(nn.Linear(prev_dim, output_dim))  # æœ€åä¸€å±‚ä¸åŠ æ¿€æ´»
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

# ========== 3. åˆå§‹åŒ–æ¨¡å‹ ==========
input_dim = X_train.shape[1]
output_dim = len(np.unique(y))
layer_list = [64, 32]  # ä¸¤ä¸ªéšè—å±‚
model = DynamicDNN(input_dim, output_dim, layer_list)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)

# ========== 4. è®­ç»ƒ ==========
epochs = 50
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    logits = model(X_train)
    loss = criterion(logits, y_train)
    loss.backward()
    optimizer.step()
    scheduler.step()

    # è®­ç»ƒå‡†ç¡®ç‡
    preds = torch.argmax(logits, dim=1)
    acc = (preds == y_train).float().mean().item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Train Acc: {acc:.4f}")

# ========== 5. æµ‹è¯• ==========
model.eval()
with torch.no_grad():
    test_logits = model(X_test)
    test_probs = torch.softmax(test_logits, dim=1)
    test_preds = torch.argmax(test_probs, dim=1)
    test_acc = (test_preds == y_test).float().mean().item()

print("\n=== æµ‹è¯•ç»“æœ ===")
print("Test Accuracy:", test_acc)
print("é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:\n", test_probs[:5].numpy())
print("é¢„æµ‹ç±»åˆ«:\n", label_enc.inverse_transform(test_preds[:5].numpy()))
print("çœŸå®ç±»åˆ«:\n", label_enc.inverse_transform(y_test[:5].numpy()))
```

------

### ğŸ”‘ è¾“å‡ºå†…å®¹

- æ¯ 10 è½®æ‰“å°ä¸€æ¬¡ **è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡**
- æœ€ç»ˆç»™å‡º **æµ‹è¯•é›†å‡†ç¡®ç‡**
- æ˜¾ç¤ºå‰ 5 æ¡é¢„æµ‹çš„ï¼š
  - æ¦‚ç‡åˆ†å¸ƒï¼ˆsoftmax è¾“å‡ºï¼‰
  - é¢„æµ‹ç±»åˆ«
  - çœŸå®ç±»åˆ«

------

å¥½å˜ âœ… æˆ‘æ¥å¸®ä½ åœ¨ä¹‹å‰çš„ **åŠ¨æ€ DNN åˆ†ç±»æ¨¡å‹** åŸºç¡€ä¸Šï¼Œæ‰©å±•æˆä¸€ä¸ª **å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹**ï¼š

- **è¾“å…¥**ï¼šåŒæ ·æ˜¯ DataFrameï¼ˆå¯ä»¥æœ‰æ•°å€¼å’Œå­—ç¬¦åˆ—ï¼‰ã€‚
- **è¾“å‡º**ï¼š
  - åˆ†ç±»ä»»åŠ¡ï¼ˆä¾‹å¦‚é¢„æµ‹ç±»åˆ«ï¼šcat/dog/mouseï¼‰
  - å›å½’ä»»åŠ¡ï¼ˆä¾‹å¦‚é¢„æµ‹æ•°å€¼ï¼šä»·æ ¼/è¯„åˆ†ç­‰ï¼‰
- **ç»“æ„**ï¼š
  - å…±äº«å‰ç½®ç½‘ç»œï¼ˆåŠ¨æ€éšè—å±‚ï¼Œç”¨ `layer_list` æ§åˆ¶ï¼‰ã€‚
  - åˆ†åˆ«æœ‰ **åˆ†ç±»å¤´** å’Œ **å›å½’å¤´**ã€‚
- **Loss**ï¼šåˆ†ç±»ç”¨ `CrossEntropyLoss`ï¼Œå›å½’ç”¨ `MSELoss`ï¼Œæœ€åå–åŠ æƒå’Œã€‚
- **é¢„æµ‹**ï¼šè¾“å‡ºåˆ†ç±»æ¦‚ç‡åˆ†å¸ƒ + å›å½’å€¼ã€‚

------

### ğŸ”¹ å®Œæ•´ PyTorch ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import pandas as pd
import numpy as np

# ========== 1. ç”Ÿæˆæ¨¡æ‹Ÿ DataFrame æ•°æ® ==========
np.random.seed(42)
N = 600

df = pd.DataFrame({
    "feature1": np.random.randn(N),
    "feature2": np.random.randn(N) * 2,
    "feature3": np.random.choice(["A", "B", "C"], size=N),
    "label_class": np.random.choice(["cat", "dog", "mouse"], size=N),  # åˆ†ç±»æ ‡ç­¾
    "label_reg": np.random.randn(N) * 10 + 50                         # å›å½’æ ‡ç­¾
})

# ç¼–ç å­—ç¬¦å‹è¾“å…¥
df = pd.get_dummies(df, columns=["feature3"], drop_first=True)

# æ ‡ç­¾å¤„ç†
label_enc = LabelEncoder()
df["label_class"] = label_enc.fit_transform(df["label_class"])

X = df.drop(columns=["label_class", "label_reg"]).values
y_class = df["label_class"].values
y_reg = df["label_reg"].values

# æ ‡å‡†åŒ–è¾“å…¥
scaler = StandardScaler()
X = scaler.fit_transform(X)

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
    X, y_class, y_reg, test_size=0.2, random_state=42
)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_class_train = torch.tensor(y_class_train, dtype=torch.long)
y_reg_train = torch.tensor(y_reg_train, dtype=torch.float32).view(-1, 1)

X_test = torch.tensor(X_test, dtype=torch.float32)
y_class_test = torch.tensor(y_class_test, dtype=torch.long)
y_reg_test = torch.tensor(y_reg_test, dtype=torch.float32).view(-1, 1)

# ========== 2. å®šä¹‰å¤šä»»åŠ¡ DNN æ¨¡å‹ ==========
class MultiTaskDNN(nn.Module):
    def __init__(self, input_dim, class_output_dim, layer_list, dropout=0.3):
        super(MultiTaskDNN, self).__init__()
        layers = []
        prev_dim = input_dim

        for h in layer_list:
            layers.append(nn.Linear(prev_dim, h))
            layers.append(nn.BatchNorm1d(h))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = h

        self.shared = nn.Sequential(*layers)

        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(prev_dim, class_output_dim)

        # å›å½’å¤´
        self.regressor = nn.Linear(prev_dim, 1)

    def forward(self, x):
        shared_out = self.shared(x)
        class_logits = self.classifier(shared_out)
        reg_output = self.regressor(shared_out)
        return class_logits, reg_output

# ========== 3. åˆå§‹åŒ–æ¨¡å‹ ==========
input_dim = X_train.shape[1]
class_output_dim = len(np.unique(y_class))
layer_list = [128, 64]

model = MultiTaskDNN(input_dim, class_output_dim, layer_list)

criterion_class = nn.CrossEntropyLoss()
criterion_reg = nn.MSELoss()

optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)

# ========== 4. è®­ç»ƒ ==========
epochs = 50
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()

    class_logits, reg_output = model(X_train)
    loss_class = criterion_class(class_logits, y_class_train)
    loss_reg = criterion_reg(reg_output, y_reg_train)
    loss = loss_class + 0.5 * loss_reg  # æƒé‡å¯è°ƒ

    loss.backward()
    optimizer.step()
    scheduler.step()

    # åˆ†ç±»å‡†ç¡®ç‡
    preds_class = torch.argmax(class_logits, dim=1)
    acc_class = (preds_class == y_class_train).float().mean().item()
    mse_reg = loss_reg.item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | "
              f"Class Acc: {acc_class:.4f} | Reg MSE: {mse_reg:.4f}")

# ========== 5. æµ‹è¯• ==========
model.eval()
with torch.no_grad():
    test_class_logits, test_reg_output = model(X_test)

    # åˆ†ç±»é¢„æµ‹
    test_probs = torch.softmax(test_class_logits, dim=1)
    test_preds_class = torch.argmax(test_probs, dim=1)
    test_acc_class = (test_preds_class == y_class_test).float().mean().item()

    # å›å½’é¢„æµ‹
    test_mse_reg = criterion_reg(test_reg_output, y_reg_test).item()

print("\n=== æµ‹è¯•ç»“æœ ===")
print("åˆ†ç±»å‡†ç¡®ç‡:", test_acc_class)
print("å›å½’ MSE:", test_mse_reg)
print("é¢„æµ‹åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒ:\n", test_probs[:5].numpy())
print("é¢„æµ‹åˆ†ç±»ç±»åˆ«:\n", label_enc.inverse_transform(test_preds_class[:5].numpy()))
print("çœŸå®åˆ†ç±»ç±»åˆ«:\n", label_enc.inverse_transform(y_class_test[:5].numpy()))
print("é¢„æµ‹å›å½’å€¼:\n", test_reg_output[:5].numpy().flatten())
print("çœŸå®å›å½’å€¼:\n", y_reg_test[:5].numpy().flatten())
```

------

### ğŸ”‘ ä»£ç è¦ç‚¹

1. **å¤šä»»åŠ¡ç½‘ç»œ**
   - `self.shared` â†’ å…±äº«éƒ¨åˆ†
   - `self.classifier` â†’ åˆ†ç±»å¤´
   - `self.regressor` â†’ å›å½’å¤´
2. **Loss ç»„åˆ**
   - åˆ†ç±» loss (`CrossEntropyLoss`)
   - å›å½’ loss (`MSELoss`)
   - æ€» loss = `loss_class + 0.5 * loss_reg`ï¼ˆæƒé‡å¯è°ƒï¼‰
3. **è®­ç»ƒ & æµ‹è¯•æŒ‡æ ‡**
   - åˆ†ç±»ï¼šå‡†ç¡®ç‡
   - å›å½’ï¼šMSE

------

