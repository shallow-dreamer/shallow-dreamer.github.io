---
layout:     post
title:      "è®­ç»ƒæµç¨‹æµ‹è¯•"
subtitle:   ""
date:       2025-06-02
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - python
---

ä½ è¿™ä¸ªéœ€æ±‚å¯ä»¥å¾ˆå¥½åœ°ä½¿ç”¨ **é€‚é…å™¨æ¨¡å¼** æ¥æž„å»ºä¸€ä¸ª**çµæ´»çš„æ¨¡åž‹è®­ç»ƒæ¡†æž¶**ï¼Œç”¨äºŽé€‚é…å„ç§ä¸åŒç±»åž‹çš„æ¨¡åž‹ï¼ˆå¦‚çº¿æ€§å›žå½’ã€ç¥žç»ç½‘ç»œã€LLMã€å¤§æ•°æ®åˆ†æžç­‰ï¼‰ã€‚

ä¸‹é¢æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ª**å®Œæ•´çš„ç»“æž„åŒ–è®­ç»ƒæµç¨‹æ¡†æž¶**ï¼ŒåŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š

------

## âœ… æ€»ä½“æµç¨‹ï¼ˆæ¨¡å—åˆ’åˆ†ï¼‰

```text
User Request
   â†“
1. å‚æ•°è¾“å…¥ï¼ˆæ¨¡åž‹å‚æ•° + å› å­å‚æ•° + è®­ç»ƒæ•°æ®åˆ—è¡¨ï¼‰
   â†“
2. æ•°æ®æ¸…æ´—ï¼ˆç¼ºå¤±å€¼ã€å­—æ®µæ˜ å°„ã€ç±»åž‹è¯†åˆ«ï¼‰
   â†“
3. æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ– / æ ‡å‡†åŒ–ï¼‰
   â†“
4. æ¨¡åž‹é€‚é…ï¼ˆé€‰æ‹©ä¸åŒæ¨¡åž‹ç±»ï¼‰
   â†“
5. æ¨¡åž‹è®­ç»ƒï¼ˆç»Ÿä¸€çš„ train æŽ¥å£ï¼‰
   â†“
6. æ•°æ®åŽå¤„ç†ï¼ˆå¦‚åå½’ä¸€åŒ–ï¼‰
   â†“
7. æ¨¡åž‹ä¿å­˜ï¼ˆæœ¬åœ°æˆ–ä¸Šä¼  S3ï¼‰
```

------

## âœ… é¡¹ç›®ç»“æž„å»ºè®®

```
ml_pipeline/
â”œâ”€â”€ adapters/                # æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ base.py              # BaseModelAdapter
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ dnn.py
â”‚   â””â”€â”€ llm.py
â”œâ”€â”€ data_cleaning/
â”‚   â”œâ”€â”€ cleaner.py           # æ•°æ®æ¸…æ´—å·¥å…·
â”‚   â””â”€â”€ field_mapper.py      # å­—æ®µæ˜ å°„/ç±»åž‹è¯†åˆ«
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ preprocessor.py      # å½’ä¸€åŒ–ç­‰é¢„å¤„ç†é€»è¾‘
â”‚   â””â”€â”€ postprocessor.py     # åå½’ä¸€åŒ–
â”œâ”€â”€ trainer.py               # è®­ç»ƒæµç¨‹ä¸»å…¥å£
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ io.py                # æ•°æ®æ–‡ä»¶è¯»å–ï¼ˆopenpyxl + csvï¼‰
â”‚   â””â”€â”€ s3_uploader.py       # æ¨¡åž‹æ–‡ä»¶ä¸Šä¼ 
â””â”€â”€ config/
    â””â”€â”€ model_registry.py    # æ³¨å†Œæ”¯æŒçš„æ¨¡åž‹ç±»åž‹
```

------

## âœ… æ ¸å¿ƒç±»ä¸Žæµç¨‹å®žçŽ°ï¼ˆå…³é”®ä»£ç ï¼‰

### 1. `BaseModelAdapter`ï¼ˆæ‰€æœ‰æ¨¡åž‹çš„ç»Ÿä¸€æŽ¥å£ï¼‰

```python
# adapters/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict):
        self.model_params = model_params

    @abstractmethod
    def preprocess(self, df):
        pass

    @abstractmethod
    def train(self, train_data, test_data):
        pass

    @abstractmethod
    def postprocess(self, results):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass
```

------

### 2. `LinearRegressionAdapter` ç¤ºä¾‹

```python
# adapters/linear_regression.py
from sklearn.linear_model import LinearRegression
from .base import BaseModelAdapter

class LinearRegressionAdapter(BaseModelAdapter):
    def __init__(self, model_params):
        super().__init__(model_params)
        self.model = LinearRegression(**model_params)

    def preprocess(self, df):
        return df  # çº¿æ€§æ¨¡åž‹é€šå¸¸ä¸è¦æ±‚é¢å¤–å¤„ç†

    def train(self, train_data, test_data):
        X_train, y_train = train_data
        self.model.fit(X_train, y_train)
        return {"score": self.model.score(*test_data)}

    def postprocess(self, results):
        return results

    def save_model(self, path):
        import joblib
        joblib.dump(self.model, path)
```

------

### 3. è®­ç»ƒæµç¨‹ä¸»å…¥å£

```python
# trainer.py
from adapters import model_registry
from data_cleaning.cleaner import clean_dataset
from preprocessing.preprocessor import preprocess_dataset
from preprocessing.postprocessor import postprocess_result
from utils.io import load_datasets
from utils.s3_uploader import upload_model_file

def train_pipeline(factor_params, model_params, model_type, dataset_files, publish=False):
    # 1. åŠ è½½æ•°æ®é›†
    df = load_datasets(dataset_files)

    # 2. æ•°æ®æ¸…æ´—
    df_clean = clean_dataset(df, factor_params)

    # 3. æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰
    df_preprocessed, scaler = preprocess_dataset(df_clean, factor_params)

    # 4. åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    X = df_preprocessed.drop(columns=["target"])
    y = df_preprocessed["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # 5. èŽ·å–æ¨¡åž‹é€‚é…å™¨
    adapter_class = model_registry.get(model_type)
    if not adapter_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")
    model = adapter_class(model_params)

    # 6. è®­ç»ƒ
    train_result = model.train((X_train, y_train), (X_test, y_test))

    # 7. åŽå¤„ç†
    final_result = model.postprocess(train_result)

    # 8. æ¨¡åž‹ä¿å­˜
    save_path = f"./output_models/{model_type}_{time.time()}.pkl"
    model.save_model(save_path)

    if publish:
        upload_model_file(save_path)

    return final_result
```

------

### 4. æ³¨å†Œæ¨¡åž‹é€‚é…å™¨

```python
# config/model_registry.py
from adapters.linear_regression import LinearRegressionAdapter
from adapters.dnn import DNNAdapter
from adapters.llm import LLMAdapter

model_registry = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
    "llm": LLMAdapter,
}
```

------

### 5. æ¸…æ´— + é¢„å¤„ç†ï¼ˆç¤ºä¾‹ï¼‰

```python
# data_cleaning/cleaner.py
def clean_dataset(df, factor_params):
    # åˆ é™¤å› å­ä¸ºç©ºçš„è¡Œ
    for factor in factor_params:
        df = df[df[factor["name"]].notnull()]
    return df

# preprocessing/preprocessor.py
from sklearn.preprocessing import MinMaxScaler

def preprocess_dataset(df, factor_params):
    numeric_cols = [f["name"] for f in factor_params if f.get("normalize") is True]
    scaler = MinMaxScaler()
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    return df, scaler
```

------

## âœ… ç¤ºä¾‹è°ƒç”¨

```python
result = train_pipeline(
    factor_params=[{"name": "feature1", "normalize": True}, {"name": "feature2", "normalize": False}],
    model_params={"fit_intercept": True},
    model_type="linear_regression",
    dataset_files=["/path/data1.xlsx", "/path/data2.csv"],
    publish=True
)
print(result)
```

------

## âœ… æ€»ç»“

- âœ” ä½¿ç”¨é€‚é…å™¨æ¨¡å¼å°è£…ä¸åŒæ¨¡åž‹è®­ç»ƒé€»è¾‘ï¼Œä¾¿äºŽæ‰©å±•æ–°æ¨¡åž‹ã€‚
- âœ” æ•°æ®æ¸…æ´—ä¸Žé¢„å¤„ç†æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºŽå¤ç”¨å’Œæµ‹è¯•ã€‚
- âœ” æ”¯æŒæ¨¡åž‹è®­ç»ƒåŽå‘å¸ƒï¼ˆå¦‚ä¸Šä¼ åˆ° S3ï¼‰ã€‚
- âœ” é«˜åº¦è§£è€¦ã€ç»“æž„æ¸…æ™°ï¼Œæ–¹ä¾¿å¤šäººåä½œæˆ–æœåŠ¡åŒ–éƒ¨ç½²ã€‚

å¦‚æžœä½ å¸Œæœ›æˆ‘ç»§ç»­è¡¥å…¨æŸä¸ªæ¨¡å—ï¼ˆå¦‚ DNNAdapterã€åå½’ä¸€åŒ–ã€ä»»åŠ¡é˜Ÿåˆ—è°ƒç”¨è®­ç»ƒï¼‰ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šç»§ç»­ç»†åŒ–ã€‚

å¾ˆå¥½ï¼Œè¿™è¯´æ˜Žä½ éœ€è¦æž„å»ºä¸€ä¸ª**çµæ´»å¯é…ç½®çš„æ¨¡åž‹è®­ç»ƒæµç¨‹**ï¼Œå…¶ä¸­æŸäº›æ­¥éª¤ï¼ˆå¦‚æ•°æ®é¢„å¤„ç†ã€å½’ä¸€åŒ–ã€åŽå¤„ç†ã€å‘å¸ƒï¼‰**å¯é€‰æ‰§è¡Œï¼Œå–å†³äºŽæ¨¡åž‹ç±»åž‹æˆ–æ¨¡åž‹é€‚é…å™¨çš„éœ€è¦**ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¢žå¼ºé€‚é…å™¨ç»“æž„ï¼Œä½¿æ¯ä¸ªæ¨¡åž‹**å£°æ˜Žéœ€è¦å“ªäº›æµç¨‹æ­¥éª¤**ï¼Œå¹¶åŠ¨æ€æŽ§åˆ¶è®­ç»ƒæµç¨‹ä¸­å„ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œã€‚

------

## âœ… æ•´ä½“è®¾è®¡ç›®æ ‡

æˆ‘ä»¬å¸Œæœ›å®žçŽ°å¦‚ä¸‹å½¢å¼çš„è®­ç»ƒæŽ§åˆ¶æµç¨‹ï¼š

```text
1. æ¨¡åž‹é€‚é…å™¨å£°æ˜Žï¼šæˆ‘æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼Ÿæ˜¯å¦éœ€è¦åŽå¤„ç†ï¼Ÿ
2. è®­ç»ƒä¸»æµç¨‹è¯»å–å£°æ˜Žå†…å®¹ï¼Œä»…æ‰§è¡Œéœ€è¦çš„æ­¥éª¤
```

------

## âœ… ä¿®æ”¹ç‚¹ä¸€ï¼šæ¨¡åž‹é€‚é…å™¨æŽ¥å£æ‰©å±•

æˆ‘ä»¬ä¸º `BaseModelAdapter` å¢žåŠ ç‰¹æ€§å£°æ˜Žï¼š

```python
# adapters/base.py
class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict):
        self.model_params = model_params

    # ========= æ–°å¢žï¼šæ¨¡åž‹æ‰€éœ€è®­ç»ƒæµç¨‹ç‰¹æ€§å£°æ˜Ž =========
    def need_preprocess(self) -> bool:
        return False

    def need_postprocess(self) -> bool:
        return False

    def need_normalization(self) -> bool:
        return False

    # ========= åŽŸå§‹æŽ¥å£ =========
    @abstractmethod
    def preprocess(self, df):
        pass

    @abstractmethod
    def train(self, train_data, test_data):
        pass

    @abstractmethod
    def postprocess(self, results):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass
```

------

## âœ… ä¿®æ”¹ç‚¹äºŒï¼šé€‚é…å™¨æŒ‰éœ€å®žçŽ°å£°æ˜Žæ–¹æ³•

ä¾‹å¦‚ï¼šçº¿æ€§å›žå½’éœ€è¦å½’ä¸€åŒ–ï¼Œä¸éœ€è¦åŽå¤„ç†

```python
# adapters/linear_regression.py
class LinearRegressionAdapter(BaseModelAdapter):
    def need_preprocess(self):
        return True

    def need_normalization(self):
        return True

    def need_postprocess(self):
        return False

    # ... å…¶ä»–æŽ¥å£åŒå‰ ...
```

------

## âœ… ä¿®æ”¹ç‚¹ä¸‰ï¼šè®­ç»ƒä¸»æµç¨‹æ”¹é€ ï¼ˆæŒ‰éœ€æ‰§è¡Œï¼‰

```python
# trainer.py
def train_pipeline(factor_params, model_params, model_type, dataset_files, publish=False):
    df = load_datasets(dataset_files)
    df_clean = clean_dataset(df, factor_params)

    adapter_class = model_registry.get(model_type)
    if not adapter_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")
    model = adapter_class(model_params)

    # ========= æ˜¯å¦éœ€è¦é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰ =========
    if model.need_preprocess():
        if model.need_normalization():
            df_clean, scaler = preprocess_dataset(df_clean, factor_params)
        else:
            df_clean, scaler = df_clean, None
    else:
        scaler = None

    # ========= åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† =========
    from sklearn.model_selection import train_test_split
    X = df_clean.drop(columns=["target"])
    y = df_clean["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # ========= æ¨¡åž‹è®­ç»ƒ =========
    train_result = model.train((X_train, y_train), (X_test, y_test))

    # ========= å¯é€‰åŽå¤„ç† =========
    if model.need_postprocess():
        final_result = model.postprocess(train_result)
    else:
        final_result = train_result

    # ========= æ¨¡åž‹ä¿å­˜ & å¯é€‰å‘å¸ƒ =========
    save_path = f"./output_models/{model_type}_{int(time.time())}.pkl"
    model.save_model(save_path)

    if publish:
        upload_model_file(save_path)

    return final_result
```

------

## âœ… æ€»ç»“ï¼šä½ èŽ·å¾—äº†ä»€ä¹ˆ

| åŠŸèƒ½               | å®žçŽ°æ–¹å¼                                       |
| ------------------ | ---------------------------------------------- |
| æ¯æ­¥æ˜¯å¦æ‰§è¡Œ       | æ¨¡åž‹é€‚é…å™¨æä¾› `.need_xxx()` æŽ¥å£è¿›è¡ŒæŽ§åˆ¶      |
| æ‰©å±•æ”¯æŒçš„æ¨¡åž‹ç±»åž‹ | åªéœ€å®žçŽ°æ–°æ¨¡åž‹é€‚é…å™¨ç±»ï¼Œå£°æ˜Žéœ€æ±‚å¹¶å®žçŽ°æŽ¥å£     |
| çµæ´»æ€§             | æµç¨‹æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥æŒ‰æ¨¡åž‹è¦æ±‚å†³å®šæ˜¯å¦æ‰§è¡Œ       |
| è§£è€¦æ€§             | æ¯ä¸ªæ¨¡å—ï¼ˆæ¸…æ´—ã€é¢„å¤„ç†ã€è®­ç»ƒã€åŽå¤„ç†ï¼‰éƒ½å¯å¤ç”¨ |

------

å¦‚æžœä½ å¸Œæœ›ä¸‹ä¸€æ­¥æˆ‘å¸®ä½ ï¼š

- æ·»åŠ æŸä¸ªå…·ä½“æ¨¡åž‹é€‚é…å™¨ï¼ˆå¦‚ DNNã€LLMï¼‰
- æ”¯æŒæŸäº›æ¨¡åž‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æ—¥å¿—ï¼ˆç”¨äºŽå‰ç«¯æ˜¾ç¤ºï¼‰
- é€šè¿‡é˜Ÿåˆ—æˆ–ä»»åŠ¡ç®¡ç†å™¨ï¼ˆå¦‚ Celeryï¼‰æ‰§è¡Œè®­ç»ƒä»»åŠ¡

æ¬¢è¿Žç»§ç»­å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šç›´æŽ¥ç»™å‡ºå¯¹åº”ä»£ç ã€‚

æ˜Žç™½äº†ï¼Œä½ çš„éœ€æ±‚æ˜¯ï¼š

> åœ¨æ¨¡åž‹è®­ç»ƒæµç¨‹ä¸­ï¼Œæ¯ä¸ªæ­¥éª¤ï¼ˆå¦‚å½’ä¸€åŒ–ã€é¢„å¤„ç†ã€åŽå¤„ç†ç­‰ï¼‰æ˜¯å¦æ‰§è¡Œç”±**ä¸¤æ–¹å†³å®š**ï¼š

1. **æ¨¡åž‹åˆ¶ä½œè€…ï¼ˆé€‚é…å™¨å†…éƒ¨å®šä¹‰ï¼‰**ï¼šè®¾ç½®é»˜è®¤æµç¨‹è¡Œä¸ºï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ã€‚
2. **å‰ç«¯ç”¨æˆ·é…ç½®**ï¼šç”¨æˆ·å¯ä»¥è¦†ç›–éƒ¨åˆ†è¡Œä¸ºï¼Œå¦‚æžœé€‚é…å™¨å…è®¸ã€‚

å¹¶ä¸”ï¼š

- **æ¨¡åž‹é€‚é…å™¨ä¼˜å…ˆçº§é«˜**ï¼šå¦‚æžœé€‚é…å™¨**å¼ºåˆ¶æŸæ­¥éª¤æ‰§è¡Œ/è·³è¿‡**ï¼Œåˆ™ç”¨æˆ·æ— æ³•æŽ§åˆ¶ã€‚
- **ç”¨æˆ·æŽ§åˆ¶ä¼˜å…ˆä½¿ç”¨**ï¼šå¦‚æžœé€‚é…å™¨æ²¡æœ‰æ˜Žç¡®å¼ºåˆ¶è¦æ±‚æ‰§è¡Œæˆ–è·³è¿‡ï¼Œåˆ™ç”¨æˆ·æŽ§åˆ¶ç”Ÿæ•ˆã€‚

------

## âœ… æ•´ä½“æŽ§åˆ¶é€»è¾‘ï¼ˆæµç¨‹èŠ‚ç‚¹æ‰§è¡Œåˆ¤æ–­ï¼‰

æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªé€šç”¨åˆ¤æ–­å‡½æ•°ï¼š

```python
def should_execute_step(
    step_name: str,
    user_control: dict,
    adapter_control: dict
) -> bool:
    """
    å†³å®šæ˜¯å¦æ‰§è¡ŒæŸä¸€æµç¨‹èŠ‚ç‚¹ï¼š
    - adapter_control ä¼˜å…ˆçº§æ›´é«˜ï¼ˆå¿…é¡»æ‰§è¡Œæˆ–å¿…é¡»è·³è¿‡ï¼‰
    - å¦åˆ™çœ‹ç”¨æˆ·æ˜¯å¦ä¼ å…¥æŽ§åˆ¶å€¼
    - å¦åˆ™é»˜è®¤æ‰§è¡Œä¸º False
    """
    if step_name in adapter_control:
        return adapter_control[step_name]
    return user_control.get(step_name, False)
```

------

## âœ… æ¨¡åž‹é€‚é…å™¨å®šä¹‰æŽ¥å£ï¼ˆå¸¦é»˜è®¤è¡Œä¸ºï¼‰

æ¯ä¸ªæ¨¡åž‹å¯ä»¥é€šè¿‡é€‚é…å™¨å®šä¹‰æµç¨‹è¡Œä¸ºï¼Œä¾‹å¦‚ï¼š

```python
class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict):
        self.model_params = model_params

    @property
    def pipeline_control(self) -> dict:
        """
        å®šä¹‰æ¨¡åž‹é»˜è®¤è¡Œä¸ºï¼š
        {
            "preprocess": True,   # è¡¨ç¤ºæ¨¡åž‹éœ€è¦é¢„å¤„ç†
            "normalize": False,   # è¡¨ç¤ºæ¨¡åž‹ä¸éœ€è¦å½’ä¸€åŒ–
            "postprocess": None,  # è¡¨ç¤ºæ˜¯å¦éœ€è¦åŽå¤„ç†ç”±ç”¨æˆ·å†³å®š
        }
        None è¡¨ç¤ºæœªæŒ‡å®šï¼Œå…è®¸ç”¨æˆ·å†³å®šã€‚
        """
        return {}

    @abstractmethod
    def preprocess(self, df):
        pass

    @abstractmethod
    def train(self, train_data, test_data):
        pass

    @abstractmethod
    def postprocess(self, results):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass
```

ç¤ºä¾‹ï¼šçº¿æ€§å›žå½’é€‚é…å™¨å¼ºåˆ¶éœ€è¦å½’ä¸€åŒ–ã€ä¸åšåŽå¤„ç†ï¼š

```python
class LinearRegressionAdapter(BaseModelAdapter):
    @property
    def pipeline_control(self):
        return {
            "preprocess": True,
            "normalize": True,
            "postprocess": False
        }

    # å®žçŽ°ç›¸å…³æŽ¥å£...
```

------

## âœ… ä¸»è®­ç»ƒæµç¨‹è°ƒæ•´ï¼ˆå¼•å…¥åŒé‡æŽ§åˆ¶ï¼‰

```python
def train_pipeline(
    factor_params: dict,
    model_params: dict,
    model_type: str,
    dataset_files: List[Union[str, IO]],
    user_pipeline_control: dict,
    publish: bool = False
):
    df = load_datasets(dataset_files)
    df_clean = clean_dataset(df, factor_params)

    adapter_class = model_registry.get(model_type)
    if not adapter_class:
        raise ValueError(f"æ¨¡åž‹ç±»åž‹ä¸æ”¯æŒ: {model_type}")

    model = adapter_class(model_params)
    adapter_control = model.pipeline_control

    # åˆ¤æ–­æ˜¯å¦æ‰§è¡Œå„ä¸ªæ­¥éª¤
    do_preprocess = should_execute_step("preprocess", user_pipeline_control, adapter_control)
    do_normalize = should_execute_step("normalize", user_pipeline_control, adapter_control)
    do_postprocess = should_execute_step("postprocess", user_pipeline_control, adapter_control)

    # ========= é¢„å¤„ç† =========
    scaler = None
    if do_preprocess:
        if do_normalize:
            df_clean, scaler = preprocess_dataset(df_clean, factor_params)
        else:
            df_clean, scaler = df_clean, None

    # ========= åˆ’åˆ†æ•°æ®é›† =========
    from sklearn.model_selection import train_test_split
    X = df_clean.drop(columns=["target"])
    y = df_clean["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # ========= è®­ç»ƒ =========
    train_result = model.train((X_train, y_train), (X_test, y_test))

    # ========= åŽå¤„ç† =========
    if do_postprocess:
        final_result = model.postprocess(train_result)
    else:
        final_result = train_result

    # ========= ä¿å­˜ & å‘å¸ƒ =========
    save_path = f"./output_models/{model_type}_{int(time.time())}.pkl"
    model.save_model(save_path)

    if publish:
        upload_model_file(save_path)

    return final_result
```

------

## âœ… ä½¿ç”¨ç¤ºä¾‹

```python
# ç”¨æˆ·ä¼ å…¥çš„æŽ§åˆ¶å‚æ•°
user_pipeline_control = {
    "preprocess": True,
    "normalize": False,
    "postprocess": True
}

train_pipeline(
    factor_params={...},
    model_params={...},
    model_type="linear_regression",
    dataset_files=["/path/to/data1.csv", "/path/to/data2.csv"],
    user_pipeline_control=user_pipeline_control,
    publish=True
)
```

å³ä½¿ç”¨æˆ·è®¾ç½® `normalize=False`ï¼Œä½†å¦‚æžœ `adapter.pipeline_control["normalize"] = True`ï¼Œä»ä¼šæ‰§è¡Œå½’ä¸€åŒ–ã€‚

------

## âœ… æ€»ç»“

| åŠŸèƒ½                 | å®žçŽ°æ–¹å¼                                         |
| -------------------- | ------------------------------------------------ |
| æ¯ä¸ªèŠ‚ç‚¹æŽ§åˆ¶æ˜¯å¦æ‰§è¡Œ | é€‚é…å™¨ `pipeline_control` å’Œç”¨æˆ·ä¼ å‚å…±åŒå†³å®š     |
| ä¼˜å…ˆçº§æŽ§åˆ¶           | é€‚é…å™¨ä¼˜å…ˆï¼›ç”¨æˆ·ä¼ å‚ä¸ºæ¬¡é€‰ï¼›å¦åˆ™é»˜è®¤ False       |
| æ–°å¢žæ¨¡åž‹é€‚é…ç®€å•     | åªéœ€å£°æ˜Žé»˜è®¤æµç¨‹è¡Œä¸º + å®žçŽ°è®­ç»ƒæŽ¥å£å³å¯          |
| å¯ç»´æŠ¤æ€§é«˜           | æ¯ä¸ªæ¨¡å—èŒè´£æ¸…æ™°ï¼ˆæ•°æ®å¤„ç†ã€è®­ç»ƒé€»è¾‘ã€ä¿å­˜é€»è¾‘ï¼‰ |

------

å¦‚æžœä½ éœ€è¦æˆ‘ç»§ç»­æä¾›ï¼š

- æµç¨‹ç»“æž„å°è£…ä¸ºæ¨¡å—åŒ…
- ä»»åŠ¡æŒä¹…åŒ–ä¸Ž Celery é˜Ÿåˆ—é›†æˆ
- å°†è®­ç»ƒæµç¨‹æ—¥å¿—å›žä¼ å‰ç«¯

æ¬¢è¿Žç»§ç»­å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ç»§ç»­è¡¥å……å®Œæ•´ç¤ºä¾‹ã€‚

å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å°†â€œæ¨¡åž‹è®­ç»ƒæµç¨‹â€å°è£…ä¸ºæ¨¡å—åŒ…ï¼Œå¹¶æ”¯æŒå°†è®­ç»ƒæ—¥å¿—å®žæ—¶ï¼ˆæµå¼ï¼‰å›žä¼ å‰ç«¯çš„å®Œæ•´è®¾è®¡æ€è·¯ä¸Žä»£ç å®žçŽ°æ–¹æ¡ˆï¼š

------

## âœ… 1. æ¨¡å—ç»“æž„è®¾è®¡

æˆ‘ä»¬å°†æ•´ä¸ªè®­ç»ƒæµç¨‹å°è£…ä¸ºä¸€ä¸ªæ¨¡å—åŒ…ï¼Œä¾‹å¦‚ `ai_pipeline`ï¼š

```
ai_pipeline/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py                  # é€‚é…å™¨åŸºç±»ä¸Žé€šç”¨æŽ§åˆ¶é€»è¾‘
â”œâ”€â”€ adapters/                # å„ç§æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data_handler.py          # æ•°æ®åŠ è½½ã€æ¸…æ´—ã€é¢„å¤„ç†ç­‰
â”œâ”€â”€ pipeline.py              # ä¸»æµç¨‹æŽ§åˆ¶
â”œâ”€â”€ logger.py                # æ—¥å¿—å·¥å…·ï¼ˆæµå¼è¾“å‡ºï¼‰
â””â”€â”€ registry.py              # æ¨¡åž‹æ³¨å†Œå™¨
```

------

## âœ… 2. æµå¼æ—¥å¿— logger å®žçŽ°ï¼ˆ`logger.py`ï¼‰

```python
import sys
import time
from typing import Callable


class StreamLogger:
    def __init__(self, log_func: Callable[[str], None] = print):
        self.log_func = log_func

    def log(self, msg: str):
        timestamp = time.strftime("[%Y-%m-%d %H:%M:%S]")
        self.log_func(f"{timestamp} {msg}")

# ç”¨äºŽé˜Ÿåˆ—æˆ– WebSocket åœºæ™¯çš„å›žè°ƒæ³¨å…¥
logger = StreamLogger()
```

------

## âœ… 3. æ•°æ®å¤„ç†é€»è¾‘ï¼ˆ`data_handler.py`ï¼‰

ç•¥ï¼ˆä½ å·²ç»æœ‰äº†è¯»å–å¤šä¸ªæ–‡ä»¶ã€å­—æ®µåŒ¹é…ã€å½’ä¸€åŒ–ã€åå½’ä¸€åŒ–ç­‰é€»è¾‘ï¼‰ï¼Œåªè¦å°è£…æˆå‡½æ•°å¹¶æŽ¥å— logger å®žä¾‹å³å¯ã€‚

ä¾‹å¦‚ï¼š

```python
def clean_and_preprocess(datasets, factor_config, normalize=False, logger=None):
    logger = logger or print
    logger("å¼€å§‹åŠ è½½ä¸Žåˆå¹¶æ•°æ®é›†")
    # åˆå¹¶ã€æ¸…æ´—ã€å½’ä¸€åŒ–ç­‰æ“ä½œ...
```

------

## âœ… 4. é€‚é…å™¨å®šä¹‰ï¼ˆ`base.py`ï¼‰

```python
from abc import ABC, abstractmethod


class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict, logger=None):
        self.model_params = model_params
        self.logger = logger or print

    @property
    def pipeline_control(self) -> dict:
        return {}

    @abstractmethod
    def train(self, X_train, y_train, X_test, y_test):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass

    def postprocess(self, results):
        return results
```

------

## âœ… 5. è®­ç»ƒä¸»æµç¨‹ï¼ˆ`pipeline.py`ï¼‰

```python
from .registry import model_registry
from .data_handler import clean_and_preprocess
from .logger import logger
from .base import BaseModelAdapter
from .utils import should_execute_step
from sklearn.model_selection import train_test_split


def train_pipeline(
    factor_params: dict,
    model_params: dict,
    model_type: str,
    dataset_files: list,
    user_pipeline_control: dict,
    logger_instance=None
):
    log = logger_instance or logger.log
    log(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: æ¨¡åž‹ç±»åž‹={model_type}")

    # 1. åŠ è½½æ¨¡åž‹é€‚é…å™¨
    model_class = model_registry.get(model_type)
    if not model_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")

    model = model_class(model_params, logger=log)
    adapter_control = model.pipeline_control

    # 2. æ•°æ®å¤„ç†
    normalize = should_execute_step("normalize", user_pipeline_control, adapter_control)
    df, scaler = clean_and_preprocess(dataset_files, factor_params, normalize, logger=log)

    X = df.drop(columns=["target"])
    y = df["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # 3. è®­ç»ƒ
    log("å¼€å§‹æ¨¡åž‹è®­ç»ƒ...")
    result = model.train(X_train, y_train, X_test, y_test)

    # 4. åŽå¤„ç†ï¼ˆå¯é€‰ï¼‰
    if should_execute_step("postprocess", user_pipeline_control, adapter_control):
        result = model.postprocess(result)

    # 5. ä¿å­˜æ¨¡åž‹
    model_path = f"./outputs/model_{model_type}_{int(time.time())}.pkl"
    model.save_model(model_path)
    log(f"æ¨¡åž‹ä¿å­˜åˆ°: {model_path}")

    return result
```

------

## âœ… 6. WebSocket å›žä¼ ï¼ˆé€‚ç”¨äºŽ Web å‰ç«¯ï¼‰

### Django + Channels ç¤ºä¾‹ï¼ˆconsumerï¼‰

```python
# consumers.py
from channels.generic.websocket import WebsocketConsumer
import threading
from ai_pipeline import train_pipeline
from ai_pipeline.logger import StreamLogger

class TrainingConsumer(WebsocketConsumer):
    def connect(self):
        self.accept()

    def disconnect(self, code):
        pass

    def receive(self, text_data=None, bytes_data=None):
        import json
        payload = json.loads(text_data)
        
        def send_log(msg):
            self.send(json.dumps({"type": "log", "message": msg}))

        logger = StreamLogger(log_func=send_log)

        # åŽå°çº¿ç¨‹è·‘ä»»åŠ¡
        threading.Thread(target=train_pipeline, kwargs={
            "factor_params": payload["factorParams"],
            "model_params": payload["modelParams"],
            "model_type": payload["modelType"],
            "dataset_files": payload["datasetFiles"],
            "user_pipeline_control": payload.get("control", {}),
            "logger_instance": logger.log
        }).start()
```

------

## âœ… æ€»ç»“ï¼šå…³é”®ç‰¹æ€§ä¸€è§ˆ

| åŠŸèƒ½ç‚¹                         | å®žçŽ°æ–¹å¼                         |
| ------------------------------ | -------------------------------- |
| è®­ç»ƒæ¨¡å—è§£è€¦å°è£…               | `ai_pipeline` åŒ…                 |
| æ”¯æŒå¤šç§æ¨¡åž‹é€‚é…               | é€‚é…å™¨æ¨¡å¼ + æ³¨å†Œå™¨              |
| æŽ§åˆ¶æµç¨‹èŠ‚ç‚¹æ‰§è¡Œ               | `should_execute_step()` æ–¹æ³•     |
| åŒé‡æŽ§åˆ¶ï¼ˆæ¨¡åž‹ä¼˜å…ˆï¼Œç”¨æˆ·å¯æŽ§ï¼‰ | `pipeline_control + user config` |
| æ—¥å¿—å¯å›žä¼                      | è‡ªå®šä¹‰ logger + WebSocket        |
| å¯é›†æˆ Celery                  | logger ä¸Žä»»åŠ¡çº¿ç¨‹è§£è€¦            |

------

å¦‚éœ€æˆ‘ï¼š

- å¢žåŠ è®­ç»ƒç»“æžœæŒ‡æ ‡æ—¥å¿—ï¼ˆRÂ²ã€MAEç­‰ï¼‰
- å°è£…æˆ pip install åŒ…ç»“æž„
- æä¾›å‰ç«¯æ¶ˆæ¯å¤„ç†ç¤ºä¾‹ï¼ˆå¦‚ Vue + Element Plusï¼‰

éƒ½å¯ä»¥ç»§ç»­æ‰©å±•ã€‚æ˜¯å¦éœ€è¦æˆ‘æ‰“åŒ…æˆä¸€ä¸ªå¯ç›´æŽ¥è¿è¡Œçš„é¡¹ç›®ç»“æž„ç¤ºä¾‹ï¼Ÿ

```python
# ai_pipeline/__init__.py
# ç©ºæ–‡ä»¶ï¼Œä½¿ ai_pipeline æˆä¸ºæ¨¡å—åŒ…

# ai_pipeline/logger.py
import time
from typing import Callable

class StreamLogger:
    def __init__(self, log_func: Callable[[str], None] = print):
        self.log_func = log_func

    def log(self, msg: str):
        timestamp = time.strftime("[%Y-%m-%d %H:%M:%S]")
        self.log_func(f"{timestamp} {msg}")

logger = StreamLogger()

# ai_pipeline/utils.py
def should_execute_step(step: str, user_control: dict, adapter_control: dict):
    if step in adapter_control:
        return adapter_control[step]
    return user_control.get(step, False)

# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params: dict, logger=None):
        self.model_params = model_params
        self.logger = logger or print

    @property
    def pipeline_control(self) -> dict:
        return {}

    @abstractmethod
    def train(self, X_train, y_train, X_test, y_test):
        pass

    @abstractmethod
    def save_model(self, path: str):
        pass

    def postprocess(self, results):
        return results

# ai_pipeline/adapters/linear_regression.py
import joblib
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from ai_pipeline.base import BaseModelAdapter

class LinearRegressionAdapter(BaseModelAdapter):
    def __init__(self, model_params: dict, logger=None):
        super().__init__(model_params, logger)
        self.model = LinearRegression(**model_params)

    @property
    def pipeline_control(self):
        return {"normalize": True, "postprocess": True}

    def train(self, X_train, y_train, X_test, y_test):
        self.logger("è®­ç»ƒçº¿æ€§å›žå½’æ¨¡åž‹...")
        self.model.fit(X_train, y_train)
        y_pred = self.model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        self.logger(f"æ¨¡åž‹è¯„ä¼° - MSE: {mse:.4f}, R2: {r2:.4f}")
        return {"mse": mse, "r2": r2}

    def save_model(self, path: str):
        joblib.dump(self.model, path)
        self.logger(f"æ¨¡åž‹ä¿å­˜åˆ° {path}")

# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter

model_registry = {
    "linear_regression": LinearRegressionAdapter
}

# ai_pipeline/data_handler.py
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ç¤ºä¾‹å› å­é…ç½®ï¼š{"age": {"type": "numeric", "normalize": True}}
def clean_and_preprocess(dataset_files, factor_config, normalize=False, logger=None):
    logger = logger or print
    logger("åŠ è½½å¹¶åˆå¹¶æ•°æ®é›†...")
    
    df_list = []
    for path in dataset_files:
        df = pd.read_csv(path)
        df_list.append(df)

    # åˆå¹¶å¹¶å¯¹é½åˆ—åï¼ˆé¡ºåºä¸ä¸€è‡´æ—¶ï¼‰
    base_cols = df_list[0].columns
    df_list = [df[base_cols] for df in df_list]
    df = pd.concat(df_list, ignore_index=True).drop_duplicates()

    logger("æ ¹æ®å› å­é…ç½®è¿›è¡Œæ•°æ®æ¸…æ´—...")
    for col, config in factor_config.items():
        if config.get("type") == "numeric":
            df = df[df[col].notna()]
            if "min" in config:
                df = df[df[col] >= config["min"]]
            if "max" in config:
                df = df[df[col] <= config["max"]]
        elif config.get("type") == "category":
            allowed = config.get("values")
            if allowed:
                df = df[df[col].isin(allowed)]

    scaler = None
    if normalize:
        numeric_cols = [col for col, conf in factor_config.items() if conf.get("type") == "numeric" and conf.get("normalize")]
        if numeric_cols:
            logger("å¯¹æ•°å€¼åž‹å­—æ®µè¿›è¡Œå½’ä¸€åŒ–...")
            scaler = MinMaxScaler()
            df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

    return df, scaler

# ai_pipeline/pipeline.py
import time
from sklearn.model_selection import train_test_split
from ai_pipeline.registry import model_registry
from ai_pipeline.logger import logger
from ai_pipeline.utils import should_execute_step
from ai_pipeline.data_handler import clean_and_preprocess

def train_pipeline(
    factor_params: dict,
    model_params: dict,
    model_type: str,
    dataset_files: list,
    user_pipeline_control: dict,
    logger_instance=None
):
    log = logger_instance or logger.log
    log(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: æ¨¡åž‹ç±»åž‹={model_type}")

    model_class = model_registry.get(model_type)
    if not model_class:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {model_type}")

    model = model_class(model_params, logger=log)
    adapter_control = model.pipeline_control

    normalize = should_execute_step("normalize", user_pipeline_control, adapter_control)
    df, scaler = clean_and_preprocess(dataset_files, factor_params, normalize, logger=log)

    X = df.drop(columns=["target"])
    y = df["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    log("å¼€å§‹æ¨¡åž‹è®­ç»ƒ...")
    result = model.train(X_train, y_train, X_test, y_test)

    if should_execute_step("postprocess", user_pipeline_control, adapter_control):
        result = model.postprocess(result)

    model_path = f"./outputs/model_{model_type}_{int(time.time())}.pkl"
    model.save_model(model_path)
    log(f"æ¨¡åž‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è‡³: {model_path}")

    return result

```

~~~python
### ðŸ§  æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆbase.pyï¼‰
```python
# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params, logger):
        self.model_params = model_params
        self.logger = logger

    @abstractmethod
    def train(self, X_train, y_train):
        pass

    @abstractmethod
    def predict(self, X):
        pass

    @abstractmethod
    def save(self, path):
        pass
```

### âœ… æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆregistry.pyï¼‰
```python
# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter
from ai_pipeline.adapters.dnn import DNNAdapter

MODEL_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
}
```

### âž• æ–°å¢ž DNN æ¨¡åž‹é€‚é…å™¨ï¼ˆadapters/dnn.pyï¼‰
```python
# ai_pipeline/adapters/dnn.py
from ai_pipeline.base import BaseModelAdapter
from sklearn.neural_network import MLPRegressor
import joblib

class DNNAdapter(BaseModelAdapter):
    def __init__(self, model_params, logger):
        super().__init__(model_params, logger)
        self.model = MLPRegressor(**model_params)

    def train(self, X_train, y_train):
        self.logger("[DNN] å¼€å§‹è®­ç»ƒ")
        self.model.fit(X_train, y_train)
        self.logger("[DNN] è®­ç»ƒå®Œæˆ")

    def predict(self, X):
        return self.model.predict(X)

    def save(self, path):
        joblib.dump(self.model, path)
```

### ðŸ§ª ç¤ºä¾‹æµ‹è¯•è°ƒç”¨æ›´æ–°
```python
from ai_pipeline.pipeline import train_pipeline

# ä½¿ç”¨æ·±åº¦ç¥žç»ç½‘ç»œæ¨¡åž‹è®­ç»ƒ
result = train_pipeline(
    factor_params={
        "feature1": {"type": "numeric", "normalize": True},
        "feature2": {"type": "numeric", "normalize": True},
    },
    model_params={
        "hidden_layer_sizes": (50, 50),
        "activation": "relu",
        "max_iter": 200
    },
    model_type="dnn",
    dataset_files=["dataset1.csv", "dataset2.csv"],
    user_pipeline_control={
        "normalize": True,
        "postprocess": True,
    }
)
```

### âœ… æµç¨‹å…¼å®¹æ€§è¯´æ˜Ž
- æ¨¡åž‹åªéœ€è¦ç»§æ‰¿ `BaseModelAdapter`
- æ³¨å†Œåœ¨ `registry.py` çš„ `MODEL_REGISTRY` ä¸­
- æŽ§åˆ¶æµç¨‹ç»“æž„å’Œæ—¥å¿—æœºåˆ¶é€šç”¨ï¼Œé€‚é…ä»»æ„ç»“æž„åŒ–æ¨¡åž‹

å¦‚éœ€æ·»åŠ æ”¯æŒåˆ†ç±»æ¨¡åž‹ã€NLP æ¨¡åž‹ã€æˆ–å¤–éƒ¨æœåŠ¡æ¨¡åž‹ï¼ˆå¦‚ OpenAIï¼‰ï¼Œç»“æž„å®Œå…¨é€‚é…ï¼Œåªéœ€æŒ‰ä¸Šè¿°ç»“æž„æ–°å¢žé€‚é…å™¨ã€‚æ˜¯å¦è¿˜éœ€è¦æˆ‘æ·»åŠ ä¸€ä¸ªå¦‚ HuggingFace BERT çš„é€‚é…å™¨ï¼Ÿ

~~~

~~~python
### ðŸ§  æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆbase.pyï¼‰
```python
# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params, logger):
        self.model_params = model_params  # æ¨¡åž‹å‚æ•°ï¼ˆå¦‚å±‚æ•°ã€å­¦ä¹ çŽ‡ç­‰ï¼‰
        self.logger = logger              # æ—¥å¿—å‡½æ•°æˆ–æµå¼æ—¥å¿—è¾“å‡ºå™¨

    @abstractmethod
    def train(self, X_train, y_train):
        pass  # å­ç±»å®žçŽ°ï¼šè®­ç»ƒæ¨¡åž‹

    @abstractmethod
    def predict(self, X):
        pass  # å­ç±»å®žçŽ°ï¼šæ‰§è¡Œé¢„æµ‹

    @abstractmethod
    def save(self, path):
        pass  # å­ç±»å®žçŽ°ï¼šä¿å­˜æ¨¡åž‹
```

### âœ… æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆregistry.pyï¼‰
```python
# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter
from ai_pipeline.adapters.dnn import DNNAdapter
from ai_pipeline.adapters.huggingface import HuggingFaceTextAdapter

MODEL_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
    "huggingface_text": HuggingFaceTextAdapter,
}
```

### âž• HuggingFace æ–‡æœ¬æ¨¡åž‹é€‚é…å™¨ï¼ˆadapters/huggingface.pyï¼‰
```python
# ai_pipeline/adapters/huggingface.py
from ai_pipeline.base import BaseModelAdapter
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import os

class HuggingFaceTextAdapter(BaseModelAdapter):
    def __init__(self, model_params, logger):
        super().__init__(model_params, logger)
        model_name = model_params.get("model_name", "distilbert-base-uncased-finetuned-sst-2-english")
        self.logger(f"[HF] åŠ è½½æ¨¡åž‹: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.pipeline = pipeline("sentiment-analysis", model=self.model, tokenizer=self.tokenizer)

    def train(self, X_train, y_train):
        self.logger("[HF] æ–‡æœ¬æ¨¡åž‹å½“å‰ä¸ºé¢„è®­ç»ƒæŽ¨ç†æ¨¡å¼ï¼Œè·³è¿‡è®­ç»ƒé˜¶æ®µ")

    def predict(self, X):
        self.logger("[HF] å¼€å§‹æ–‡æœ¬é¢„æµ‹")
        texts = X["text"] if isinstance(X, dict) else X  # æ”¯æŒ DataFrame æˆ–åŽŸå§‹åˆ—è¡¨
        return self.pipeline(texts)

    def save(self, path):
        self.logger("[HF] ä¿å­˜æ¨¡åž‹å’Œåˆ†è¯å™¨")
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)
```

### âœ… è¯´æ˜Žï¼ˆæ¯è¡Œè§£é‡Šï¼‰
```python
model_name = model_params.get("model_name", "distilbert-base-uncased-finetuned-sst-2-english")
# èŽ·å–æ¨¡åž‹åç§°ï¼ˆç”¨æˆ·ä¼ å…¥æˆ–é»˜è®¤ä½¿ç”¨æƒ…æ„Ÿåˆ†æžæ¨¡åž‹ï¼‰

self.tokenizer = AutoTokenizer.from_pretrained(model_name)
self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
# åŠ è½½ HuggingFace æ¨¡åž‹å’Œ tokenizer

self.pipeline = pipeline("sentiment-analysis", model=self.model, tokenizer=self.tokenizer)
# å°è£…ä¸º transformers pipelineï¼Œä¾¿äºŽç›´æŽ¥æŽ¨ç†
```

### ðŸ§ª ç¤ºä¾‹æµ‹è¯•è°ƒç”¨ï¼ˆæ–‡æœ¬æ¨¡åž‹ï¼‰
```python
from ai_pipeline.pipeline import train_pipeline

result = train_pipeline(
    factor_params={},  # æ–‡æœ¬æ¨¡åž‹é€šå¸¸ä¸éœ€è¦æ•°å€¼å› å­
    model_params={"model_name": "distilbert-base-uncased-finetuned-sst-2-english"},
    model_type="huggingface_text",
    dataset_files=["texts.csv"],
    user_pipeline_control={
        "normalize": False,
        "postprocess": False,
    }
)
```

### ðŸ“¦ è¾“å…¥æ–‡æœ¬æ•°æ®è¦æ±‚
- CSV æ–‡ä»¶åº”åŒ…å« `text` å­—æ®µåˆ—ï¼š
```
text
"I love this product."
"This is terrible."
```

---
å¦‚éœ€æ‰©å±•ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ã€å¤šè¯­è¨€æ¨¡åž‹ï¼Œåªéœ€æ›´æ¢æ¨¡åž‹åå’Œ pipeline ç±»åž‹å³å¯ã€‚
æ˜¯å¦æŽ¥ä¸‹æ¥éœ€è¦æŽ¥å…¥ç±»ä¼¼ DeepSeek æˆ– OpenAI API çš„æŽ¨ç†æ¨¡åž‹ï¼Ÿ

~~~

~~~python
## âœ… AI å¹³å°é€šç”¨æ¨¡åž‹è®­ç»ƒæµç¨‹æ¨¡å—ï¼ˆè®­ç»ƒæµç¨‹ã€é€‚é…å™¨ç»“æž„ã€API æŽ¨ç†æ¨¡åž‹æ”¯æŒï¼‰

---

## ðŸ“ é¡¹ç›®æ¨¡å—æ¦‚è§ˆ
```bash
ai_pipeline/
â”œâ”€â”€ base.py                    # æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆå®šä¹‰æŽ¥å£ï¼‰
â”œâ”€â”€ pipeline.py                # ä¸»è®­ç»ƒæµç¨‹æ¨¡å—
â”œâ”€â”€ registry.py                # æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆç”¨äºŽæ¨¡åž‹ç±»åž‹æŸ¥æ‰¾ï¼‰
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ linear_regression.py   # sklearn çº¿æ€§å›žå½’æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ dnn.py                 # sklearn DNN æ¨¡åž‹é€‚é…å™¨
â”‚   â”œâ”€â”€ huggingface.py         # æœ¬åœ° transformer æ¨¡åž‹é€‚é…å™¨
â”‚   â””â”€â”€ openai_api.py          # å¤–éƒ¨ OpenAI / DeepSeek æŽ¥å£é€‚é…å™¨
```

---

## ðŸ“¦ æ¨¡åž‹é€‚é…å™¨åŸºç±»ï¼ˆbase.pyï¼‰
```python
# ai_pipeline/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, model_params, logger):
        self.model_params = model_params  # æ¨¡åž‹åˆå§‹åŒ–å‚æ•°
        self.logger = logger              # æ—¥å¿—å‡½æ•°

    @abstractmethod
    def train(self, X_train, y_train):
        pass  # è®­ç»ƒæ–¹æ³•

    @abstractmethod
    def predict(self, X):
        pass  # é¢„æµ‹æ–¹æ³•

    @abstractmethod
    def save(self, path):
        pass  # æ¨¡åž‹ä¿å­˜æ–¹æ³•
```

---

## ðŸ§  å¤–éƒ¨ API æ¨¡åž‹é€‚é…å™¨ï¼ˆå¦‚ OpenAI æˆ– DeepSeekï¼‰ï¼ˆadapters/openai_api.pyï¼‰
```python
# ai_pipeline/adapters/openai_api.py
import openai
from ai_pipeline.base import BaseModelAdapter

class OpenAIChatAdapter(BaseModelAdapter):
    def __init__(self, model_params, logger):
        super().__init__(model_params, logger)
        self.api_key = model_params.get("api_key")
        self.model = model_params.get("model", "gpt-4")
        openai.api_key = self.api_key
        logger(f"[OpenAI] ä½¿ç”¨æ¨¡åž‹ï¼š{self.model}")

    def train(self, X_train, y_train):
        self.logger("[OpenAI] å¤–éƒ¨ API æ— éœ€è®­ç»ƒ")

    def predict(self, X):
        prompt = X.get("prompt")
        self.logger(f"[OpenAI] è¯·æ±‚æç¤ºè¯: {prompt}")

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        output = ""
        for chunk in response:
            delta = chunk["choices"][0]["delta"].get("content", "")
            output += delta
            self.logger(delta, stream=True)  # æµå¼è¿”å›ž
        return output

    def save(self, path):
        self.logger("[OpenAI] API æ¨¡åž‹æ— éœ€ä¿å­˜")
```

### ðŸ” æ¯è¡Œè§£é‡Šï¼š
```python
openai.api_key = self.api_key
# è®¾ç½® OpenAI API å¯†é’¥

response = openai.ChatCompletion.create(..., stream=True)
# å¯ç”¨æµå¼å“åº”ï¼Œé€å—æŽ¥æ”¶æ¨¡åž‹è¾“å‡º

for chunk in response:
    delta = chunk["choices"][0]["delta"].get("content", "")
    self.logger(delta, stream=True)
# å°†å†…å®¹é€æ¡æµå¼è¾“å‡ºç»™å‰ç«¯
```

---

## ðŸ”Œ æ¨¡åž‹æ³¨å†Œè¡¨ï¼ˆregistry.pyï¼‰
```python
# ai_pipeline/registry.py
from ai_pipeline.adapters.linear_regression import LinearRegressionAdapter
from ai_pipeline.adapters.dnn import DNNAdapter
from ai_pipeline.adapters.huggingface import HuggingFaceTextAdapter
from ai_pipeline.adapters.openai_api import OpenAIChatAdapter

MODEL_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "dnn": DNNAdapter,
    "huggingface_text": HuggingFaceTextAdapter,
    "openai_chat": OpenAIChatAdapter,
}
```

---

## ðŸ” é€šç”¨è®­ç»ƒæµç¨‹å°è£…æ¨¡å—ï¼ˆpipeline.pyï¼‰
```python
# ai_pipeline/pipeline.py
import pandas as pd
from ai_pipeline.registry import MODEL_REGISTRY

def default_logger(msg, stream=False):
    print("[LOG]", msg, flush=True)  # é»˜è®¤æ—¥å¿—è¾“å‡ºå‡½æ•°

def train_pipeline(factor_params, model_params, model_type,
                   dataset_files, user_pipeline_control=None,
                   base_pipeline_control=None, logger=default_logger):

    logger("[Pipeline] åŠ è½½æ¨¡åž‹é€‚é…å™¨")
    model_cls = MODEL_REGISTRY[model_type]
    model = model_cls(model_params, logger)

    logger("[Pipeline] è¯»å–å¹¶åˆå¹¶æ•°æ®é›†")
    df = pd.concat([pd.read_csv(f) for f in dataset_files], ignore_index=True)
    df.drop_duplicates(inplace=True)

    logger("[Pipeline] åº”ç”¨å­—æ®µæ˜ å°„å’Œå› å­æ¸…æ´—")
    df = df.dropna(subset=factor_params.get("required_fields", []))

    pipeline_ctl = {"normalize": True, "postprocess": True}  # é»˜è®¤æµç¨‹å¼€å…³
    if base_pipeline_control:
        pipeline_ctl.update(base_pipeline_control)
    if user_pipeline_control:
        pipeline_ctl.update(user_pipeline_control)

    if pipeline_ctl["normalize"]:
        logger("[Pipeline] æ‰§è¡Œå½’ä¸€åŒ–")
        for col in factor_params.get("numeric_fields", []):
            df[col] = (df[col] - df[col].mean()) / df[col].std()

    logger("[Pipeline] åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†")
    train_df = df.sample(frac=0.8, random_state=42)
    test_df = df.drop(train_df.index)

    logger("[Pipeline] å¼€å§‹è®­ç»ƒæ¨¡åž‹")
    X_train, y_train = train_df.drop("target", axis=1), train_df["target"]
    model.train(X_train, y_train)

    logger("[Pipeline] æ¨¡åž‹è®­ç»ƒå®Œæˆï¼Œå¼€å§‹é¢„æµ‹")
    y_pred = model.predict(test_df.drop("target", axis=1))

    if pipeline_ctl["postprocess"]:
        logger("[Pipeline] è¿›è¡Œåå½’ä¸€åŒ–ï¼ˆä¼ªæ“ä½œï¼‰")

    logger("[Pipeline] ä¿å­˜æ¨¡åž‹")
    model.save("saved_models/{}_model.bin".format(model_type))

    return {"predictions": y_pred}
```

### ðŸ” æ¯è¡Œä»£ç è§£é‡Š
```python
model_cls = MODEL_REGISTRY[model_type]
model = model_cls(model_params, logger)
# èŽ·å–æ¨¡åž‹ç±»å¹¶åˆå§‹åŒ–å®žä¾‹

pd.concat([pd.read_csv(f) for f in dataset_files])
# åˆå¹¶å¤šä¸ª CSV æ•°æ®é›†

df.dropna(subset=factor_params.get("required_fields", []))
# åˆ é™¤å› å­ç¼ºå¤±å€¼çš„æ ·æœ¬

if pipeline_ctl["normalize"]:
    for col in factor_params.get("numeric_fields", []):
        df[col] = ...
# å¯¹æ•°å€¼åž‹å­—æ®µè¿›è¡Œå½’ä¸€åŒ–

model.train(X_train, y_train)
# å¯åŠ¨æ¨¡åž‹è®­ç»ƒ

model.predict(...)
# æ‰§è¡Œé¢„æµ‹ï¼ˆå¯è¿”å›žæ•°ç»„æˆ–æ–‡æœ¬ï¼‰

model.save(...)
# ä¿å­˜æ¨¡åž‹åˆ°æŒ‡å®šè·¯å¾„
```

---

## âœ… ç¤ºä¾‹ä½¿ç”¨ï¼š
```python
train_pipeline(
    factor_params={
        "required_fields": ["text"],
        "numeric_fields": [],
    },
    model_params={
        "api_key": "sk-xxx",
        "model": "gpt-4"
    },
    model_type="openai_chat",
    dataset_files=["chat_prompts.csv"],
    user_pipeline_control={"normalize": False, "postprocess": False}
)
```

CSV å†…å®¹ï¼š
```csv
prompt
"è¯·å¸®æˆ‘æ€»ç»“è¿™æ®µè¯"
"è§£é‡Šä»€ä¹ˆæ˜¯ Transformer æ¨¡åž‹"
```

---

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ åŠ å…¥ S3 è‡ªåŠ¨ä¸Šä¼ ã€è®­ç»ƒæ—¥å¿—å®žæ—¶ WebSocket å›žä¼ ã€æˆ–æ¨¡åž‹å‘å¸ƒé€»è¾‘ï¼Ÿ

~~~

~~~python
# å®Œæ•´çš„ AI æ¨¡åž‹è®­ç»ƒæµç¨‹æ¡†æž¶ï¼ˆå«é€‚é…å™¨ + æ³¨å†Œè¡¨ + æ•°æ®å¤„ç† + æ¨¡åž‹å®žä¾‹ï¼‰

æœ¬æž¶æž„è®¾è®¡é€‚ç”¨äºŽï¼š
- å¤šæ¨¡åž‹é€‚é…ï¼ˆçº¿æ€§å›žå½’ã€å¤§è¯­è¨€æ¨¡åž‹ã€åˆ†æžæ¨¡åž‹ç­‰ï¼‰
- æ¨¡åž‹è®­ç»ƒæµç¨‹èŠ‚ç‚¹çš„çµæ´»æŽ§åˆ¶ï¼ˆå½’ä¸€åŒ–ã€åŽå¤„ç†ç­‰ï¼‰
- æ”¯æŒç”¨æˆ·æŽ§åˆ¶ä¸Žæ¨¡åž‹åˆ›å»ºè€…æŽ§åˆ¶çš„ä¼˜å…ˆçº§æœºåˆ¶
- å¤šæ•°æ®é›†åˆå¹¶å¤„ç†ã€å­—æ®µç»Ÿä¸€ã€æ¸…æ´—ã€å½’ä¸€åŒ–ç­‰
- è®­ç»ƒæ—¥å¿—æ”¯æŒå®žæ—¶æŽ¨é€ï¼ˆå¯æŽ¥å…¥ WebSocketï¼‰

---

## é¡¹ç›®ç›®å½•ç»“æž„
```
ml_platform/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ registry.py                # æ¨¡åž‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ base_model.py             # æ¨¡åž‹é€‚é…å™¨åŸºç±»
â”‚   â””â”€â”€ linear_regression.py      # ç¤ºä¾‹æ¨¡åž‹ï¼šçº¿æ€§å›žå½’
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ trainer.py                # ä¸»è®­ç»ƒæµç¨‹æŽ§åˆ¶å™¨
â”œâ”€â”€ preprocess/
â”‚   â”œâ”€â”€ dataset_handler.py        # æ•°æ®é›†è¯»å–ä¸Žåˆå¹¶ï¼ˆæ”¯æŒ openpyxlï¼‰
â”‚   â””â”€â”€ cleaner.py                # æ•°æ®æ¸…æ´—ä¸Žé¢„å¤„ç†é€»è¾‘
â””â”€â”€ run_train.py                 # æ¨¡æ‹Ÿè®­ç»ƒæµç¨‹å…¥å£
```

---

## core/registry.py
```python
# æ¨¡åž‹æ³¨å†Œä¸­å¿ƒï¼Œæ‰€æœ‰æ¨¡åž‹éœ€æ³¨å†Œè¿›æ¥ä»¥ä¾¿è®­ç»ƒå™¨ä½¿ç”¨
MODEL_REGISTRY = {}

def register_model(model_type):
    def wrapper(cls):
        MODEL_REGISTRY[model_type] = cls
        return cls
    return wrapper

def get_model_class(model_type):
    return MODEL_REGISTRY.get(model_type)
```

---

## core/base_model.py
```python
# æ‰€æœ‰æ¨¡åž‹å¿…é¡»ç»§æ‰¿çš„åŸºç±»
from abc import ABC, abstractmethod

class BaseModel(ABC):
    def __init__(self, model_params, factor_params, control_config):
        self.model_params = model_params
        self.factor_params = factor_params
        self.control_config = control_config  # å½’ä¸€åŒ–/åŽå¤„ç†æ‰§è¡Œæ ‡å¿—

    @abstractmethod
    def train(self, train_data):
        pass

    @abstractmethod
    def predict(self, test_data):
        pass

    def should_normalize(self):
        return self.control_config.get("normalize", False)

    def should_postprocess(self):
        return self.control_config.get("postprocess", False)
```

---

## core/linear_regression.py
```python
# ç¤ºä¾‹ï¼šçº¿æ€§å›žå½’æ¨¡åž‹
import numpy as np
from sklearn.linear_model import LinearRegression
from core.base_model import BaseModel
from core.registry import register_model

@register_model("linear_regression")
class LinearRegressionModel(BaseModel):
    def __init__(self, model_params, factor_params, control_config):
        super().__init__(model_params, factor_params, control_config)
        self.model = LinearRegression(**self.model_params)

    def train(self, train_data):
        X, y = train_data.drop(columns=["target"]), train_data["target"]
        self.model.fit(X, y)

    def predict(self, test_data):
        return self.model.predict(test_data)
```

---

## preprocess/dataset_handler.py
```python
# ä½¿ç”¨ openpyxl è¯»å–æ‰€æœ‰æ•°æ®é›†å†…å®¹ï¼Œè¿”å›ž pandas DataFrame
import pandas as pd
from openpyxl import load_workbook
import io

def read_excel_as_dataframe(file):
    if isinstance(file, (str, bytes)):
        wb = load_workbook(filename=file, read_only=True)
    else:
        wb = load_workbook(filename=io.BytesIO(file.read()), read_only=True)
    sheet = wb.active
    data = [[cell.value for cell in row] for row in sheet.iter_rows()]
    headers, rows = data[0], data[1:]
    return pd.DataFrame(rows, columns=headers)

def merge_datasets(files):
    dfs = [read_excel_as_dataframe(f) for f in files]
    # å¯¹é½åˆ—åé¡ºåºåŽåˆå¹¶
    columns = dfs[0].columns
    aligned_dfs = [df[columns] for df in dfs]
    merged = pd.concat(aligned_dfs, ignore_index=True).drop_duplicates()
    return merged
```

---

## preprocess/cleaner.py
```python
# æ•°æ®æ¸…æ´—ï¼šåŒ…æ‹¬å› å­ç¼ºå¤±å‰”é™¤ã€å­—æ®µæ˜ å°„ã€å½’ä¸€åŒ–
import pandas as pd

def clean_data(df, factor_config):
    for factor in factor_config:
        name = factor["name"]
        dtype = factor.get("type")
        if dtype == "number":
            min_val = factor.get("min")
            max_val = factor.get("max")
            enum = factor.get("enum")
            if min_val is not None:
                df = df[df[name] >= min_val]
            if max_val is not None:
                df = df[df[name] <= max_val]
            if enum:
                df = df[df[name].isin(enum)]
        elif dtype == "string":
            enum = factor.get("enum")
            if enum:
                df = df[df[name].isin(enum)]
    df = df.dropna(subset=[f["name"] for f in factor_config])
    return df

def normalize_data(df, factor_config):
    for factor in factor_config:
        if factor.get("normalize") and factor.get("type") == "number":
            col = factor["name"]
            min_val = df[col].min()
            max_val = df[col].max()
            df[col] = (df[col] - min_val) / (max_val - min_val)
    return df

def split_data(df, train_ratio=0.8):
    df = df.sample(frac=1).reset_index(drop=True)
    split = int(len(df) * train_ratio)
    return df[:split], df[split:]
```

---

## pipeline/trainer.py
```python
from core.registry import get_model_class
from preprocess.dataset_handler import merge_datasets
from preprocess.cleaner import clean_data, normalize_data, split_data

class TrainingPipeline:
    def __init__(self, model_type, model_params, factor_params, dataset_files, control_flags):
        self.model_type = model_type
        self.model_params = model_params
        self.factor_params = factor_params
        self.dataset_files = dataset_files
        self.control_flags = control_flags

    def run(self):
        print("[æµç¨‹] åˆå¹¶æ•°æ®é›†...")
        df = merge_datasets(self.dataset_files)

        print("[æµç¨‹] æ¸…æ´—æ•°æ®...")
        df = clean_data(df, self.factor_params)

        if self.control_flags.get("normalize"):
            print("[æµç¨‹] æ•°æ®å½’ä¸€åŒ–...")
            df = normalize_data(df, self.factor_params)

        print("[æµç¨‹] åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†...")
        train_df, test_df = split_data(df)

        print("[æµç¨‹] åŠ è½½æ¨¡åž‹å¹¶è®­ç»ƒ...")
        model_cls = get_model_class(self.model_type)
        if not model_cls:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: {self.model_type}")

        model = model_cls(self.model_params, self.factor_params, self.control_flags)
        model.train(train_df)

        print("[æµç¨‹] è®­ç»ƒå®Œæˆï¼Œæ‰§è¡Œé¢„æµ‹...")
        preds = model.predict(test_df.drop(columns=["target"]))

        if model.should_postprocess():
            print("[æµç¨‹] æ‰§è¡ŒåŽå¤„ç†...")
            # å¯ä»¥æ·»åŠ åå½’ä¸€åŒ–ç­‰é€»è¾‘

        print("[æµç¨‹] ä¿å­˜æ¨¡åž‹ä¸Žç»“æžœ...")
        # TODO: ä¿å­˜æ¨¡åž‹åˆ° S3ã€æœ¬åœ°ç­‰

        return preds
```

---

## run_train.pyï¼ˆè®­ç»ƒå…¥å£æ¨¡æ‹Ÿï¼‰
```python
from pipeline.trainer import TrainingPipeline

# æ¨¡æ‹Ÿå‰ç«¯è¯·æ±‚ä¼ å‚
model_type = "linear_regression"
model_params = {"fit_intercept": True}
factor_params = [
    {"name": "age", "type": "number", "normalize": True, "min": 10, "max": 80},
    {"name": "gender", "type": "string", "enum": ["male", "female"]}
]
dataset_files = ["./datasets/sample1.xlsx", "./datasets/sample2.xlsx"]
control_flags = {"normalize": True, "postprocess": False}

pipeline = TrainingPipeline(
    model_type=model_type,
    model_params=model_params,
    factor_params=factor_params,
    dataset_files=dataset_files,
    control_flags=control_flags
)
preds = pipeline.run()
print("é¢„æµ‹ç»“æžœï¼š", preds)
```

---

## âœ… æ€»ç»“
è¯¥æ¡†æž¶æ”¯æŒï¼š
- å„ç§æ¨¡åž‹æ³¨å†Œä¸Žè®­ç»ƒæµç¨‹è§£è€¦
- ä½¿ç”¨ openpyxl è¿›è¡Œ Excel è¯»å–
- åŸºäºŽå› å­é…ç½®çš„æ•°æ®æ¸…æ´—ã€å½’ä¸€åŒ–ä¸Žæ‹†åˆ†
- æ¨¡åž‹è®­ç»ƒä¸Žé¢„æµ‹æŽ¥å£æ ‡å‡†ç»Ÿä¸€
- æŽ§åˆ¶é€»è¾‘çµæ´»ï¼Œå¯æ‰©å±•å…¶ä»–æ¨¡åž‹

ðŸ“Œ å¦‚éœ€ï¼š
- S3 å‘å¸ƒä¸Žæ¨¡åž‹æŒä¹…åŒ–
- ä½¿ç”¨ Celery åŠ å…¥ä»»åŠ¡é˜Ÿåˆ—
- æ—¥å¿—å®žæ—¶å›žä¼ ï¼ˆWebSocket æˆ– SSEï¼‰

å¯ç»§ç»­æ‰©å±•ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æ·»åŠ ç›¸å…³æ¨¡å—ã€‚

~~~

å½“ç„¶å¯ä»¥ï¼æˆ‘ä»¬ä»¥ä¸€ä¸ª**çº¿æ€§ DNN å›žå½’æ¨¡åž‹**ä¸ºä¾‹ï¼Œä½¿ç”¨ PyTorch ç¼–å†™å®Œæ•´è®­ç»ƒæµç¨‹ï¼Œå¹¶å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®­ç»ƒé›†ï¼ˆtrainï¼‰ã€éªŒè¯é›†ï¼ˆvalï¼‰å’Œæµ‹è¯•é›†ï¼ˆtestï¼‰ã€‚

æˆ‘ä»¬ä¼šæ¶µç›–ä»¥ä¸‹å†…å®¹ï¼š

1. æ•°æ®æ‹†åˆ†ï¼ˆtrain/val/testï¼‰
2. æž„å»ºæ¨¡åž‹
3. æ¨¡åž‹è®­ç»ƒï¼ˆä½¿ç”¨ trainã€valï¼‰
4. æ¨¡åž‹è¯„ä¼°ï¼ˆä½¿ç”¨ testï¼‰
5. æ¯ä¸€æ­¥ä»£ç éƒ½æœ‰è¯¦ç»†æ³¨é‡Š âœ…

------

## âœ… çŽ¯å¢ƒä¾èµ–ï¼ˆè¯·å…ˆå®‰è£…ï¼‰

```bash
pip install torch pandas scikit-learn
```

------

## âœ… å®Œæ•´ä»£ç ï¼ˆä½¿ç”¨ PyTorch å®žçŽ°çº¿æ€§ DNN å›žå½’ï¼‰

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# ======== æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡ =========
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæˆ¿ä»·é¢„æµ‹æ•°æ®é›†
def generate_synthetic_data(n=1000):
    np.random.seed(42)
    X = np.random.rand(n, 3) * 100  # ä¸‰ä¸ªç‰¹å¾ï¼šé¢ç§¯ã€å§å®¤æ•°ã€è·ç¦»
    y = 0.3 * X[:, 0] + 0.5 * X[:, 1] - 0.2 * X[:, 2] + np.random.randn(n) * 2
    return pd.DataFrame(X, columns=["area", "bedrooms", "distance"]), pd.Series(y, name="price")

# ç”Ÿæˆæ•°æ®
X_df, y_series = generate_synthetic_data()

# ======== æ•°æ®é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰ =========
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_df)
X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)

# ======== æ•°æ®é›†æ‹†åˆ† =========
X_trainval, X_test, y_trainval, y_test = train_test_split(X_scaled_df, y_series, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)

# ======== PyTorch æ•°æ®é›†å°è£… =========
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X.values, dtype=torch.float32)
        self.y = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)
    
    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# æž„å»º Dataloader
train_loader = DataLoader(RegressionDataset(X_train, y_train), batch_size=32, shuffle=True)
val_loader = DataLoader(RegressionDataset(X_val, y_val), batch_size=32)
test_loader = DataLoader(RegressionDataset(X_test, y_test), batch_size=32)

# ======== æž„å»º DNN æ¨¡åž‹ =========
class LinearDNNRegressor(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # è¾“å‡ºä¸ºä¸€ä¸ªå›žå½’å€¼
        )

    def forward(self, x):
        return self.net(x)

# å®žä¾‹åŒ–æ¨¡åž‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LinearDNNRegressor(input_dim=X_train.shape[1]).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# ======== æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ï¼ˆä½¿ç”¨ train å’Œ valï¼‰=========
num_epochs = 50
best_val_loss = float("inf")
best_model_state = None

for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * len(X_batch)

    train_loss /= len(train_loader.dataset)

    # éªŒè¯è¿‡ç¨‹
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch)
            val_loss += loss.item() * len(X_batch)
    val_loss /= len(val_loader.dataset)

    # æ‰“å°æ—¥å¿—
    print(f"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

    # ä¿å­˜æœ€ä¼˜æ¨¡åž‹
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict()

# æ¢å¤æœ€ä¼˜æ¨¡åž‹
model.load_state_dict(best_model_state)

# ======== æ¨¡åž‹æµ‹è¯•ï¼ˆä½¿ç”¨ testï¼‰=========
model.eval()
test_loss = 0
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch)
        test_loss += loss.item() * len(X_batch)

test_loss /= len(test_loader.dataset)
print(f"Test Loss (MSE): {test_loss:.4f}")
```

------

## âœ… æ¯ä¸€æ­¥è§£é‡Š

### æ•°æ®éƒ¨åˆ†ï¼š

- `generate_synthetic_data`ï¼šç”Ÿæˆä¸‰åˆ—ç‰¹å¾å’Œä¸€ä¸ªç›®æ ‡å€¼çš„æ¨¡æ‹Ÿæ•°æ®ã€‚
- `train_test_split`ï¼šå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

### æ¨¡åž‹è®­ç»ƒï¼š

- è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ª epochï¼š
  - ä½¿ç”¨è®­ç»ƒé›†ï¼ˆtrain_loaderï¼‰è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚
  - ä½¿ç”¨éªŒè¯é›†ï¼ˆval_loaderï¼‰è¯„ä¼°æ¨¡åž‹æ•ˆæžœã€‚
  - å¦‚æžœéªŒè¯æŸå¤±æ›´ä¼˜ï¼Œå°±ä¿å­˜å½“å‰æ¨¡åž‹å‚æ•°ã€‚

### æ¨¡åž‹è¯„ä¼°ï¼š

- ä½¿ç”¨æµ‹è¯•é›†ï¼ˆtest_loaderï¼‰åœ¨è®­ç»ƒå®ŒæˆåŽä¸€æ¬¡æ€§è¯„ä¼°ï¼ŒæŠ¥å‘Šæœ€ç»ˆæ¨¡åž‹åœ¨â€œæœªçŸ¥æ•°æ®â€ä¸Šçš„è¡¨çŽ°ã€‚

------

## âœ… å°ç»“ï¼ˆè®­ç»ƒ vs éªŒè¯ vs æµ‹è¯•ï¼‰

| é˜¶æ®µ | ç›®æ ‡             | ä½¿ç”¨çš„æ•°æ®     |
| ---- | ---------------- | -------------- |
| è®­ç»ƒ | æ›´æ–°æ¨¡åž‹å‚æ•°     | `train_loader` |
| éªŒè¯ | æå‰åœæ­¢æˆ–è°ƒå‚   | `val_loader`   |
| æµ‹è¯• | æ¨¡æ‹Ÿç”Ÿäº§çŽ¯å¢ƒè¯„ä¼° | `test_loader`  |

------

å¦‚æžœä½ åŽç»­è¦å°†è¿™å¥—è®­ç»ƒç»“æž„å°è£…æˆå¯é…ç½®çš„é€‚é…å™¨æ¨¡å—ï¼ˆå¦‚ä½ å‰é¢æåˆ°çš„è®­ç»ƒæœåŠ¡ï¼‰ï¼Œæˆ‘å¯ä»¥å¸®ä½ å°è£…å¹¶åŠ ä¸Šæ—¥å¿—ã€æ¨¡åž‹å‘å¸ƒå’Œé¢„æµ‹æµç¨‹çš„å¯¹æŽ¥ã€‚

æ˜¯å¦éœ€è¦æˆ‘å°†è¿™æ®µä»£ç å°è£…ä¸ºè®­ç»ƒæœåŠ¡ç±»å¹¶é€‚é…ä½ å‰é¢è®¨è®ºçš„æŽ¥å£ï¼Ÿ

ä¸‹é¢æ˜¯ä¸€ä¸ª**åŠ¨æ€ç”Ÿæˆç¥žç»ç½‘ç»œç»“æž„ï¼ˆDNNï¼‰**çš„å®žçŽ°ç¤ºä¾‹ï¼Œå¹¶æ”¯æŒï¼š

- ðŸ§  åŠ¨æ€è®¾ç½®å±‚ç»“æž„ï¼ˆéšè—å±‚æ•°é‡ã€å¤§å°ï¼‰
- ðŸ”¢ è‡ªåŠ¨è¯†åˆ«**ç±»åˆ«ç‰¹å¾**å¹¶æ·»åŠ  **Embedding å±‚**
- ðŸ“ Embedding çš„ç»´åº¦ï¼ˆåµŒå…¥ç»´åº¦ï¼‰æ ¹æ®ç±»åˆ«æ•°é‡åŠ¨æ€ç¡®å®š
- ðŸ§ª è¾“å…¥å¯ä¸º pandas DataFrame
- âœ… ç»“æž„æ¸…æ™°ã€é€‚åˆé›†æˆåˆ°å‰é¢è®­ç»ƒæµç¨‹ä¸­

------

## âœ… ä½¿ç”¨ç¤ºä¾‹å‰çš„è¯´æ˜Ž

æˆ‘ä»¬è€ƒè™‘å¦‚ä¸‹è¾“å…¥ç‰¹å¾åœºæ™¯ï¼š

- è¿žç»­ç‰¹å¾ï¼ˆæ•°å€¼åž‹ï¼‰ï¼š`area`, `distance`
- ç±»åˆ«ç‰¹å¾ï¼ˆåˆ†ç±»åž‹ï¼‰ï¼š`city`, `material`ï¼ˆå­—ç¬¦ä¸²æˆ– intï¼‰

------

## âœ… åŠ¨æ€ç¥žç»ç½‘ç»œå®Œæ•´ä»£ç ï¼ˆå«æ³¨é‡Šï¼‰

```python
import torch
import torch.nn as nn
import pandas as pd

class DynamicDNN(nn.Module):
    def __init__(self, df: pd.DataFrame, 
                 categorical_cols: list[str], 
                 continuous_cols: list[str],
                 hidden_layers: list[int] = [64, 32],
                 embedding_dim_rule=lambda n: min(50, (n + 1) // 2)):
        """
        :param df: åŽŸå§‹ DataFrameï¼Œç”¨äºŽç»Ÿè®¡ embedding ä¿¡æ¯
        :param categorical_cols: ç±»åˆ«åž‹åˆ—å
        :param continuous_cols: æ•°å€¼åž‹åˆ—å
        :param hidden_layers: éšè—å±‚çš„ç»“æž„ï¼Œä¾‹å¦‚ [64, 32]
        :param embedding_dim_rule: ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥ç±»åˆ«æ•°ï¼Œè¾“å‡º embedding ç»´åº¦
        """
        super().__init__()
        self.categorical_cols = categorical_cols
        self.continuous_cols = continuous_cols

        # ===== æž„å»º Embedding å±‚ =====
        self.embeddings = nn.ModuleDict()
        total_emb_dim = 0
        for col in categorical_cols:
            num_classes = df[col].nunique()
            emb_dim = embedding_dim_rule(num_classes)
            self.embeddings[col] = nn.Embedding(num_classes, emb_dim)
            total_emb_dim += emb_dim

        self.input_dim = total_emb_dim + len(continuous_cols)

        # ===== æž„å»ºéšè—å±‚ï¼ˆå…¨è¿žæŽ¥ï¼‰ =====
        layers = []
        prev_dim = self.input_dim
        for h_dim in hidden_layers:
            layers.append(nn.Linear(prev_dim, h_dim))
            layers.append(nn.ReLU())
            prev_dim = h_dim
        layers.append(nn.Linear(prev_dim, 1))  # å›žå½’ä»»åŠ¡è¾“å‡ºä¸€ç»´
        self.model = nn.Sequential(*layers)

    def forward(self, x_cat: dict, x_cont: torch.Tensor):
        """
        :param x_cat: dictï¼Œæ¯ä¸ª key æ˜¯ç±»åˆ«åˆ—åï¼Œvalue æ˜¯ LongTensor çš„ column vector
        :param x_cont: FloatTensorï¼Œè¿žç»­ç‰¹å¾çš„ batch è¾“å…¥
        """
        emb_list = []
        for col in self.categorical_cols:
            emb = self.embeddings[col](x_cat[col])
            emb_list.append(emb)

        x = torch.cat(emb_list + [x_cont], dim=1)
        return self.model(x)
```

------

## âœ… æ•°æ®å‡†å¤‡ç¤ºä¾‹

```python
import numpy as np
from sklearn.preprocessing import LabelEncoder

# æ¨¡æ‹Ÿæ•°æ®
df = pd.DataFrame({
    "area": np.random.rand(1000) * 100,
    "distance": np.random.rand(1000) * 10,
    "city": np.random.choice(["A", "B", "C", "D"], size=1000),
    "material": np.random.choice(["wood", "brick", "steel"], size=1000),
    "price": np.random.rand(1000) * 500
})

categorical_cols = ["city", "material"]
continuous_cols = ["area", "distance"]

# Label encode categorical columns
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# è½¬æ¢ä¸ºå¼ é‡è¾“å…¥
x_cat_tensor = {col: torch.tensor(df[col].values, dtype=torch.long) for col in categorical_cols}
x_cont_tensor = torch.tensor(df[continuous_cols].values, dtype=torch.float32)
y_tensor = torch.tensor(df["price"].values, dtype=torch.float32).view(-1, 1)

# æž„å»ºæ¨¡åž‹
model = DynamicDNN(df, categorical_cols, continuous_cols)
pred = model(x_cat_tensor, x_cont_tensor)

print("é¢„æµ‹è¾“å‡ºå½¢çŠ¶:", pred.shape)  # [batch_size, 1]
```

------

## âœ… è¾“å‡ºè¯´æ˜Ž

- Embedding å±‚çš„ç»´åº¦æ˜¯åŠ¨æ€ç¡®å®šçš„ï¼Œå¦‚ `city` æœ‰ 4 ä¸ªç±»åˆ«ï¼Œåˆ™é»˜è®¤åµŒå…¥ç»´åº¦ä¸º `(4 + 1) // 2 = 2`
- æ¨¡åž‹ç»“æž„ç”± `hidden_layers` æŽ§åˆ¶ï¼Œå¯è‡ªç”±æ‰©å±•
- æ”¯æŒå¤šåˆ— embeddingï¼ŒåŒæ­¥å¤„ç†è¿žç»­å€¼ä¸Žç±»åˆ«å€¼
- æœ€åŽè¾“å‡º shape ä¸º `[batch_size, 1]`ï¼Œé€‚ç”¨äºŽå›žå½’

------

## âœ… æŽ¥å…¥ä½ çŽ°æœ‰æ¡†æž¶çš„æ–¹æ³•å»ºè®®

1. å°†æ­¤ `DynamicDNN` æ¨¡å—æ·»åŠ ä¸ºæ¨¡åž‹é€‚é…å™¨çš„å®žçŽ°ç±»ï¼ˆå¦‚ä¹‹å‰çš„ `BaseModelAdapter` å­ç±»ï¼‰ã€‚
2. åœ¨æ•°æ®æ¸…æ´—è¿‡ç¨‹ä¸­ï¼Œè‡ªåŠ¨è¯†åˆ«ç±»åˆ«åˆ—å¹¶ä¼ å…¥ `DynamicDNN`ã€‚
3. æ¨¡åž‹è®­ç»ƒå™¨è°ƒç”¨ `model(x_cat_dict, x_cont_tensor)` è¿›è¡Œå‰å‘ä¼ æ’­ã€‚
4. è‡ªåŠ¨æž„å»º `x_cat_dict` å’Œ `x_cont_tensor`ï¼ˆå·²é›†æˆåœ¨å‰é¢çš„ Dataset å°è£…ä¹Ÿå¯ä»¥åšè¿™ä¸ªäº‹ï¼‰ã€‚

------

å¦‚æžœä½ å¸Œæœ›æˆ‘å¸®ä½ **å°è£…æˆé€‚é…å™¨æ¨¡åž‹**ã€**æ·»åŠ è®­ç»ƒå’Œé¢„æµ‹æ¨¡å—**ã€**åŠ å…¥æ—¥å¿—æˆ–ä»»åŠ¡é˜Ÿåˆ—æ”¯æŒ**ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ æž„å»ºä¸‹ä¸€ä¸ªæ¨¡å—ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ

ä¸‹é¢æ˜¯å°† DNN å›žå½’æ¨¡åž‹å°è£…æˆé€‚é…å™¨å½¢å¼ï¼Œå¹¶æ·»åŠ å®Œæ•´çš„è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ä¸Žé¢„æµ‹æ¨¡å—çš„ä»£ç ã€‚ä½ å¯ä»¥å°†è¿™å¥—ä»£ç ä½œä¸ºå¹³å°ä¸­â€œæ¨¡åž‹é€‚é…å™¨â€çš„ä¸€éƒ¨åˆ†é›†æˆè¿›é€šç”¨è®­ç»ƒæµç¨‹ä¸­ã€‚

------

## ðŸ“¦ ç›®å½•ç»“æž„å»ºè®®

```
ml_adapters/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py              # åŸºç±»å®šä¹‰
â”œâ”€â”€ dnn_regression.py    # æœ¬æ¬¡å®žçŽ°çš„ DNN æ¨¡åž‹é€‚é…å™¨
```

------

## ðŸ§© base.py â€“ é€šç”¨æ¨¡åž‹é€‚é…å™¨åŸºç±»

```python
# ml_adapters/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    @abstractmethod
    def train(self, train_data, val_data, **kwargs):
        pass

    @abstractmethod
    def evaluate(self, test_data):
        pass

    @abstractmethod
    def predict(self, input_data):
        pass

    @abstractmethod
    def save(self, path):
        pass

    @abstractmethod
    def load(self, path):
        pass
```

------

## ðŸ§  dnn_regression.py â€“ DNN é€‚é…å™¨å®žçŽ°

```python
# ml_adapters/dnn_regression.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import os

from .base import BaseModelAdapter


class DynamicDNN(nn.Module):
    def __init__(self, df: pd.DataFrame, categorical_cols, continuous_cols, hidden_layers=[64, 32]):
        super().__init__()
        self.categorical_cols = categorical_cols
        self.continuous_cols = continuous_cols

        self.embeddings = nn.ModuleDict()
        total_emb_dim = 0
        for col in categorical_cols:
            num_classes = df[col].nunique()
            emb_dim = min(50, (num_classes + 1) // 2)
            self.embeddings[col] = nn.Embedding(num_classes, emb_dim)
            total_emb_dim += emb_dim

        self.input_dim = total_emb_dim + len(continuous_cols)

        layers = []
        prev = self.input_dim
        for h in hidden_layers:
            layers.append(nn.Linear(prev, h))
            layers.append(nn.ReLU())
            prev = h
        layers.append(nn.Linear(prev, 1))
        self.model = nn.Sequential(*layers)

    def forward(self, x_cat_dict, x_cont):
        emb_list = [self.embeddings[k](x_cat_dict[k]) for k in self.categorical_cols]
        x = torch.cat(emb_list + [x_cont], dim=1)
        return self.model(x)


class DNNRegressionAdapter(BaseModelAdapter):
    def __init__(self, df, categorical_cols, continuous_cols, target_col, hidden_layers=[64, 32], device=None):
        self.df = df
        self.categorical_cols = categorical_cols
        self.continuous_cols = continuous_cols
        self.target_col = target_col
        self.hidden_layers = hidden_layers
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = DynamicDNN(df, categorical_cols, continuous_cols, hidden_layers).to(self.device)

    def _prepare_loader(self, df, batch_size=32, shuffle=True):
        x_cat = {col: torch.tensor(df[col].values, dtype=torch.long).to(self.device) for col in self.categorical_cols}
        x_cont = torch.tensor(df[self.continuous_cols].values, dtype=torch.float32).to(self.device)
        y = torch.tensor(df[self.target_col].values, dtype=torch.float32).view(-1, 1).to(self.device)
        return DataLoader(TensorDataset(x_cont, y, *x_cat.values()), batch_size=batch_size, shuffle=shuffle)

    def _batch_to_input(self, batch):
        x_cont, y, *cat_vals = batch
        x_cat_dict = {col: val for col, val in zip(self.categorical_cols, cat_vals)}
        return x_cat_dict, x_cont, y

    def train(self, train_df, val_df=None, epochs=10, lr=0.001, batch_size=32):
        train_loader = self._prepare_loader(train_df, batch_size)
        val_loader = self._prepare_loader(val_df, batch_size, shuffle=False) if val_df is not None else None

        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.MSELoss()

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            for batch in train_loader:
                x_cat, x_cont, y = self._batch_to_input(batch)
                pred = self.model(x_cat, x_cont)
                loss = criterion(pred, y)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"[Train] Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}")

            if val_loader:
                self.evaluate(val_df, prefix="[Val]")

    def evaluate(self, test_df, prefix="[Test]"):
        test_loader = self._prepare_loader(test_df, shuffle=False)
        self.model.eval()
        total_loss = 0
        criterion = nn.MSELoss()
        with torch.no_grad():
            for batch in test_loader:
                x_cat, x_cont, y = self._batch_to_input(batch)
                pred = self.model(x_cat, x_cont)
                loss = criterion(pred, y)
                total_loss += loss.item()
        print(f"{prefix} MSE: {total_loss/len(test_loader):.4f}")

    def predict(self, input_df):
        self.model.eval()
        loader = self._prepare_loader(input_df, shuffle=False)
        predictions = []
        with torch.no_grad():
            for batch in loader:
                x_cat, x_cont, _ = self._batch_to_input(batch)
                pred = self.model(x_cat, x_cont)
                predictions.append(pred.cpu())
        return torch.cat(predictions).numpy()

    def save(self, path):
        torch.save(self.model.state_dict(), path)

    def load(self, path):
        self.model.load_state_dict(torch.load(path, map_location=self.device))
```

------

## âœ… æµ‹è¯•ç¤ºä¾‹

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from ml_adapters.dnn_regression import DNNRegressionAdapter

# ç”Ÿæˆæµ‹è¯•æ•°æ®
df = pd.DataFrame({
    "city": pd.Categorical(["A", "B", "A", "C", "B", "A"]).codes,
    "material": pd.Categorical(["wood", "steel", "brick", "wood", "steel", "brick"]).codes,
    "area": [50, 60, 70, 80, 90, 100],
    "distance": [3, 4, 5, 6, 7, 8],
    "price": [100, 120, 140, 160, 180, 200]
})

train_df, test_df = train_test_split(df, test_size=0.33)

model = DNNRegressionAdapter(
    df=train_df,
    categorical_cols=["city", "material"],
    continuous_cols=["area", "distance"],
    target_col="price"
)

model.train(train_df, test_df, epochs=10)
model.evaluate(test_df)

preds = model.predict(test_df)
print("é¢„æµ‹ç»“æžœ:", preds)

# æ¨¡åž‹ä¿å­˜ & åŠ è½½
model.save("dnn_model.pth")
model.load("dnn_model.pth")
```

------

## âœ… æ”¯æŒé›†æˆä»»åŠ¡æµç¨‹

ä½ å¯ä»¥æŠŠè¿™ä¸ªç±»æ³¨å†Œåˆ°ä½ çš„é€‚é…å™¨å·¥åŽ‚ï¼Œä¾‹å¦‚ï¼š

```python
ADAPTERS = {
    "dnn_regression": DNNRegressionAdapter,
    # future: "linear_regression": LinearRegressionAdapter,
}
```

------

æ˜¯å¦éœ€è¦æˆ‘ç»§ç»­ï¼š

- å°†å…¶æŽ¥å…¥è®­ç»ƒä¸»æµç¨‹ç®¡ç†å™¨
- æ·»åŠ æ—¥å¿—è®°å½•ä¸Žä»»åŠ¡ ID æ ‡è¯†
- æä¾›æ¨¡åž‹å‘å¸ƒï¼ˆå¦‚ S3 ä¸Šä¼ ï¼‰ä¸Žé¢„æµ‹æœåŠ¡æŽ¥å£

éœ€è¦æˆ‘ç»§ç»­å®Œå–„å“ªäº›éƒ¨åˆ†ï¼Ÿ

æ˜¯çš„ï¼Œ**å¯ä»¥å¹¶ä¸”æŽ¨è**å°†å½’ä¸€åŒ–å’Œåå½’ä¸€åŒ–é€»è¾‘æŠ½è±¡å‡ºæ¥ï¼Œä½œä¸º**å¯æ’æ‹”çš„å½’ä¸€åŒ–ç­–ç•¥æ¨¡å—ï¼ˆStrategy Patternï¼‰**ï¼Œè¿™æ ·å¯ä»¥ï¼š

- âœ… æ”¯æŒå¤šç§å½’ä¸€åŒ–æ–¹æ³•ï¼ˆMinMaxã€Z-scoreã€Log ç­‰ï¼‰ï¼›
- âœ… ä¿è¯å½’ä¸€åŒ–ä¸Žåå½’ä¸€åŒ–æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼›
- âœ… ä¿æŒè®­ç»ƒæ¨¡å—è§£è€¦ï¼Œåªä¾èµ–ç»Ÿä¸€æŽ¥å£ï¼›
- âœ… æ˜“äºŽæµ‹è¯•å’Œæ‰©å±•ã€‚

------

## âœ… ä¸€ã€ç»“æž„è®¾è®¡ï¼ˆå½’ä¸€åŒ–ç­–ç•¥æŽ¥å£ï¼‰

```python
normalization/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py                  # æŠ½è±¡åŸºç±»
â”œâ”€â”€ minmax.py                # MinMaxå½’ä¸€åŒ–
â”œâ”€â”€ zscore.py                # ZScoreæ ‡å‡†åŒ–
â”œâ”€â”€ factory.py               # åˆ›å»ºç­–ç•¥çš„å·¥åŽ‚æ–¹æ³•
```

------

## âœ… äºŒã€ä»£ç å®žçŽ°

### `base.py`ï¼ˆæŠ½è±¡å½’ä¸€åŒ–æŽ¥å£ï¼‰

```python
from abc import ABC, abstractmethod
import pandas as pd

class Normalizer(ABC):
    @abstractmethod
    def fit(self, series: pd.Series):
        """è®¡ç®—å½’ä¸€åŒ–æ‰€éœ€çš„å‚æ•°"""
        pass

    @abstractmethod
    def transform(self, series: pd.Series) -> pd.Series:
        """æ‰§è¡Œå½’ä¸€åŒ–"""
        pass

    @abstractmethod
    def inverse_transform(self, series: pd.Series) -> pd.Series:
        """åå½’ä¸€åŒ–"""
        pass

    @abstractmethod
    def get_params(self) -> dict:
        """è¿”å›žå‚æ•°ï¼ˆç”¨äºŽæŒä¹…åŒ–æˆ–è°ƒè¯•ï¼‰"""
        pass
```

------

### `minmax.py`

```python
from .base import Normalizer
import pandas as pd

class MinMaxNormalizer(Normalizer):
    def __init__(self):
        self.min_val = None
        self.max_val = None

    def fit(self, series: pd.Series):
        self.min_val = float(series.min())
        self.max_val = float(series.max())

    def transform(self, series: pd.Series) -> pd.Series:
        return (series - self.min_val) / (self.max_val - self.min_val + 1e-8)

    def inverse_transform(self, series: pd.Series) -> pd.Series:
        return series * (self.max_val - self.min_val) + self.min_val

    def get_params(self):
        return {"min": self.min_val, "max": self.max_val}
```

------

### `zscore.py`

```python
from .base import Normalizer
import pandas as pd

class ZScoreNormalizer(Normalizer):
    def __init__(self):
        self.mean = None
        self.std = None

    def fit(self, series: pd.Series):
        self.mean = float(series.mean())
        self.std = float(series.std())

    def transform(self, series: pd.Series) -> pd.Series:
        return (series - self.mean) / (self.std + 1e-8)

    def inverse_transform(self, series: pd.Series) -> pd.Series:
        return series * self.std + self.mean

    def get_params(self):
        return {"mean": self.mean, "std": self.std}
```

------

### `factory.py`ï¼ˆç­–ç•¥å·¥åŽ‚ï¼‰

```python
from .minmax import MinMaxNormalizer
from .zscore import ZScoreNormalizer

def get_normalizer(strategy: str):
    """
    èŽ·å–å½’ä¸€åŒ–ç­–ç•¥
    """
    strategy = strategy.lower()
    if strategy == "minmax":
        return MinMaxNormalizer()
    elif strategy == "zscore":
        return ZScoreNormalizer()
    else:
        raise ValueError(f"Unsupported normalization strategy: {strategy}")
```

------

## âœ… ä¸‰ã€åœ¨æ•°æ®é¢„å¤„ç†ä¸­ä½¿ç”¨å½’ä¸€åŒ–ç­–ç•¥

åœ¨ `DataPreprocessor` ä¸­é›†æˆï¼š

```python
from normalization.factory import get_normalizer

class DataPreprocessor:
    def __init__(self, factor_configs: Dict[str, Dict]):
        self.factor_configs = factor_configs
        self.normalizers = {}

    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        for col, cfg in self.factor_configs.items():
            if cfg.get("type") == "numerical" and cfg.get("normalize") and not cfg.get("encoded"):
                strategy = cfg.get("strategy", "minmax")
                normalizer = get_normalizer(strategy)
                normalizer.fit(df[col])
                df[col] = normalizer.transform(df[col])
                self.normalizers[col] = normalizer
        return df

    def inverse_normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        for col, normalizer in self.normalizers.items():
            df[col] = normalizer.inverse_transform(df[col])
        return df
```

------

## âœ… å››ã€å­—æ®µé…ç½®ä¸­çš„æ‰©å±•å‚æ•°ç¤ºä¾‹

```python
factor_configs = {
    "age": {
        "type": "numerical",
        "normalize": True,
        "strategy": "minmax",     # ä¹Ÿå¯ä»¥ä½¿ç”¨ "zscore"
        "encoded": False
    },
    "height": {
        "type": "numerical",
        "normalize": True,
        "strategy": "zscore",
        "encoded": False
    }
}
```

------

## âœ… äº”ã€ä¼˜åŠ¿æ€»ç»“

| ä¼˜ç‚¹         | æè¿°                                             |
| ------------ | ------------------------------------------------ |
| âœ… æ’æ‹”å¼ç­–ç•¥ | å¯ä»¥å¿«é€Ÿæ·»åŠ è‡ªå®šä¹‰å½’ä¸€åŒ–æ–¹å¼                     |
| âœ… æŽ¥å£ç»Ÿä¸€   | æ‰€æœ‰å½’ä¸€åŒ–ç±»éƒ½æœ‰ fit/transform/inverse_transform |
| âœ… æ¨¡å—è§£è€¦   | è®­ç»ƒå™¨/é¢„å¤„ç†å™¨ä¸ä¾èµ–å…·ä½“å®žçŽ°ç»†èŠ‚                |
| âœ… é€‚é…åœºæ™¯å¹¿ | ä¸åŒæ¨¡åž‹å¯è¦æ±‚ä¸åŒå­—æ®µå½’ä¸€åŒ–ç­–ç•¥                 |

------

æ˜¯å¦è¿˜éœ€è¦æˆ‘ï¼š

- å¢žåŠ è‡ªå®šä¹‰å½’ä¸€åŒ–æ–¹å¼ï¼ˆå¦‚å¯¹æ•°å½’ä¸€åŒ–ã€æœ€å¤§ç»å¯¹å€¼å½’ä¸€åŒ–ï¼‰ï¼Ÿ
- å°†å½’ä¸€åŒ–ç­–ç•¥åºåˆ—åŒ–ä¿å­˜ï¼Œæ–¹ä¾¿æ¨¡åž‹éƒ¨ç½²åå½’ä¸€åŒ–æ—¶åŠ è½½ä½¿ç”¨ï¼Ÿ
- åŠ å…¥æ—¥å¿—è®°å½•å’Œè°ƒè¯•æ¨¡å¼ï¼Ÿ

æˆ‘å¯ä»¥æ ¹æ®ä½ çš„ä½¿ç”¨åœºæ™¯ç»§ç»­å°è£…ã€‚

ä¸ºäº†åœ¨æ¨¡åž‹éƒ¨ç½²æ—¶æ”¯æŒ **åå½’ä¸€åŒ–æ“ä½œ**ï¼Œæˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªå­—æ®µä½¿ç”¨çš„å½’ä¸€åŒ–ç­–ç•¥å’Œå…¶å‚æ•°è¿›è¡Œ **åºåˆ—åŒ–ä¿å­˜ï¼ˆä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“ï¼‰**ï¼Œå¹¶åœ¨åŠ è½½æ¨¡åž‹æ—¶ä¸€å¹¶è¿˜åŽŸè¿™äº›å½’ä¸€åŒ–é…ç½®ã€‚

------

## âœ… ä¸€ã€æ ¸å¿ƒç›®æ ‡

- åœ¨è®­ç»ƒé˜¶æ®µï¼š
  - æ¯ä¸ªå­—æ®µçš„å½’ä¸€åŒ–ç­–ç•¥ï¼ˆå¦‚ MinMaxã€ZScoreï¼‰å’Œå¯¹åº”å‚æ•°è¢«ä¿å­˜ï¼›
- åœ¨é¢„æµ‹/éƒ¨ç½²é˜¶æ®µï¼š
  - ä»Žä¿å­˜çš„é…ç½®ä¸­ **åŠ è½½å½’ä¸€åŒ–å™¨å¯¹è±¡**ï¼Œç”¨äºŽ `inverse_transform` åå½’ä¸€åŒ–æ“ä½œã€‚

------

## âœ… äºŒã€æ”¹é€  Normalizer æŽ¥å£ï¼ˆæ”¯æŒä¿å­˜/åŠ è½½ï¼‰

### `base.py` å¢žåŠ  `to_dict()` / `from_dict()` æ–¹æ³•

```python
from abc import ABC, abstractmethod
import pandas as pd

class Normalizer(ABC):
    @abstractmethod
    def fit(self, series: pd.Series):
        pass

    @abstractmethod
    def transform(self, series: pd.Series) -> pd.Series:
        pass

    @abstractmethod
    def inverse_transform(self, series: pd.Series) -> pd.Series:
        pass

    @abstractmethod
    def get_params(self) -> dict:
        pass

    @abstractmethod
    def to_dict(self) -> dict:
        """ç”¨äºŽåºåˆ—åŒ–ä¿å­˜"""
        pass

    @classmethod
    @abstractmethod
    def from_dict(cls, data: dict):
        """ä»Žåºåˆ—åŒ–æ•°æ®ä¸­è¿˜åŽŸå½’ä¸€åŒ–å™¨"""
        pass
```

------

## âœ… ä¸‰ã€å®žçŽ° MinMaxNormalizer çš„ä¿å­˜ä¸ŽåŠ è½½

### `minmax.py`

```python
from .base import Normalizer
import pandas as pd

class MinMaxNormalizer(Normalizer):
    def __init__(self, min_val=None, max_val=None):
        self.min_val = min_val
        self.max_val = max_val

    def fit(self, series: pd.Series):
        self.min_val = float(series.min())
        self.max_val = float(series.max())

    def transform(self, series: pd.Series) -> pd.Series:
        return (series - self.min_val) / (self.max_val - self.min_val + 1e-8)

    def inverse_transform(self, series: pd.Series) -> pd.Series:
        return series * (self.max_val - self.min_val) + self.min_val

    def get_params(self):
        return {"min": self.min_val, "max": self.max_val}

    def to_dict(self):
        return {
            "type": "minmax",
            "min_val": self.min_val,
            "max_val": self.max_val
        }

    @classmethod
    def from_dict(cls, data: dict):
        return cls(min_val=data["min_val"], max_val=data["max_val"])
```

------

## âœ… å››ã€é€šç”¨åŠ è½½å‡½æ•°

### `factory.py` æ‰©å±•ååºåˆ—åŒ–åŠŸèƒ½

```python
from .minmax import MinMaxNormalizer
from .zscore import ZScoreNormalizer

NORMALIZER_CLASSES = {
    "minmax": MinMaxNormalizer,
    "zscore": ZScoreNormalizer
}

def get_normalizer(strategy: str):
    strategy = strategy.lower()
    if strategy in NORMALIZER_CLASSES:
        return NORMALIZER_CLASSES[strategy]()
    else:
        raise ValueError(f"Unsupported normalization strategy: {strategy}")

def load_normalizer(data: dict):
    strategy = data["type"]
    if strategy in NORMALIZER_CLASSES:
        return NORMALIZER_CLASSES[strategy].from_dict(data)
    else:
        raise ValueError(f"Unknown normalizer type in serialized data: {strategy}")
```

------

## âœ… äº”ã€ä¿å­˜å’ŒåŠ è½½ç¤ºä¾‹ï¼ˆJSON æ–‡ä»¶ï¼‰

### âœ… ä¿å­˜å½’ä¸€åŒ–å™¨é…ç½®

```python
import json

def save_normalizers(normalizers: dict, path: str):
    """
    ä¿å­˜å½’ä¸€åŒ–å™¨é…ç½®ä¸º JSON æ–‡ä»¶
    """
    data = {col: normalizer.to_dict() for col, normalizer in normalizers.items()}
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
```

### âœ… åŠ è½½å½’ä¸€åŒ–å™¨é…ç½®

```python
from normalization.factory import load_normalizer

def load_normalizers(path: str) -> dict:
    """
    åŠ è½½å½’ä¸€åŒ–å™¨é…ç½®
    """
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return {col: load_normalizer(cfg) for col, cfg in data.items()}
```

------

## âœ… å…­ã€åœ¨è®­ç»ƒä¸Žé¢„æµ‹é˜¶æ®µä½¿ç”¨

### è®­ç»ƒç»“æŸæ—¶ä¿å­˜é…ç½®ï¼š

```python
preprocessor = DataPreprocessor(factor_configs)
df = preprocessor.normalize(df)
save_normalizers(preprocessor.normalizers, "normalizers.json")
```

### éƒ¨ç½²é¢„æµ‹æ—¶åŠ è½½é…ç½®å¹¶åå½’ä¸€åŒ–ï¼š

```python
normalizers = load_normalizers("normalizers.json")
for col, normalizer in normalizers.items():
    df[col] = normalizer.inverse_transform(df[col])
```

------

## âœ… ä¸ƒã€æ€»ç»“ä¼˜åŠ¿

| åŠŸèƒ½ç‚¹           | è¯´æ˜Ž                                 |
| ---------------- | ------------------------------------ |
| âœ… å½’ä¸€åŒ–ç­–ç•¥å¯é€‰ | æ¯ä¸ªå­—æ®µç‹¬ç«‹é€‰æ‹©åˆé€‚çš„å½’ä¸€åŒ–ç­–ç•¥     |
| âœ… å¯åå½’ä¸€åŒ–     | æ”¯æŒæ¨¡åž‹é¢„æµ‹è¾“å‡ºå›žè½¬åˆ°åŽŸå§‹å•ä½       |
| âœ… å¯æŒä¹…åŒ–       | å¯å­˜ä¸º JSONï¼Œé€‚åˆæ¨¡åž‹éƒ¨ç½²ä¸Žè°ƒè¯•      |
| âœ… æ¨¡åž‹è§£è€¦       | æ¨¡åž‹æœ¬èº«åªå¤„ç†æ•°å€¼ï¼Œä¸å…³å¿ƒå½’ä¸€åŒ–ç­–ç•¥ |

------

å¦‚æžœä½ éœ€è¦ï¼Œæˆ‘ä¹Ÿå¯ä»¥æ‰©å±•æ”¯æŒï¼š

- å°†å½’ä¸€åŒ–ç­–ç•¥ä¿å­˜åœ¨æ•°æ®åº“ä¸­ï¼›
- å°† `normalizers.json` è‡ªåŠ¨ä¸Žæ¨¡åž‹æ–‡ä»¶æ‰“åŒ…ï¼›
- å¤šæ¨¡åž‹å…±äº«çš„å½’ä¸€åŒ–ç¼“å­˜ç›®å½•è®¾è®¡ã€‚

æ˜¯å¦éœ€è¦å°†å®ƒä»¬é›†æˆè¿›ä½ å½“å‰çš„è®­ç»ƒæµç¨‹ç»“æž„ï¼Ÿ

åå½’ä¸€åŒ–çš„ä½¿ç”¨ä½ç½®å–å†³äºŽä½ çš„ **ç›®æ ‡** æ˜¯ä»€ä¹ˆï¼šå¯è§†åŒ–æŒ‡æ ‡ã€è¿˜åŽŸè¾“å‡ºã€æˆ–ç”Ÿæˆç»“æžœæŠ¥å‘Šã€‚ä¸‹é¢æ˜¯è¯¦ç»†çš„è¯´æ˜Žï¼š

------

## âœ… ä¸€ã€åå½’ä¸€åŒ–åº”è¯¥ä½¿ç”¨åœ¨å“ªäº›åœºæ™¯ï¼Ÿ

### ðŸ“Œ 1. **é¢„æµ‹ç»“æžœè¾“å‡ºå‰ï¼ˆéƒ¨ç½²æˆ–ç¦»çº¿æŽ¨ç†ï¼‰**

**åœºæ™¯**ï¼šä½ ä½¿ç”¨æ¨¡åž‹é¢„æµ‹æ•°æ®åŽï¼Œéœ€è¦å°†æ¨¡åž‹è¾“å‡ºè¿˜åŽŸä¸ºç”¨æˆ·å¯ç†è§£çš„åŽŸå§‹å•ä½ï¼ˆå¦‚ä»·æ ¼ã€æ¸©åº¦ã€é”€å”®é‡ç­‰ï¼‰ã€‚

- **ä½ç½®**ï¼šæ¨¡åž‹é¢„æµ‹ç»“æžœä¹‹åŽï¼Œæ¨¡åž‹è¾“å‡º â†’ `inverse_transform`
- **æ˜¯å¦å¿…é¡»**ï¼šâœ…å¿…é¡»ï¼Œå¦‚æžœæ¨¡åž‹è¾“å‡ºçš„æ˜¯å½’ä¸€åŒ–åŽçš„å€¼

```python
# æ¨¡åž‹é¢„æµ‹
y_pred_norm = model.predict(X_test)

# åå½’ä¸€åŒ–
y_pred = normalizer.inverse_transform(y_pred_norm)
```

------

### ðŸ“Œ 2. **è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‰ï¼ˆMSEã€MAEã€RÂ² ç­‰ï¼‰**

**åœºæ™¯**ï¼šä½ å¸Œæœ›è¯„ä¼°æŒ‡æ ‡çš„å•ä½æ˜¯â€œçœŸå®žä¸–ç•Œå•ä½â€ï¼Œå¦åˆ™å½’ä¸€åŒ–æ•°æ®ä¸‹çš„ MAEã€MSE æ²¡æœ‰å®žé™…æ„ä¹‰ã€‚

- **ä½ç½®**ï¼šæ¨¡åž‹é¢„æµ‹ + æ ‡ç­¾éƒ½éœ€åå½’ä¸€åŒ–åŽå†è®¡ç®—æŒ‡æ ‡
- **æ˜¯å¦å¿…é¡»**ï¼šâœ…æŽ¨èï¼ˆé™¤éžä½ å°±æƒ³çœ‹å½’ä¸€åŒ–å•ä½ä¸‹çš„è¯¯å·®ï¼‰

```python
# åå½’ä¸€åŒ–
y_pred = normalizer.inverse_transform(y_pred_norm)
y_true = normalizer.inverse_transform(y_test)

# å†è¿›è¡Œè¯¯å·®è¯„ä¼°
mae = mean_absolute_error(y_true, y_pred)
```

------

### ðŸ“Œ 3. **è®­ç»ƒé›†è¾“å‡ºå¯è§†åŒ–ï¼ˆéžå¿…é¡»ï¼‰**

**åœºæ™¯**ï¼šå¦‚æžœä½ åœ¨è®­ç»ƒåŽæƒ³ç”»å›¾è§‚å¯Ÿæ¨¡åž‹å¯¹è®­ç»ƒé›†æ‹Ÿåˆå¾—å¦‚ä½•ï¼Œé€šå¸¸ä½ ä¹Ÿä¼šæƒ³çœ‹åŽŸå§‹å•ä½çš„è¾“å‡ºã€‚

- **ä½ç½®**ï¼šä»…ç”¨äºŽå¯è§†åŒ–ã€æŠ¥å‘Šç”Ÿæˆç­‰çŽ¯èŠ‚
- **æ˜¯å¦å¿…é¡»**ï¼šâŒå¯é€‰

------

## âœ… äºŒã€åå½’ä¸€åŒ–åº”è¯¥ **ä¸ä½¿ç”¨** çš„åœ°æ–¹

### ðŸš« 1. æ¨¡åž‹å†…éƒ¨ï¼ˆä¾‹å¦‚ loss è®¡ç®—ï¼‰

- åœ¨æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰çš„ loss è®¡ç®—ã€åå‘ä¼ æ’­éƒ½åº”åœ¨ **å½’ä¸€åŒ–åŽçš„æ•°æ®** ä¸Šè¿›è¡Œã€‚
- ä¸åº”åœ¨æ¨¡åž‹å†…éƒ¨å¯¹è¾“å‡º `inverse_transform` åŽå†è®¡ç®— lossã€‚

```python
# é”™è¯¯ âŒ
loss = MSE(y_true, normalizer.inverse_transform(model(X)))

# æ­£ç¡® âœ…
loss = MSE(y_true_normalized, model(X))
```

------

## âœ… ä¸‰ã€ä½ çš„æƒ…å†µè¯´æ˜Ž

> æˆ‘çŽ°åœ¨æ˜¯åœ¨æ¨¡åž‹è®­ç»ƒåŽçš„å¤–éƒ¨å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œçš„åå½’ä¸€åŒ–

### âœ… æ­£ç¡®åšæ³•åº”æ˜¯ï¼š

1. **è®­ç»ƒå‰å½’ä¸€åŒ–è¾“å…¥å’Œè¾“å‡º**
2. **è®­ç»ƒæ—¶ä½¿ç”¨å½’ä¸€åŒ–åŽçš„æ•°æ®**
3. **è®­ç»ƒåŽä¸å¯¹è®­ç»ƒæ•°æ®åå½’ä¸€åŒ–**
   - é™¤éžä½ è¦ç”»å›¾çœ‹æ¨¡åž‹æ•ˆæžœï¼Œæˆ–è¾“å‡ºé¢„æµ‹ç»“æžœ
4. **é¢„æµ‹æ—¶æ¨¡åž‹è¾“å‡ºè¿›è¡Œåå½’ä¸€åŒ–**

------

## âœ… å››ã€æ€»ç»“

| é˜¶æ®µ       | æ˜¯å¦å½’ä¸€åŒ–          | æ˜¯å¦åå½’ä¸€åŒ–   | åŽŸå›                    |
| ---------- | ------------------- | -------------- | ---------------------- |
| è®­ç»ƒå‰æ•°æ® | âœ… éœ€è¦              | âŒ ä¸éœ€è¦       | æå‡æ¨¡åž‹è®­ç»ƒç¨³å®šæ€§     |
| æ¨¡åž‹å†…éƒ¨   | âœ… ä½¿ç”¨å½’ä¸€åŒ–        | âŒ ä¸åšåå½’ä¸€åŒ– | loss è®¡ç®—åŸºäºŽå½’ä¸€åŒ–    |
| æ¨¡åž‹è¾“å‡º   | âœ… è¾“å‡ºå½’ä¸€åŒ–        | âœ… å†åå½’ä¸€åŒ–   | æä¾›ç”¨æˆ·çœŸå®žå•ä½çš„é¢„æµ‹ |
| è¯„ä¼°æŒ‡æ ‡   | âœ… é¢„æµ‹+æ ‡ç­¾åå½’ä¸€åŒ– | âœ…              | ä¿è¯æŒ‡æ ‡æœ‰å®žé™…æ„ä¹‰     |
| å¯è§†åŒ–     | âœ… åå½’ä¸€åŒ–          | âœ…              | å‘ˆçŽ°åŽŸå§‹å€¼æ›²çº¿         |

------

æ˜¯å¦éœ€è¦æˆ‘æ•´åˆåˆ°ä½ ç›®å‰çš„æ¨¡åž‹è®­ç»ƒå’Œé¢„æµ‹æ¡†æž¶ä¸­ï¼Ÿæˆ‘å¯ä»¥ç›´æŽ¥å¸®ä½ å†™ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ + é¢„æµ‹ + åå½’ä¸€åŒ–è¯„ä¼°çš„ç¤ºä¾‹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªæ•´åˆäº†**è®­ç»ƒã€é¢„æµ‹ã€åå½’ä¸€åŒ–ã€è¯„ä¼°**çš„å®Œæ•´æµç¨‹ä»£ç ç¤ºä¾‹ï¼Œä½¿ç”¨çº¿æ€§å›žå½’ï¼ˆæˆ– DNNï¼‰æ¨¡åž‹ä¸ºä¾‹ï¼Œå¹¶ä¸”é€‚é…ä½ çš„æ¡†æž¶ç»“æž„ï¼š

------

## âœ… ä¸€ã€ç›®å½•ç»“æž„æ¦‚è§ˆï¼ˆæŽ¨èï¼‰

```
ml_platform/
â”œâ”€â”€ normalizers/
â”‚   â””â”€â”€ base.py         # å½’ä¸€åŒ–ç­–ç•¥åŸºç¡€ç±»
â”‚   â””â”€â”€ minmax.py       # MinMaxå½’ä¸€åŒ–
â”œâ”€â”€ models/
â”‚   â””â”€â”€ base_adapter.py # æ¨¡åž‹é€‚é…å™¨æŽ¥å£
â”‚   â””â”€â”€ linear.py       # çº¿æ€§æ¨¡åž‹é€‚é…å™¨
â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ trainer.py      # æ ¸å¿ƒè®­ç»ƒä¸Žè¯„ä¼°é€»è¾‘
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ io.py           # æ¨¡åž‹ä¿å­˜ä¸Žå½’ä¸€åŒ–å‚æ•°ä¿å­˜
â””â”€â”€ main.py             # ç¤ºä¾‹å…¥å£ç‚¹
```

------

## âœ… äºŒã€å½’ä¸€åŒ–ç»„ä»¶ï¼ˆminmax.pyï¼‰

```python
# ml_platform/normalizers/minmax.py
import numpy as np
import json

class MinMaxNormalizer:
    def __init__(self):
        self.min = None
        self.max = None

    def fit(self, data: np.ndarray):
        self.min = data.min(axis=0)
        self.max = data.max(axis=0)

    def transform(self, data: np.ndarray) -> np.ndarray:
        return (data - self.min) / (self.max - self.min + 1e-8)

    def inverse_transform(self, data: np.ndarray) -> np.ndarray:
        return data * (self.max - self.min + 1e-8) + self.min

    def save(self, filepath):
        with open(filepath, 'w') as f:
            json.dump({'min': self.min.tolist(), 'max': self.max.tolist()}, f)

    def load(self, filepath):
        with open(filepath, 'r') as f:
            data = json.load(f)
            self.min = np.array(data['min'])
            self.max = np.array(data['max'])
```

------

## âœ… ä¸‰ã€æ¨¡åž‹é€‚é…å™¨æŽ¥å£ä¸Žçº¿æ€§å®žçŽ°ï¼ˆbase_adapter.py, linear.pyï¼‰

```python
# ml_platform/models/base_adapter.py
class BaseModelAdapter:
    def train(self, X_train, y_train, X_val, y_val): ...
    def predict(self, X): ...
    def save(self, path): ...
    def load(self, path): ...

# ml_platform/models/linear.py
from sklearn.linear_model import LinearRegression
from .base_adapter import BaseModelAdapter
import joblib

class LinearModelAdapter(BaseModelAdapter):
    def __init__(self):
        self.model = LinearRegression()

    def train(self, X_train, y_train, X_val=None, y_val=None):
        self.model.fit(X_train, y_train)

    def predict(self, X):
        return self.model.predict(X)

    def save(self, path):
        joblib.dump(self.model, path)

    def load(self, path):
        self.model = joblib.load(path)
```

------

## âœ… å››ã€è®­ç»ƒå™¨å®žçŽ°ï¼ˆtrainer.pyï¼‰

```python
# ml_platform/trainer/trainer.py
import numpy as np
from ml_platform.normalizers.minmax import MinMaxNormalizer

class ModelTrainer:
    def __init__(self, model_adapter, normalization_enabled=True):
        self.model = model_adapter
        self.normalizer = MinMaxNormalizer() if normalization_enabled else None

    def train(self, X_train, y_train, X_val=None, y_val=None):
        if self.normalizer:
            self.normalizer.fit(y_train)
            y_train = self.normalizer.transform(y_train)
            if y_val is not None:
                y_val = self.normalizer.transform(y_val)

        self.model.train(X_train, y_train, X_val, y_val)

    def predict(self, X):
        y_pred = self.model.predict(X)
        if self.normalizer:
            y_pred = self.normalizer.inverse_transform(y_pred)
        return y_pred

    def evaluate(self, X_test, y_test):
        y_pred = self.model.predict(X_test)
        if self.normalizer:
            y_pred = self.normalizer.inverse_transform(y_pred)
            y_test = self.normalizer.inverse_transform(y_test)

        mae = np.mean(np.abs(y_test - y_pred))
        mse = np.mean((y_test - y_pred) ** 2)
        return {"MAE": mae, "MSE": mse}
```

------

## âœ… äº”ã€æµ‹è¯•å…¥å£ï¼ˆmain.pyï¼‰

```python
# ml_platform/main.py
import numpy as np
from ml_platform.models.linear import LinearModelAdapter
from ml_platform.trainer.trainer import ModelTrainer

# 1. æ¨¡æ‹Ÿæ•°æ®
X = np.random.rand(100, 5)
y = X @ np.array([1.5, -2, 0.5, 3.0, 0.2]) + 0.1 * np.random.randn(100)

# 2. åˆ’åˆ†æ•°æ®é›†
train_size = int(len(X) * 0.7)
val_size = int(len(X) * 0.1)
X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]

# 3. æ¨¡åž‹åˆå§‹åŒ–ä¸Žè®­ç»ƒ
model_adapter = LinearModelAdapter()
trainer = ModelTrainer(model_adapter)
trainer.train(X_train, y_train.reshape(-1, 1), X_val, y_val.reshape(-1, 1))

# 4. æ¨¡åž‹é¢„æµ‹ä¸Žè¯„ä¼°
pred = trainer.predict(X_test)
metrics = trainer.evaluate(X_test, y_test.reshape(-1, 1))

print("é¢„æµ‹å€¼:", pred[:5].flatten())
print("è¯„ä¼°ç»“æžœ:", metrics)
```

------

## âœ… å…­ã€é‡ç‚¹è¯´æ˜Ž

| æ¨¡å—                 | è¯´æ˜Ž                                                         |
| -------------------- | ------------------------------------------------------------ |
| `MinMaxNormalizer`   | å®žçŽ°äº† `fit`, `transform`, `inverse_transform`, å¯åºåˆ—åŒ–     |
| `LinearModelAdapter` | é€‚é… `sklearn` çš„çº¿æ€§å›žå½’æ¨¡åž‹ï¼Œå°è£… `train`, `predict`, `save/load` |
| `ModelTrainer`       | è´Ÿè´£æ•´ä½“è®­ç»ƒæµç¨‹ï¼Œé€‚é…å½’ä¸€åŒ–ã€åå½’ä¸€åŒ–é€»è¾‘ä¸Žè¯„ä¼°             |
| `main.py`            | æ¨¡æ‹Ÿæµç¨‹å¹¶å±•ç¤ºæ•´ä¸ªè®­ç»ƒã€é¢„æµ‹ã€è¯„ä¼°è¿‡ç¨‹                       |

------

æ˜¯å¦è¿˜å¸Œæœ›æˆ‘æ·»åŠ ï¼š

- å¯¹å½’ä¸€åŒ–å‚æ•°çš„ä¿å­˜/åŠ è½½é€»è¾‘ï¼ˆéƒ¨ç½²ä½¿ç”¨ï¼‰ï¼Ÿ
- æ”¯æŒæµå¼è®­ç»ƒæ—¥å¿—å›žä¼ ç»™å‰ç«¯ï¼Ÿ
- æŽ¥å…¥ä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¦‚ Celeryï¼‰è°ƒç”¨æ•´ä¸ªæµç¨‹ï¼Ÿ

æˆ‘ä»¬å°†å½’ä¸€åŒ–ä¸Žåå½’ä¸€åŒ–æ¨¡å—ä½œä¸ºæµç¨‹èŠ‚ç‚¹ `NormalizationNode`ï¼Œæ•´åˆè¿›çŽ°æœ‰çš„è®­ç»ƒé€‚é…å™¨ç»“æž„ä¸­ï¼Œå¹¶æ”¯æŒæ—¥å¿—å›žä¼ ã€ç­–ç•¥ä¿å­˜ã€åŠ è½½ä¸Žåˆ‡æ¢å½’ä¸€åŒ–æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„æ¨¡å—é›†æˆæ–¹æ¡ˆã€‚

------

## âœ… æ¨¡å—ç»“æž„è¯´æ˜Ž

```
ml_pipeline/
â”‚
â”œâ”€â”€ base_node.py               # æµç¨‹èŠ‚ç‚¹åŸºç±»
â”œâ”€â”€ normalization_strategies.py  # å¤šç§å½’ä¸€åŒ–ç­–ç•¥å®šä¹‰
â”œâ”€â”€ normalization_node.py     # å°è£…çš„å½’ä¸€åŒ–æµç¨‹èŠ‚ç‚¹
â”œâ”€â”€ training_flow.py          # è®­ç»ƒæµç¨‹å®šä¹‰ä¸Žä»»åŠ¡è¿è¡Œç®¡ç†
â”œâ”€â”€ adapter_base.py           # æ¨¡åž‹è®­ç»ƒé€‚é…å™¨åŸºç±»
â”œâ”€â”€ dnn_regression_adapter.py # ç¤ºä¾‹ï¼šDNNå›žå½’æ¨¡åž‹é€‚é…å™¨
â””â”€â”€ utils/
    â””â”€â”€ logger.py             # æ—¥å¿—å›žä¼ æ¨¡å—
```

------

## 1ï¸âƒ£ `base_node.py`

```python
# ml_pipeline/base_node.py
from abc import ABC, abstractmethod

class BaseNode(ABC):
    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def run(self, context: dict) -> dict:
        """æ‰§è¡Œå½“å‰èŠ‚ç‚¹çš„é€»è¾‘ï¼ŒæŽ¥æ”¶å¹¶è¿”å›ž context"""
        pass
```

------

## 2ï¸âƒ£ `normalization_strategies.py`

```python
# ml_pipeline/normalization_strategies.py
import pandas as pd
import joblib
from abc import ABC, abstractmethod

class NormalizationStrategy(ABC):
    @abstractmethod
    def fit(self, data: pd.DataFrame): pass

    @abstractmethod
    def transform(self, data: pd.DataFrame) -> pd.DataFrame: pass

    @abstractmethod
    def inverse_transform(self, data: pd.DataFrame) -> pd.DataFrame: pass

    def save(self, path: str):
        joblib.dump(self, path)

    def load(self, path: str):
        obj = joblib.load(path)
        self.__dict__.update(obj.__dict__)

class MinMaxNormalization(NormalizationStrategy):
    def __init__(self):
        self.min = None
        self.max = None

    def fit(self, data: pd.DataFrame):
        self.min = data.min()
        self.max = data.max()

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        return (data - self.min) / (self.max - self.min + 1e-8)

    def inverse_transform(self, data: pd.DataFrame) -> pd.DataFrame:
        return data * (self.max - self.min + 1e-8) + self.min

class StandardNormalization(NormalizationStrategy):
    def __init__(self):
        self.mean = None
        self.std = None

    def fit(self, data: pd.DataFrame):
        self.mean = data.mean()
        self.std = data.std()

    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        return (data - self.mean) / (self.std + 1e-8)

    def inverse_transform(self, data: pd.DataFrame) -> pd.DataFrame:
        return data * (self.std + 1e-8) + self.mean

def get_normalization_strategy(name: str) -> NormalizationStrategy:
    if name == "minmax":
        return MinMaxNormalization()
    elif name == "standard":
        return StandardNormalization()
    else:
        raise ValueError(f"Unknown normalization strategy: {name}")
```

------

## 3ï¸âƒ£ `normalization_node.py`

```python
# ml_pipeline/normalization_node.py
from .base_node import BaseNode
from .normalization_strategies import get_normalization_strategy
from typing import Optional
import pandas as pd
import os

class NormalizationNode(BaseNode):
    def __init__(self, strategy_name: str, normalize_columns: Optional[list] = None, save_path: Optional[str] = None):
        super().__init__("NormalizationNode")
        self.strategy_name = strategy_name
        self.normalize_columns = normalize_columns
        self.save_path = save_path or f"./normalizers/{strategy_name}_normalizer.pkl"
        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)

    def run(self, context: dict) -> dict:
        df = context["train_data"]
        strategy = get_normalization_strategy(self.strategy_name)
        columns = self.normalize_columns or df.select_dtypes(include=['float', 'int']).columns.tolist()
        strategy.fit(df[columns])
        df[columns] = strategy.transform(df[columns])
        context["train_data"] = df
        strategy.save(self.save_path)
        context["normalization_strategy"] = strategy
        context["normalize_columns"] = columns
        return context
```

------

## 4ï¸âƒ£ è®­ç»ƒæµç¨‹æ•´åˆï¼ˆtraining_flow.pyï¼‰

```python
# ml_pipeline/training_flow.py
from .normalization_node import NormalizationNode
from .base_node import BaseNode
from typing import List
import logging

logger = logging.getLogger(__name__)

class TrainingFlow:
    def __init__(self, nodes: List[BaseNode]):
        self.nodes = nodes

    def run(self, context: dict):
        for node in self.nodes:
            logger.info(f"Running node: {node.name}")
            context = node.run(context)
        return context
```

------

## âœ… ç¤ºä¾‹ä½¿ç”¨ï¼ˆrun_training.pyï¼‰

```python
from ml_pipeline.training_flow import TrainingFlow
from ml_pipeline.normalization_node import NormalizationNode
from mynodes.data_cleaning_node import DataCleaningNode
from mynodes.train_model_node import TrainModelNode
import pandas as pd

# åŠ è½½è®­ç»ƒæ•°æ®
df = pd.read_csv("data.csv")

context = {
    "train_data": df,
    "model_type": "dnn_regression"
}

flow = TrainingFlow(nodes=[
    DataCleaningNode(),
    NormalizationNode(strategy_name="minmax"),
    TrainModelNode()
])

context = flow.run(context)
```

------

## ðŸ” åœ¨æ¨¡åž‹æŽ¨ç†æ—¶åå½’ä¸€åŒ–ï¼ˆä¾‹å¦‚é¢„æµ‹ï¼‰

```python
# predict.py
from ml_pipeline.normalization_strategies import get_normalization_strategy

strategy = get_normalization_strategy("minmax")
strategy.load("./normalizers/minmax_normalizer.pkl")

# å‡è®¾ result æ˜¯é¢„æµ‹è¾“å‡º
inverse_result = strategy.inverse_transform(result)
```

------

### ðŸ” æ€»ç»“

| æ­¥éª¤   | åŠŸèƒ½                       | æ¨¡å—                  |
| ------ | -------------------------- | --------------------- |
| åˆå§‹åŒ– | å½’ä¸€åŒ–èŠ‚ç‚¹æ·»åŠ åˆ°æµç¨‹       | `NormalizationNode`   |
| æ‰§è¡Œ   | å½’ä¸€åŒ–è®­ç»ƒæ•°æ®             | `run()` æ–¹æ³•          |
| ä¿å­˜   | å½’ä¸€åŒ–ç­–ç•¥æŒä¹…åŒ–           | `joblib.dump()`       |
| é¢„æµ‹   | ä½¿ç”¨ç›¸åŒç­–ç•¥åå½’ä¸€åŒ–é¢„æµ‹å€¼ | `inverse_transform()` |

------

éœ€è¦æˆ‘è¿›ä¸€æ­¥ä¸ºä½ æŽ¥å…¥æ¨¡åž‹é€‚é…å™¨ç»“æž„ï¼ˆæ¯”å¦‚ `DNNRegressionAdapter`ï¼‰å¹¶ç»“åˆæµç¨‹èŠ‚ç‚¹è‡ªåŠ¨è¿è¡Œå—ï¼Ÿæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ ç”Ÿæˆå¯¹åº”çš„è®­ç»ƒæ—¥å¿—æµå¼è¿”å›žé€»è¾‘ã€‚

ä¸‹é¢æ˜¯å®Œæ•´æœºåˆ¶çš„å®žçŽ°ï¼Œç¡®ä¿è®­ç»ƒå’Œé¢„æµ‹é˜¶æ®µä½¿ç”¨ä¸€è‡´çš„å½’ä¸€åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š

1. âœ… æµç¨‹èŠ‚ç‚¹å°è£…ï¼ˆå½’ä¸€åŒ–ã€åå½’ä¸€åŒ–ï¼‰
2. âœ… ç­–ç•¥ä¿å­˜ä¸ŽåŠ è½½
3. âœ… æ¨¡åž‹å…ƒä¿¡æ¯è®°å½•å½’ä¸€åŒ–ç­–ç•¥è·¯å¾„
4. âœ… ç¤ºä¾‹ä»£ç é›†æˆä¸Žæ³¨é‡Š

------

## âœ… 1. å½’ä¸€åŒ–ç­–ç•¥åŸºç±» + å¤šç­–ç•¥å®žçŽ°

```python
# normalization/strategies/base.py
import abc
import pandas as pd
import pickle

class BaseNormalizer(abc.ABC):
    def __init__(self, fields: list[str]):
        self.fields = fields

    @abc.abstractmethod
    def fit(self, df: pd.DataFrame): ...
    
    @abc.abstractmethod
    def transform(self, df: pd.DataFrame) -> pd.DataFrame: ...
    
    @abc.abstractmethod
    def inverse_transform(self, df: pd.DataFrame) -> pd.DataFrame: ...
    
    def save(self, path: str):
        with open(path, "wb") as f:
            pickle.dump(self, f)
    
    @classmethod
    def load(cls, path: str) -> "BaseNormalizer":
        with open(path, "rb") as f:
            return pickle.load(f)
# normalization/strategies/minmax.py
import pandas as pd
from .base import BaseNormalizer

class MinMaxNormalizer(BaseNormalizer):
    def __init__(self, fields: list[str]):
        super().__init__(fields)
        self.min = {}
        self.max = {}

    def fit(self, df: pd.DataFrame):
        for field in self.fields:
            self.min[field] = df[field].min()
            self.max[field] = df[field].max()

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        for field in self.fields:
            df[field] = (df[field] - self.min[field]) / (self.max[field] - self.min[field])
        return df

    def inverse_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        for field in self.fields:
            df[field] = df[field] * (self.max[field] - self.min[field]) + self.min[field]
        return df
```

------

## âœ… 2. å½’ä¸€åŒ–ç­–ç•¥å·¥åŽ‚

```python
# normalization/factory.py
from .strategies.minmax import MinMaxNormalizer

def get_normalizer(name: str, fields: list[str]):
    if name == "minmax":
        return MinMaxNormalizer(fields)
    raise ValueError(f"Unsupported normalization strategy: {name}")
```

------

## âœ… 3. å°è£…æµç¨‹èŠ‚ç‚¹ï¼ˆNodeï¼‰

```python
# pipeline/nodes/normalize.py
from normalization.factory import get_normalizer

class NormalizationNode:
    def __init__(self, strategy_name: str, fields: list[str], save_path: str):
        self.strategy_name = strategy_name
        self.fields = fields
        self.save_path = save_path

    def run(self, df: pd.DataFrame) -> pd.DataFrame:
        strategy = get_normalizer(self.strategy_name, self.fields)
        strategy.fit(df)
        df_transformed = strategy.transform(df)
        strategy.save(self.save_path)
        return df_transformed
# pipeline/nodes/inverse_normalize.py
from normalization.factory import get_normalizer

class InverseNormalizationNode:
    def __init__(self, strategy_name: str, fields: list[str], load_path: str):
        self.strategy_name = strategy_name
        self.fields = fields
        self.load_path = load_path

    def run(self, df: pd.DataFrame) -> pd.DataFrame:
        strategy = get_normalizer(self.strategy_name, self.fields)
        strategy = strategy.load(self.load_path)
        return strategy.inverse_transform(df)
```

------

## âœ… 4. æ¨¡åž‹å…ƒä¿¡æ¯ä¿å­˜æ ¼å¼ï¼ˆç¤ºä¾‹ï¼‰

```json
{
  "model_id": "model_xyz",
  "normalization": {
    "strategy": "minmax",
    "fields": ["age", "income"],
    "path": "./models/model_xyz/normalizer_minmax.pkl"
  }
}
```

å¯ä»¥ä¿å­˜åœ¨æ•°æ®åº“å­—æ®µã€JSONé…ç½®æˆ–æ¨¡åž‹ç›®å½•ä¸­ã€‚

------

## âœ… 5. åœ¨è®­ç»ƒæµç¨‹ä¸­ä½¿ç”¨

```python
df_raw = pd.read_csv(dataset_path)
fields_to_normalize = ["age", "income"]

# æ­¥éª¤1ï¼šå½’ä¸€åŒ–
norm_node = NormalizationNode("minmax", fields_to_normalize, save_path="./models/model_xyz/normalizer_minmax.pkl")
df_normalized = norm_node.run(df_raw)

# æ­¥éª¤2ï¼šè®­ç»ƒæ¨¡åž‹...
```

------

## âœ… 6. åœ¨é¢„æµ‹æµç¨‹ä¸­ä½¿ç”¨

```python
df_input = pd.read_csv(input_path)
fields_to_normalize = ["age", "income"]

# æ­¥éª¤1ï¼šå½’ä¸€åŒ–è¾“å…¥
strategy = get_normalizer("minmax", fields_to_normalize).load("./models/model_xyz/normalizer_minmax.pkl")
df_input_normalized = strategy.transform(df_input)

# æ­¥éª¤2ï¼šé¢„æµ‹
predictions = model.predict(df_input_normalized)

# æ­¥éª¤3ï¼šåå½’ä¸€åŒ–
inverse_node = InverseNormalizationNode("minmax", ["target"], "./models/model_xyz/normalizer_minmax.pkl")
df_output = inverse_node.run(predictions)
```

------

## âœ… ä¸‹ä¸€æ­¥å»ºè®®

ä½ å¯ä»¥ï¼š

- âœ… å°†è¿™äº›èŠ‚ç‚¹æ•´åˆå…¥ä½ å·²æœ‰çš„ `TrainingPipeline` ä¸­
- âœ… ä¸ºé¢„æµ‹æµç¨‹å¢žåŠ ç›¸åº”çš„æ ‡å‡† Node
- âœ… æ·»åŠ æ•°æ®åº“è®°å½•æ”¯æŒè®°å½• `normalizer_path`

------

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ ï¼š

- æ·»åŠ å…¶ä»–å½’ä¸€åŒ–ç­–ç•¥ï¼ˆå¦‚ Z-Scoreã€Robustï¼‰ï¼Ÿ
- å°†è¿™äº›èŠ‚ç‚¹è‡ªåŠ¨åŠ å…¥ä½ çš„æµç¨‹æ¡†æž¶ä¸­å¹¶æ”¯æŒ YAML/JSON é…ç½®æ–¹å¼åŠ è½½ï¼Ÿ
- ä¸ºè®­ç»ƒ/é¢„æµ‹æµç¨‹å°è£…å®Œæ•´çš„ CLI æˆ– Django API æŽ¥å£ï¼Ÿ

å‘Šè¯‰æˆ‘ä½ çš„ä¸‹ä¸€æ­¥ç›®æ ‡ï¼Œæˆ‘å¸®ä½ å®Œå–„ã€‚

å½“ç„¶å¯ä»¥ï¼ä½ å®Œå…¨å¯ä»¥**ä¸ä½¿ç”¨ä»»ä½•ç¬¬ä¸‰æ–¹åº“**ï¼Œä½¿ç”¨ Python æ ‡å‡†åº“ + è‡ªå®šä¹‰ç»“æž„ æ¥å®žçŽ°ä¸€ä¸ªæ¨¡å—åŒ–ã€æµç¨‹åŒ–çš„æ¨¡åž‹è®­ç»ƒç³»ç»Ÿã€‚

ä¸‹é¢æˆ‘å°†ä¸ºä½ æä¾›ä¸€ä¸ª**ä¸ä¾èµ–ä»»ä½•ç¬¬ä¸‰æ–¹æ¡†æž¶ï¼ˆå¦‚ Airflowã€Prefectã€Hydra ç­‰ï¼‰**çš„è®­ç»ƒæµç¨‹è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…å«ï¼š

- âœ… æ ¸å¿ƒæµç¨‹å¼•æ“Žï¼ˆæ”¯æŒé“¾å¼ / å‡½æ•°å¼ï¼‰
- âœ… é…ç½®é©±åŠ¨æ”¯æŒï¼ˆåŸºäºŽå­—å…¸æˆ– JSONï¼‰
- âœ… æ— ç¬¬ä¸‰æ–¹ä¾èµ–
- âœ… ç®€æ´æ¸…æ™°ã€é€‚åˆå¹³å°æ‰©å±•

------

## âœ… ä¸€ã€æ ¸å¿ƒæ€è·¯

- æ¯ä¸ªæµç¨‹æ­¥éª¤æ˜¯ä¸€ä¸ªç±»ï¼Œç»§æ‰¿ `BaseNode`ï¼Œå®žçŽ° `run(context)` æ–¹æ³•ã€‚
- `context` æ˜¯ä¸€ä¸ª Python å­—å…¸ï¼Œä¼ é€’æ•°æ®ã€‚
- `Pipeline` ç±»æŽ¥æ”¶å¤šä¸ªæ­¥éª¤ï¼Œå¹¶ä¾æ¬¡è°ƒç”¨å®ƒä»¬çš„ `run()`ã€‚

------

## âœ… äºŒã€å®Œæ•´ç¤ºä¾‹ä»£ç ï¼ˆæ— ç¬¬ä¸‰æ–¹ä¾èµ–ï¼‰

### 1ï¸âƒ£ `base_node.py` â€” åŸºç¡€æµç¨‹èŠ‚ç‚¹å®šä¹‰

```python
# base_node.py
from abc import ABC, abstractmethod

class BaseNode(ABC):
    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def run(self, context: dict) -> dict:
        """å¤„ç† context å¹¶è¿”å›žæ›´æ–°åŽçš„ context"""
        pass
```

------

### 2ï¸âƒ£ è‡ªå®šä¹‰å‡ ä¸ªèŠ‚ç‚¹ï¼ˆæ­¥éª¤æ¨¡å—ï¼‰

```python
# nodes.py
from base_node import BaseNode

class LoadDatasetNode(BaseNode):
    def __init__(self, paths: list[str]):
        super().__init__("LoadDataset")
        self.paths = paths

    def run(self, context: dict) -> dict:
        print(f"[{self.name}] åŠ è½½æ•°æ®: {self.paths}")
        context["raw_data"] = f"Data from {self.paths}"
        return context


class CleanDataNode(BaseNode):
    def __init__(self, required_columns: list[str]):
        super().__init__("CleanData")
        self.required_columns = required_columns

    def run(self, context: dict) -> dict:
        print(f"[{self.name}] æ¸…æ´—æ•°æ®ï¼šä¿ç•™ {self.required_columns}")
        context["clean_data"] = f"Cleaned({context['raw_data']})"
        return context


class TrainModelNode(BaseNode):
    def __init__(self, model_type: str, params: dict):
        super().__init__("TrainModel")
        self.model_type = model_type
        self.params = params

    def run(self, context: dict) -> dict:
        print(f"[{self.name}] è®­ç»ƒæ¨¡åž‹ï¼š{self.model_type} with {self.params}")
        context["trained_model"] = f"Model({self.model_type})"
        return context
```

------

### 3ï¸âƒ£ `pipeline.py` â€” æµç¨‹ç®¡ç†å™¨

```python
# pipeline.py
from typing import List
from base_node import BaseNode

class Pipeline:
    def __init__(self):
        self.steps: List[BaseNode] = []

    def add(self, step: BaseNode):
        self.steps.append(step)
        return self  # æ”¯æŒé“¾å¼è°ƒç”¨

    def run(self, context=None):
        context = context or {}
        for step in self.steps:
            print(f"\nâž¡ï¸ æ‰§è¡Œæ­¥éª¤ï¼š{step.name}")
            context = step.run(context)
        print("\nâœ… æµç¨‹å®Œæˆ")
        return context
```

------

### 4ï¸âƒ£ ä½¿ç”¨ç¤ºä¾‹ï¼šé“¾å¼æ–¹å¼è°ƒç”¨æµç¨‹

```python
# run_pipeline.py
from pipeline import Pipeline
from nodes import LoadDatasetNode, CleanDataNode, TrainModelNode

pipeline = (
    Pipeline()
    .add(LoadDatasetNode(paths=["data1.xlsx", "data2.xlsx"]))
    .add(CleanDataNode(required_columns=["age", "salary"]))
    .add(TrainModelNode(model_type="dnn", params={"layers": [64, 32]}))
)

final_context = pipeline.run()
print("\næœ€ç»ˆè¾“å‡ºä¸Šä¸‹æ–‡ï¼š", final_context)
```

------

## âœ… ä¸‰ã€æ”¯æŒ JSON/å­—å…¸ é…ç½®æ‰§è¡Œ

### ç”¨å­—å…¸å®šä¹‰ï¼š

```python
pipeline_config = [
    {"type": "LoadDatasetNode", "params": {"paths": ["a.xlsx", "b.xlsx"]}},
    {"type": "CleanDataNode", "params": {"required_columns": ["x", "y"]}},
    {"type": "TrainModelNode", "params": {"model_type": "dnn", "params": {"layers": [64]}}}
]
```

### åŠ è½½æ‰§è¡Œï¼ˆæ— éœ€å¤–éƒ¨ä¾èµ–ï¼‰ï¼š

```python
# dynamic_pipeline.py
from nodes import LoadDatasetNode, CleanDataNode, TrainModelNode
from pipeline import Pipeline

NODE_MAP = {
    "LoadDatasetNode": LoadDatasetNode,
    "CleanDataNode": CleanDataNode,
    "TrainModelNode": TrainModelNode,
}

def load_pipeline_from_config(config: list[dict]) -> Pipeline:
    pipeline = Pipeline()
    for step in config:
        node_class = NODE_MAP[step["type"]]
        node = node_class(**step["params"])
        pipeline.add(node)
    return pipeline
```

------

## âœ… å››ã€ç‰¹ç‚¹æ€»ç»“

| ç‰¹æ€§         | è¯´æ˜Ž                             |
| ------------ | -------------------------------- |
| âœ… æ— ä¾èµ–     | å…¨éƒ¨ä½¿ç”¨ Python æ ‡å‡†åº“           |
| ðŸ”Œ æ¨¡å—è§£è€¦   | æ¯ä¸ªæ­¥éª¤å•ç‹¬å°è£…ï¼Œä¾¿äºŽå¤ç”¨ä¸Žæµ‹è¯• |
| ðŸ“¦ æ˜“å¹³å°é›†æˆ | å¯åµŒå…¥ Django/Celery ä»»æ„ä»»åŠ¡ä¸­  |
| ðŸ“‹ å¯é…ç½®     | å­—å…¸æˆ– JSON æŽ§åˆ¶æµç¨‹             |
| ðŸ§© å¯æ‰©å±•     | æ–°èŠ‚ç‚¹åªéœ€ç»§æ‰¿ `BaseNode` å³å¯   |

------

## âœ… æ˜¯å¦ç»§ç»­ï¼Ÿ

æˆ‘å¯ä»¥å¸®ä½ ï¼š

- ðŸ” å¢žåŠ æ•°æ®é¢„å¤„ç†ã€å½’ä¸€åŒ–ã€æ¨¡åž‹ä¿å­˜ç­‰ Node
- ðŸ§ª ç”Ÿæˆä¸€ä¸ªæµ‹è¯•è¿è¡Œç¤ºä¾‹ï¼ˆè®­ç»ƒ + æ¨¡åž‹ä¿å­˜ + åå½’ä¸€åŒ–ï¼‰
- â˜ï¸ å¯¹æŽ¥ Celery å¼‚æ­¥é˜Ÿåˆ—/æ—¥å¿—æµå¼è¾“å‡º
- ðŸ› ï¸ æ•´ç†ä¸ºä¸€ä¸ª Python åŒ…ç»“æž„ï¼Œä¾›ä½ é¡¹ç›®å¯¼å…¥ä½¿ç”¨

ä½ æƒ³è¦æˆ‘ä¸‹ä¸€æ­¥å®Œå–„å“ªéƒ¨åˆ†ï¼Ÿæ˜¯å¦å¸Œæœ›æˆ‘æä¾›å®Œæ•´çš„é¡¹ç›®ç»“æž„å¹¶å°è£…ä¸ºæ¨¡å—åŒ…ï¼Ÿ

å¾ˆå¥½ï¼Œæˆ‘ä»¬æ¥æ‰©å±•ä½ çš„æ— ç¬¬ä¸‰æ–¹ä¾èµ–è®­ç»ƒæµç¨‹æ¡†æž¶ï¼š

------

## âœ… ç›®æ ‡

- å¢žåŠ ä»¥ä¸‹é€šç”¨èŠ‚ç‚¹ï¼š
  - `PreprocessNode`ï¼ˆæ•°æ®é¢„å¤„ç†ï¼Œå¦‚ç¼ºå¤±å¡«å……ã€ç±»åž‹è½¬æ¢ç­‰ï¼‰
  - `NormalizeNode`ï¼ˆæ•°å€¼åž‹å½’ä¸€åŒ–ï¼‰
  - `SaveModelNode`ï¼ˆä¿å­˜æ¨¡åž‹ç»“æž„æˆ–æƒé‡ä¿¡æ¯ï¼‰
- æ¯ä¸ªèŠ‚ç‚¹æ”¯æŒä¸€ä¸ªå¯æ‰©å±•çš„**`persist()`æœºåˆ¶**ï¼š
  - å¯é€‰æ˜¯å¦æŒä¹…åŒ–ç»“æžœï¼ˆå¦‚ä¿å­˜è‡³ S3ã€æœ¬åœ°æˆ–æ•°æ®åº“ï¼‰
  - æš‚ä¸å®žçŽ°é€»è¾‘ï¼Œä»…è®¾è®¡é€šç”¨æŽ¥å£ä¸Žç»“æž„

------

## âœ… ä¿®æ”¹ `BaseNode`ï¼šæ·»åŠ æŒä¹…åŒ–æŒ‚é’©

```python
# base_node.py
from abc import ABC, abstractmethod

class BaseNode(ABC):
    def __init__(self, name: str, persist: bool = False):
        self.name = name
        self.persist = persist  # æ˜¯å¦å¯ç”¨æŒä¹…åŒ–

    @abstractmethod
    def run(self, context: dict) -> dict:
        pass

    def save_result(self, key: str, value):
        """ç”¨äºŽä¿å­˜ç»“æžœï¼Œå¯æ‰©å±•åˆ° S3ã€æ•°æ®åº“æˆ–æœ¬åœ°"""
        if self.persist:
            print(f"[{self.name}] ðŸ”„ æ¨¡æ‹ŸæŒä¹…åŒ–ä¿å­˜ï¼š{key} -> {value[:50]}...")
            # çœŸå®žå®žçŽ°ä¸­å¯è°ƒç”¨ save_to_s3/save_to_db/save_to_disk ç­‰
```

------

## âœ… æ–°å¢žèŠ‚ç‚¹ 1ï¼šé¢„å¤„ç†èŠ‚ç‚¹ `PreprocessNode`

```python
# nodes.py
class PreprocessNode(BaseNode):
    def __init__(self, fields: list[str], fillna: float = 0.0, persist: bool = False):
        super().__init__("Preprocess", persist)
        self.fields = fields
        self.fillna = fillna

    def run(self, context: dict) -> dict:
        print(f"[{self.name}] æ•°æ®é¢„å¤„ç†ï¼šå¡«å……å­—æ®µ {self.fields} çš„ç¼ºå¤±å€¼ä¸º {self.fillna}")
        data = context["clean_data"]
        processed = f"Preprocessed({data})"
        context["preprocessed_data"] = processed
        self.save_result("preprocessed_data", processed)
        return context
```

------

## âœ… æ–°å¢žèŠ‚ç‚¹ 2ï¼šå½’ä¸€åŒ–èŠ‚ç‚¹ `NormalizeNode`

```python
class NormalizeNode(BaseNode):
    def __init__(self, fields: list[str], strategy: str = "minmax", persist: bool = False):
        super().__init__("Normalize", persist)
        self.fields = fields
        self.strategy = strategy

    def run(self, context: dict) -> dict:
        print(f"[{self.name}] å¯¹å­—æ®µ {self.fields} ä½¿ç”¨ç­–ç•¥ {self.strategy} å½’ä¸€åŒ–")
        data = context.get("preprocessed_data") or context.get("clean_data")
        normalized = f"Normalized({data})"
        context["normalized_data"] = normalized
        self.save_result("normalized_data", normalized)
        return context
```

------

## âœ… æ–°å¢žèŠ‚ç‚¹ 3ï¼šæ¨¡åž‹ä¿å­˜èŠ‚ç‚¹ `SaveModelNode`

```python
class SaveModelNode(BaseNode):
    def __init__(self, save_path: str, persist: bool = False):
        super().__init__("SaveModel", persist)
        self.save_path = save_path

    def run(self, context: dict) -> dict:
        model = context.get("trained_model")
        print(f"[{self.name}] ä¿å­˜æ¨¡åž‹åˆ°è·¯å¾„ {self.save_path}")
        self.save_result("model", f"ModelFile({model})")
        context["model_path"] = self.save_path
        return context
```

------

## âœ… ä½¿ç”¨ç¤ºä¾‹ï¼ˆé“¾å¼æˆ–å‡½æ•°å¼å‡å¯ï¼‰

```python
pipeline = (
    Pipeline()
    .add(LoadDatasetNode(paths=["a.xlsx", "b.xlsx"]))
    .add(CleanDataNode(required_columns=["age", "income"]))
    .add(PreprocessNode(fields=["age", "income"], fillna=0.0, persist=True))
    .add(NormalizeNode(fields=["age", "income"], persist=True))
    .add(TrainModelNode(model_type="dnn", params={"layers": [64, 32]}))
    .add(SaveModelNode(save_path="./model/dnn.pkl", persist=True))
)
context = pipeline.run()
```

------

## âœ… æŒä¹…åŒ–æ‰©å±•å»ºè®®

ä½ å¯ä»¥é€šè¿‡ç­–ç•¥ç±»å®žçŽ°æŒä¹…åŒ–æ”¯æŒï¼š

```python
class ResultSaver:
    def __init__(self, mode="local"):
        self.mode = mode  # å¯ä¸º 'local', 's3', 'db'

    def save(self, key: str, value):
        if self.mode == "local":
            print(f"[Local] Save {key}")
        elif self.mode == "s3":
            print(f"[S3] Upload {key}")
        elif self.mode == "db":
            print(f"[DB] Insert {key}")
```

åœ¨æ¯ä¸ª `Node` ä¸­è°ƒç”¨ `ResultSaver().save(key, value)` å³å¯è½»æ¾æ‰©å±•ã€‚

------

## âœ… å°ç»“

| æ¨¡å—               | è¯´æ˜Ž                                       |
| ------------------ | ------------------------------------------ |
| `PreprocessNode`   | æ‰§è¡Œå­—æ®µçš„ç¼ºå¤±å¡«å……ç­‰é¢„å¤„ç†æ“ä½œ             |
| `NormalizeNode`    | æ”¯æŒä¸åŒå½’ä¸€åŒ–ç­–ç•¥ï¼ˆå ä½å®žçŽ°ï¼‰             |
| `SaveModelNode`    | æ¨¡æ‹Ÿæ¨¡åž‹æ–‡ä»¶ä¿å­˜                           |
| `BaseNode.persist` | æŽ§åˆ¶æ˜¯å¦ä¿å­˜èŠ‚ç‚¹ä¸­é—´æ•°æ®                   |
| `save_result()`    | æ¨¡æ‹Ÿå°†ç»“æžœå†™å…¥å­˜å‚¨ç³»ç»Ÿï¼ŒåŽç»­å¯æ³¨å…¥å®žé™…é€»è¾‘ |

------

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ ï¼š

- ðŸ’¾ æ·»åŠ  `ResultSaver` å¯æ³¨å…¥ç­–ç•¥ç¤ºä¾‹
- â˜ï¸ æ•´åˆ Django ä¸­çš„ S3 å­˜å‚¨ï¼ˆä¾‹å¦‚ä½¿ç”¨ `boto3`ï¼‰ä½œä¸º `save_result()` çš„é»˜è®¤å®žçŽ°
- ðŸ§ª ä¸ºæ¯ä¸ª Node å†™ä¸€ä»½å•å…ƒæµ‹è¯•/æ¨¡æ‹Ÿä¸Šä¸‹æ–‡æµ‹è¯•

å‘Šè¯‰æˆ‘ä½ å¸Œæœ›ä¼˜å…ˆæŽ¨è¿›å“ªä¸€å—ï¼Œæˆ‘ç«‹åˆ»ä¸ºä½ æ‰©å±•ã€‚

å½“ç„¶å¯ä»¥ï¼Œæˆ‘å°†ä¸ºä½ **è¯¦ç»†è§£é‡Šè¿™ä¸ªæ¨¡åž‹è®­ç»ƒæµç¨‹çš„æ•´ä½“è®¾è®¡é€»è¾‘**ï¼Œä»¥åŠæ¯ä¸ªéƒ¨åˆ†çš„ä»£ç ç»“æž„å’Œä½œç”¨ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯**è‡ªå®šä¹‰æµç¨‹ç³»ç»Ÿ**ï¼Œå®Œå…¨åŸºäºŽ Python æ ‡å‡†åº“æž„å»ºï¼Œå…·å¤‡ï¼š

- æ¨¡å—åŒ–ï¼ˆæ¯ä¸€æ­¥éƒ½æ˜¯ä¸€ä¸ª Nodeï¼‰
- è§£è€¦ï¼ˆæ•°æ®é€šè¿‡ä¸Šä¸‹æ–‡ context æµè½¬ï¼‰
- å¯æ‰©å±•ï¼ˆæ”¯æŒæ¨¡åž‹ã€æ¸…æ´—ã€å½’ä¸€åŒ–ç­‰æ‰©å±•ï¼‰
- å¯æŒä¹…åŒ–ï¼ˆæ”¯æŒä¿å­˜ä¸­é—´ç»“æžœè‡³ S3/æ•°æ®åº“ç­‰ï¼‰

------

## âœ… ä¸€ã€æ•´ä½“æµç¨‹æ¦‚è§ˆå›¾

```
è®­ç»ƒæµç¨‹ï¼ˆPipelineï¼‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Pipeline.run()       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Node1â”‚â†’â†’â†’â”‚Node2 â”‚â†’â†’â†’â”‚Node3â”‚â†’â†’ ... â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¯ä¸ª Node æ‰§è¡Œ run(context)ï¼Œæ›´æ–°ä¸Šä¸‹æ–‡å¹¶è¿”å›ž
```

------

## âœ… äºŒã€æ ¸å¿ƒç»“æž„è¯´æ˜Ž

### 1. `BaseNode`: æ‰€æœ‰æµç¨‹æ­¥éª¤çš„åŸºç±»

```python
# base_node.py
from abc import ABC, abstractmethod

class BaseNode(ABC):
    def __init__(self, name: str, persist: bool = False):
        self.name = name
        self.persist = persist  # æ˜¯å¦æŒä¹…åŒ–å½“å‰èŠ‚ç‚¹è¾“å‡º

    @abstractmethod
    def run(self, context: dict) -> dict:
        pass

    def save_result(self, key: str, value):
        """æŒä¹…åŒ–ä¿å­˜ç»“æžœï¼ˆç›®å‰ä¸ºæ‰“å°æ¨¡æ‹Ÿï¼‰"""
        if self.persist:
            print(f"[{self.name}] æ¨¡æ‹Ÿä¿å­˜ {key}: {str(value)[:50]}...")
```

âœ… åŠŸèƒ½è¯´æ˜Žï¼š

- æä¾›ç»Ÿä¸€çš„ `run()` æŽ¥å£ï¼Œæ‰€æœ‰æµç¨‹èŠ‚ç‚¹å¿…é¡»å®žçŽ°ã€‚
- æä¾› `save_result()`ï¼Œé¢„ç•™æŒä¹…åŒ–èƒ½åŠ›ã€‚

------

### 2. `Pipeline`: æ‰§è¡Œæµç¨‹çš„è°ƒåº¦å™¨

```python
# pipeline.py
class Pipeline:
    def __init__(self):
        self.steps = []

    def add(self, step: BaseNode):
        self.steps.append(step)
        return self  # æ”¯æŒé“¾å¼è°ƒç”¨

    def run(self, context=None):
        context = context or {}
        for step in self.steps:
            print(f"âž¡ï¸ æ‰§è¡Œæ­¥éª¤ï¼š{step.name}")
            context = step.run(context)
        return context
```

âœ… åŠŸèƒ½è¯´æ˜Žï¼š

- æŽ¥æ”¶å¤šä¸ª Node å®žä¾‹
- æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ª `run(context)`ï¼Œå°†è¿”å›žç»“æžœä¼ ç»™ä¸‹ä¸€ä¸ª Node

------

## âœ… ä¸‰ã€æ¯ä¸ª Node çš„ä½œç”¨åŠä»£ç é€»è¾‘

### 3.1 åŠ è½½æ•°æ® `LoadDatasetNode`

```python
class LoadDatasetNode(BaseNode):
    def __init__(self, paths):
        super().__init__("LoadDataset")
        self.paths = paths

    def run(self, context):
        print(f"[{self.name}] åŠ è½½æ•°æ®ï¼š{self.paths}")
        context["raw_data"] = f"Data from {self.paths}"  # å®žé™…åº”è¯»å–æ–‡ä»¶
        return context
```

- æŽ¥æ”¶å¤šä¸ªè·¯å¾„ â†’ æ¨¡æ‹ŸåŠ è½½æ•°æ® â†’ ä¿å­˜ä¸º `raw_data`

------

### 3.2 æ•°æ®æ¸…æ´— `CleanDataNode`

```python
class CleanDataNode(BaseNode):
    def __init__(self, required_columns):
        super().__init__("CleanData")
        self.required_columns = required_columns

    def run(self, context):
        print(f"[{self.name}] æ¸…æ´—å­—æ®µï¼š{self.required_columns}")
        data = context["raw_data"]
        cleaned = f"Cleaned({data})"
        context["clean_data"] = cleaned
        return context
```

- æ¸…æ´—åªä¿ç•™æŒ‡å®šå­—æ®µ â†’ å­˜å…¥ `clean_data`

------

### 3.3 æ•°æ®é¢„å¤„ç† `PreprocessNode`

```python
class PreprocessNode(BaseNode):
    def __init__(self, fields, fillna=0.0, persist=False):
        super().__init__("Preprocess", persist)
        self.fields = fields
        self.fillna = fillna

    def run(self, context):
        print(f"[{self.name}] ç¼ºå¤±å€¼å¡«å……ï¼šå­—æ®µ={self.fields}, å¡«å……={self.fillna}")
        data = context["clean_data"]
        processed = f"Preprocessed({data})"
        context["preprocessed_data"] = processed
        self.save_result("preprocessed_data", processed)
        return context
```

- æ‰§è¡Œç¼ºå¤±å¡«å……ã€å­—æ®µæ ¡éªŒç­‰é¢„å¤„ç†é€»è¾‘
- å¯é€‰æŒä¹…åŒ–ä¿å­˜ï¼ˆå¦‚åˆ° S3ï¼‰

------

### 3.4 æ•°æ®å½’ä¸€åŒ– `NormalizeNode`

```python
class NormalizeNode(BaseNode):
    def __init__(self, fields, strategy="minmax", persist=False):
        super().__init__("Normalize", persist)
        self.fields = fields
        self.strategy = strategy

    def run(self, context):
        print(f"[{self.name}] å¯¹å­—æ®µ {self.fields} ä½¿ç”¨ {self.strategy} å½’ä¸€åŒ–")
        data = context.get("preprocessed_data") or context["clean_data"]
        normalized = f"Normalized({data})"
        context["normalized_data"] = normalized
        self.save_result("normalized_data", normalized)
        return context
```

- å¯¹æ•°å€¼åž‹å­—æ®µè¿›è¡Œå½’ä¸€åŒ–
- å°†å½’ä¸€åŒ–åŽçš„ç»“æžœä¿å­˜åœ¨ `normalized_data`

------

### 3.5 æ¨¡åž‹è®­ç»ƒ `TrainModelNode`

```python
class TrainModelNode(BaseNode):
    def __init__(self, model_type, params):
        super().__init__("TrainModel")
        self.model_type = model_type
        self.params = params

    def run(self, context):
        print(f"[{self.name}] è®­ç»ƒæ¨¡åž‹ç±»åž‹={self.model_type} å‚æ•°={self.params}")
        model = f"TrainedModel({self.model_type})"
        context["trained_model"] = model
        return context
```

- æ ¹æ®æ¨¡åž‹ç±»åž‹å’Œå‚æ•°è®­ç»ƒï¼ˆæ­¤å¤„ä¸ºæ¨¡æ‹Ÿï¼‰
- ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡åž‹å¯¹è±¡

------

### 3.6 æ¨¡åž‹ä¿å­˜ `SaveModelNode`

```python
class SaveModelNode(BaseNode):
    def __init__(self, save_path, persist=False):
        super().__init__("SaveModel", persist)
        self.save_path = save_path

    def run(self, context):
        model = context["trained_model"]
        print(f"[{self.name}] ä¿å­˜æ¨¡åž‹åˆ° {self.save_path}")
        self.save_result("model", f"ModelFile({model})")
        context["model_path"] = self.save_path
        return context
```

- å°†æ¨¡åž‹ä¿å­˜è·¯å¾„ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ï¼ˆå®žé™…å¯è°ƒç”¨ pickle/S3 ä¿å­˜ï¼‰
- ä½¿ç”¨ `save_result()` é¢„ç•™çœŸå®žä¿å­˜æŽ¥å£

------

## âœ… å››ã€è¿è¡Œå…¥å£è„šæœ¬ç¤ºä¾‹

```python
# run_pipeline.py
from pipeline import Pipeline
from nodes import LoadDatasetNode, CleanDataNode, PreprocessNode, NormalizeNode, TrainModelNode, SaveModelNode

pipeline = (
    Pipeline()
    .add(LoadDatasetNode(paths=["data1.xlsx", "data2.xlsx"]))
    .add(CleanDataNode(required_columns=["age", "salary"]))
    .add(PreprocessNode(fields=["age", "salary"], fillna=0, persist=True))
    .add(NormalizeNode(fields=["age", "salary"], strategy="minmax", persist=True))
    .add(TrainModelNode(model_type="dnn", params={"layers": [64, 32]}))
    .add(SaveModelNode(save_path="./model/dnn.pkl", persist=True))
)

context = pipeline.run()
print("æœ€ç»ˆè¾“å‡ºï¼š", context)
```

------

## âœ… æ€»ç»“é€»è¾‘æµç¨‹å›¾

```
context ç»“æž„
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ raw_data   â”‚ â† LoadDatasetNode
â”‚ clean_data â”‚ â† CleanDataNode
â”‚ preprocessed_data â† PreprocessNode
â”‚ normalized_data   â† NormalizeNode
â”‚ trained_model     â† TrainModelNode
â”‚ model_path        â† SaveModelNode
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

------

## âœ… ä½ ä¸‹ä¸€æ­¥å¯ä»¥é€‰æ‹©ï¼š

1. è®©æˆ‘å¸®ä½ å°†è¿™ä¸ªç»“æž„ç»„ç»‡æˆå®Œæ•´çš„ Python åŒ…ç›®å½•
2. ç”ŸæˆçœŸå®žçš„æ•°æ®å¤„ç†ä¸Žè®­ç»ƒé€»è¾‘ï¼ˆä½¿ç”¨ Pandasã€PyTorchï¼‰
3. æŽ¥å…¥ä»»åŠ¡é˜Ÿåˆ— Celery/Redis + å¼‚æ­¥è®­ç»ƒæµç¨‹
4. æŽ¥å…¥ S3ã€PostgreSQL è¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨

å‘Šè¯‰æˆ‘ä½ å¸Œæœ›è¿›ä¸€æ­¥å®Œå–„å“ªä¸ªæ¨¡å—ï¼Œæˆ‘å°±ç»§ç»­ä¸ºä½ å®žçŽ°ã€‚

è¿™æ˜¯ä¸€ä¸ªéžå¸¸é‡è¦ä¹Ÿéžå¸¸æœ‰ä»·å€¼çš„é—®é¢˜ â€”â€” **åœ¨è®­ç»ƒæµç¨‹ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹ï¼ˆNodeï¼‰ä¹‹é—´ç¡®å®žå¯èƒ½å­˜åœ¨æ•°æ®ä¾èµ–å…³ç³»**ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ä¸€å¥—é€šç”¨åˆæ¸…æ™°çš„æ–¹å¼æ¥ï¼š

- **è§„èŒƒæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥ï¼ˆä¾èµ–ï¼‰å’Œè¾“å‡ºï¼ˆäº§å‡ºï¼‰**
- **é¿å…æ•°æ®ä¸¢å¤±æˆ–æ‰§è¡Œé¡ºåºå‡ºé”™**
- **æ”¯æŒçµæ´»æ’æ‹”ã€é‡æŽ’æµç¨‹çš„èƒ½åŠ›**

------

## âœ… æ€»ä½“æ–¹æ¡ˆï¼šèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡ºä¾èµ–ç®¡ç†

æˆ‘ä»¬ä¸ºæ¯ä¸ª `BaseNode` å¢žåŠ ä¸¤ä¸ªæ˜¾å¼å±žæ€§ï¼š

| å±žæ€§                  | è¯´æ˜Ž                                          |
| --------------------- | --------------------------------------------- |
| `requires: list[str]` | ä¾èµ–çš„ `context` é”®ï¼Œå³å‰ç½®æ­¥éª¤å¿…é¡»äº§å‡ºçš„å†…å®¹ |
| `produces: list[str]` | æœ¬æ­¥éª¤è¾“å‡ºçš„ `context` é”®ï¼Œç”¨äºŽåŽç»­æ­¥éª¤æ¶ˆè´¹   |

------

## âœ… 1. ä¿®æ”¹ `BaseNode` æ”¯æŒä¾èµ–å£°æ˜Žæ£€æŸ¥

```python
class BaseNode(ABC):
    def __init__(self, name: str, persist: bool = False):
        self.name = name
        self.persist = persist
        self.requires: list[str] = []    # ä¾èµ–çš„ context key
        self.produces: list[str] = []    # æœ¬èŠ‚ç‚¹è¾“å‡ºçš„ context key

    @abstractmethod
    def run(self, context: dict) -> dict:
        pass

    def save_result(self, key: str, value):
        if self.persist:
            print(f"[{self.name}] æ¨¡æ‹ŸæŒä¹…åŒ–ï¼š{key} -> {str(value)[:50]}")

    def check_requirements(self, context: dict):
        missing = [k for k in self.requires if k not in context]
        if missing:
            raise ValueError(f"[{self.name}] ç¼ºå°‘ä¾èµ–æ•°æ®: {missing}")
```

åœ¨ `Pipeline.run()` ä¸­å¢žåŠ ä¾èµ–æ£€æŸ¥ï¼š

```python
def run(self, context=None):
    context = context or {}
    for step in self.steps:
        print(f"\nâž¡ï¸ æ‰§è¡Œæ­¥éª¤ï¼š{step.name}")
        step.check_requirements(context)
        context = step.run(context)
    return context
```

------

## âœ… 2. ç¤ºä¾‹ï¼šä¿®æ”¹æ¯ä¸ªèŠ‚ç‚¹çš„å®žçŽ°

### âœ… `LoadDatasetNode`

```python
class LoadDatasetNode(BaseNode):
    def __init__(self, paths):
        super().__init__("LoadDataset")
        self.paths = paths
        self.produces = ["raw_data"]

    def run(self, context):
        data = f"Data from {self.paths}"
        context["raw_data"] = data
        return context
```

------

### âœ… `CleanDataNode`

```python
class CleanDataNode(BaseNode):
    def __init__(self, required_columns):
        super().__init__("CleanData")
        self.required_columns = required_columns
        self.requires = ["raw_data"]
        self.produces = ["clean_data"]

    def run(self, context):
        data = context["raw_data"]
        cleaned = f"Cleaned({data})"
        context["clean_data"] = cleaned
        return context
```

------

### âœ… `NormalizeNode`

```python
class NormalizeNode(BaseNode):
    def __init__(self, fields, strategy="minmax", persist=False):
        super().__init__("Normalize", persist)
        self.fields = fields
        self.strategy = strategy
        self.requires = ["preprocessed_data"]  # æˆ– ["clean_data"]
        self.produces = ["normalized_data"]

    def run(self, context):
        data = context.get("preprocessed_data") or context["clean_data"]
        normalized = f"Normalized({data})"
        context["normalized_data"] = normalized
        self.save_result("normalized_data", normalized)
        return context
```

------

## âœ… 3. å¢žå¼ºï¼šè‡ªåŠ¨æ£€æµ‹èŠ‚ç‚¹é¡ºåºæ˜¯å¦åˆç†ï¼ˆå¯é€‰ï¼‰

åœ¨ `Pipeline.run()` ä¹‹å‰ï¼Œåšä¸€ä¸ª**é¡ºåºåˆç†æ€§é¢„æ£€æŸ¥**ï¼š

```python
def validate_dependencies(self):
    available = set()
    for step in self.steps:
        missing = [key for key in step.requires if key not in available]
        if missing:
            raise ValueError(f"[{step.name}] é¡ºåºé”™è¯¯ï¼Œç¼ºå°‘ä¾èµ–ï¼š{missing}")
        available.update(step.produces)
```

è°ƒç”¨ï¼š

```python
def run(self, context=None):
    self.validate_dependencies()
    ...
```

------

## âœ… æ€»ç»“ï¼šè®¾è®¡æ”¶ç›Š

| ç‰¹æ€§           | å®žçŽ°                                                    |
| -------------- | ------------------------------------------------------- |
| æ˜Žç¡®æ•°æ®æµä¾èµ– | æ¯ä¸ªèŠ‚ç‚¹ `requires` æ˜Žç¡®ä¾èµ–å“ªäº›é”®                      |
| é˜²æ­¢ä¹±åºæ‰§è¡Œ   | `Pipeline.validate_dependencies()` è‡ªåŠ¨æ£€æŸ¥             |
| ä¿è¯æµç¨‹å¥å£®æ€§ | å¦‚æžœç¼ºå¤±ä¾èµ–ï¼Œæå‰æŠ›é”™                                  |
| æ˜“äºŽç»´æŠ¤       | æ’æ‹”èŠ‚ç‚¹æ—¶ä¸ç”¨çœ‹å†…éƒ¨å®žçŽ°ï¼Œåªçœ‹ `requires` å’Œ `produces` |
| æ˜“äºŽæ–‡æ¡£ç”Ÿæˆ   | å¯è‡ªåŠ¨è¾“å‡ºæµç¨‹å›¾ä¸Žä¾èµ–è¯´æ˜Ž                              |

------

## âœ… ç¤ºä¾‹ï¼šè‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆï¼ˆå¯é€‰ï¼‰

ä½ ç”šè‡³å¯ä»¥è‡ªåŠ¨æ‰“å°å‡ºæµç¨‹ï¼š

```python
for step in pipeline.steps:
    print(f"ðŸ”§ {step.name}")
    print(f"   âž¤ è¾“å…¥ä¾èµ–: {step.requires}")
    print(f"   âž¤ è¾“å‡ºç»“æžœ: {step.produces}")
```

------

## âœ… ä¸‹ä¸€æ­¥å»ºè®®

æˆ‘å¯ä»¥ç»§ç»­å¸®ä½ ï¼š

1. ðŸ”§ æç‚¼å‡º `Node` çš„ schema æè¿°å’Œæ³¨å†Œæœºåˆ¶
2. ðŸ“Š ç”Ÿæˆæµç¨‹å›¾ï¼ˆä¾‹å¦‚è¾“å‡ºä¸º Graphviz æˆ– Mermaid å›¾ï¼‰
3. ðŸ§ª ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¾è®¡å•å…ƒæµ‹è¯•å’Œä¸Šä¸‹æ–‡æ¨¡æ‹Ÿæµ‹è¯•
4. â˜ï¸ å®žçŽ° `save_result()` çš„æŒä¹…åŒ–ç­–ç•¥ï¼ˆå¦‚ S3ã€PostgreSQLï¼‰

ä½ æƒ³ä¼˜å…ˆå¤„ç†å“ªä¸€å—ï¼Ÿæˆ‘å¯ä»¥é©¬ä¸Šå¼€å§‹ã€‚
