---
layout:     post
title:      "dnn"
subtitle:   ""
date:       2025-05-13
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - torch
---

æˆ‘ä»¬é€æ­¥è®²è§£ DNNï¼ˆDeep Neural Networkï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼‰æ¨¡å‹çš„å®ç°ã€åŸç†ä»¥åŠè®¾è®¡æ–¹æ³•ï¼Œå¹¶é™„ä¸€ä¸ªç®€å•çš„å®ç°ç¤ºä¾‹ã€‚

------

## ä¸€ã€ä»€ä¹ˆæ˜¯ DNNï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰ï¼Ÿ

DNN æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼‰ï¼Œç”±å¤šä¸ª**éšè—å±‚**ç»„æˆï¼Œæ¯ä¸€å±‚éƒ½åŒ…å«å¤šä¸ª**ç¥ç»å…ƒï¼ˆç¥ç»èŠ‚ç‚¹ï¼‰**ã€‚å¸¸ç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®ã€å›¾åƒã€è¯­éŸ³ã€è‡ªç„¶è¯­è¨€ç­‰ä»»åŠ¡ã€‚

### åŸºæœ¬ç»“æ„ï¼š

- è¾“å…¥å±‚ï¼šæ¥æ”¶åŸå§‹æ•°æ®ã€‚
- éšè—å±‚ï¼šé€šè¿‡éçº¿æ€§å˜æ¢æå–ç‰¹å¾ã€‚
- è¾“å‡ºå±‚ï¼šè¾“å‡ºç»“æœï¼Œå¦‚åˆ†ç±»æ¦‚ç‡ã€æ•°å€¼é¢„æµ‹ç­‰ã€‚

------

## äºŒã€DNN ä¸ºä»€ä¹ˆå¯ä»¥å®ç°é¢„æµ‹ï¼Ÿ

DNN æ¨¡å‹æœ¬è´¨æ˜¯ä¸€ä¸ª**å¤åˆå‡½æ•°**ï¼Œå®ƒé€šè¿‡å åŠ å¤šä¸ªçº¿æ€§å˜æ¢å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæ¨¡æ‹Ÿå¤æ‚çš„è¾“å…¥è¾“å‡ºæ˜ å°„å…³ç³»ã€‚

ä¾‹å¦‚ï¼Œä¸‰å±‚ç½‘ç»œç»“æ„å¦‚ä¸‹ï¼š

```
Output = f3(W3 Â· f2(W2 Â· f1(W1 Â· X + b1) + b2) + b3)
```

- `W` è¡¨ç¤ºæƒé‡çŸ©é˜µï¼Œ`b` è¡¨ç¤ºåç½®ï¼Œ`f` æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰ã€‚
- æ¯ä¸€å±‚å­¦åˆ°çš„æƒé‡ç»„åˆå’Œéçº¿æ€§å¯ä»¥é€¼è¿‘ä»»æ„å¤æ‚å‡½æ•°ï¼ˆå³**é€šç”¨é€¼è¿‘å®šç†**ï¼‰ã€‚

------

## ä¸‰ã€DNN å¦‚ä½•å®ç°ï¼šä»£ç ç¤ºä¾‹ï¼ˆPyTorchï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# å®šä¹‰ä¸€ä¸ªç®€å•çš„ DNN æ¨¡å‹
class SimpleDNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(SimpleDNN, self).__init__()
        layers = []

        last_size = input_size
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(last_size, hidden_size))
            layers.append(nn.ReLU())  # éçº¿æ€§æ¿€æ´»å‡½æ•°
            last_size = hidden_size

        layers.append(nn.Linear(last_size, output_size))  # è¾“å‡ºå±‚

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# æ¨¡å‹å®ä¾‹åŒ–
model = SimpleDNN(input_size=10, hidden_sizes=[64, 32], output_size=1)

# ç¤ºä¾‹è¾“å…¥
x = torch.randn(5, 10)  # 5ä¸ªæ ·æœ¬ï¼Œ10ç»´ç‰¹å¾
output = model(x)
print(output)
```

------

## å››ã€éšè—å±‚æ€ä¹ˆè®¾è®¡ï¼Ÿæœ‰ä»€ä¹ˆé€šç”¨è§„åˆ™ï¼Ÿ

è™½ç„¶æ²¡æœ‰**ç»å¯¹å›ºå®š**çš„è®¾è®¡æ–¹å¼ï¼Œä½†ä»¥ä¸‹æ˜¯ç»éªŒæ³•åˆ™ï¼š

### 1. å±‚æ•°ï¼ˆDepthï¼‰ï¼š

- **æµ…å±‚ï¼ˆ1-2å±‚ï¼‰**ï¼šé€‚åˆç®€å•ä»»åŠ¡æˆ–å°æ•°æ®é›†ã€‚
- **æ·±å±‚ï¼ˆ>3å±‚ï¼‰**ï¼šé€‚åˆå¤æ‚ä»»åŠ¡ï¼Œå¦‚å›¾åƒã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚

### 2. æ¯å±‚ç¥ç»å…ƒæ•°é‡ï¼ˆWidthï¼‰ï¼š

- é€šå¸¸ä»è¾“å…¥å¤§å°å‘ä¸­é—´å‹ç¼©æˆ–æ‰©å±•ï¼Œç„¶åå†åˆ°è¾“å‡ºå¤§å°æ”¶ç¼©ã€‚
- å¯å°è¯•ï¼š
  - é€’å‡ï¼šå¦‚ `[128, 64, 32]`
  - æ’å®šï¼šå¦‚ `[64, 64, 64]`

### 3. æ¿€æ´»å‡½æ•°ï¼š

- æ¨èä½¿ç”¨ `ReLU`ã€`LeakyReLU`ã€æˆ– `GELU`ï¼Œæ¯” `Sigmoid`/`Tanh` æ›´æ˜“è®­ç»ƒã€‚

### 4. é˜²æ­¢è¿‡æ‹Ÿåˆï¼š

- åŠ å…¥ **Dropout** å±‚ã€‚
- ä½¿ç”¨ **BatchNorm** å±‚ã€‚
- è®­ç»ƒæ—¶ä½¿ç”¨æ­£åˆ™åŒ–ã€æ—©åœç­‰æŠ€å·§ã€‚

------

## äº”ã€å¤æ‚ DNN çš„è®¾è®¡å»ºè®®

1. **é€æ­¥å¢åŠ å±‚æ•°å’Œå®½åº¦**ï¼Œè§‚å¯ŸéªŒè¯é›†æ€§èƒ½å˜åŒ–ï¼›
2. **å­¦ä¹ æ›²çº¿åˆ†æ**ï¼šè®­ç»ƒè¯¯å·®ä½ä½†éªŒè¯è¯¯å·®é«˜ â†’ æ¨¡å‹è¿‡æ‹Ÿåˆï¼›
3. **ä½¿ç”¨è‡ªåŠ¨è°ƒå‚**ï¼ˆå¦‚ Optunaã€Ray Tuneï¼‰ï¼›
4. **å¯è§†åŒ–ç½‘ç»œç»“æ„**å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œä¾¿äºè°ƒè¯•ï¼›
5. **åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆåœ¨å›¾åƒã€æ–‡æœ¬é¢†åŸŸå°¤ä¸ºæœ‰æ•ˆï¼‰ï¼›

------

## æ€»ç»“ï¼šDNN æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

1. å¤šå±‚ç¥ç»å…ƒå åŠ æ„æˆä¸€ä¸ªå¤æ‚çš„å‡½æ•°ï¼›
2. ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è®­ç»ƒæƒé‡ï¼›
3. æ¯å±‚é€šè¿‡çº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•°é€æ­¥æå–é«˜é˜¶ç‰¹å¾ï¼›
4. è¶Šå¤æ‚çš„é—®é¢˜ï¼Œéœ€è¦è¶Šæ·±ã€è¶Šå¹¿çš„ç½‘ç»œç»“æ„ã€‚

------

éœ€è¦æˆ‘ä¸ºä½ è®¾è®¡ä¸€ä¸ªé€‚ç”¨äºæŸä¸ªå…·ä½“ä»»åŠ¡çš„ DNN æ¨¡å‹ç»“æ„å—ï¼Ÿæ¯”å¦‚é¢„æµ‹æˆ¿ä»·ã€å›¾åƒåˆ†ç±»æˆ–æ–‡æœ¬æƒ…æ„Ÿåˆ†æç­‰ï¼Ÿ

å¥½çš„ï¼è¯·ä½ å…ˆå‘Šè¯‰æˆ‘ä»¥ä¸‹ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘æ‰èƒ½ä¸ºä½ è®¾è®¡ä¸€ä¸ªåˆé€‚çš„ DNN æ¨¡å‹ç»“æ„ï¼š

------

### â“1. **ä»»åŠ¡ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ**

- å›å½’ï¼ˆé¢„æµ‹æˆ¿ä»·ã€è‚¡ç¥¨ä»·æ ¼ï¼‰
- åˆ†ç±»ï¼ˆäºŒåˆ†ç±»/å¤šåˆ†ç±»ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€å›¾åƒè¯†åˆ«ï¼‰
- å…¶ä»–ï¼ˆå¦‚å¤šæ ‡ç­¾åˆ†ç±»ã€æ—¶é—´åºåˆ—é¢„æµ‹ï¼‰

### â“2. **è¾“å…¥æ•°æ®æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ**

- æœ‰å¤šå°‘ä¸ªç‰¹å¾ï¼ˆç»´åº¦ï¼‰ï¼Ÿ
- æ˜¯ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ã€å›¾åƒã€æ–‡æœ¬ï¼Œè¿˜æ˜¯å…¶ä»–å½¢å¼ï¼Ÿ

### â“3. **é¢„æœŸè¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ**

- ä¸€ä¸ªæ•°å€¼ï¼ˆå›å½’ï¼‰
- ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ï¼ˆåˆ†ç±»ï¼‰
- å¤šä¸ªæ¦‚ç‡ï¼ˆå¤šæ ‡ç­¾ï¼‰

------

ä¾‹å¦‚ï¼Œå¦‚æœä½ è¦åšâ€œæˆ¿ä»·é¢„æµ‹â€ï¼Œä½ å¯ä»¥å‘Šè¯‰æˆ‘ï¼š

- ä»»åŠ¡ï¼šå›å½’
- è¾“å…¥ï¼šç»“æ„åŒ–æ•°æ®ï¼Œæœ‰ 15 ä¸ªç‰¹å¾ï¼ˆå¦‚é¢ç§¯ã€ä½ç½®ã€æˆ¿é—´æ•°ç­‰ï¼‰
- è¾“å‡ºï¼šä¸€ä¸ªè¿ç»­çš„æˆ¿ä»·æ•°å€¼

åªè¦ä½ æä¾›è¿™äº›åŸºæœ¬ä¿¡æ¯ï¼Œæˆ‘å°±å¯ä»¥ç»™å‡º**æ¨¡å‹ç»“æ„ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€è®­ç»ƒå»ºè®®å’Œä»£ç å®ç°**ã€‚ä½ ç°åœ¨æ‰“ç®—å®ç°å“ªä¸€ç±»ä»»åŠ¡ï¼Ÿ

å¾ˆå¥½ï¼ä½ çš„ä»»åŠ¡æ˜¯ä¸€ä¸ª**å›å½’é—®é¢˜**ï¼Œè¾“å…¥æ˜¯**ç»“æ„åŒ–è¡¨æ ¼æ•°æ®**ï¼Œç›®æ ‡æ˜¯é¢„æµ‹**ä¸€ä¸ªæ•°å€¼**ï¼ˆå…¸å‹çš„å¦‚æˆ¿ä»·é¢„æµ‹ã€é”€å”®é¢é¢„æµ‹ç­‰ï¼‰ã€‚

------

## âœ… 1. ç½‘ç»œè®¾è®¡åŸåˆ™ï¼ˆé€‚ç”¨äºç»“æ„åŒ–æ•°æ®å›å½’ï¼‰

- **è¾“å…¥å±‚**ï¼šç‰¹å¾æ•°é‡ = ä½ çš„æ•°æ®ä¸­æ¯ä¸ªæ ·æœ¬çš„åˆ—æ•°ï¼ˆæ¯”å¦‚ 10 ä¸ªå­—æ®µå°±è¾“å…¥ 10ï¼‰ã€‚
- **éšè—å±‚**ï¼š
  - 2ï½4 å±‚é€šå¸¸å°±è¶³å¤Ÿã€‚
  - ç¥ç»å…ƒä¸ªæ•°å¯ä» 64 å¼€å§‹é€å±‚å‡å°‘ï¼ˆä¾‹å¦‚ 64 â†’ 32 â†’ 16ï¼‰ã€‚
- **æ¿€æ´»å‡½æ•°**ï¼š`ReLU`ï¼ˆå¿«é€Ÿæ”¶æ•›ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚
- **è¾“å‡ºå±‚**ï¼š
  - åªæœ‰ **1 ä¸ªç¥ç»å…ƒ**ï¼Œå› ä¸ºä½ åªé¢„æµ‹ä¸€ä¸ªæ•°å€¼ã€‚
  - **æ— æ¿€æ´»å‡½æ•°**ï¼ˆå³çº¿æ€§è¾“å‡ºï¼‰ç”¨äºå›å½’ã€‚
- **æŸå¤±å‡½æ•°**ï¼š`MSELoss`ï¼ˆå‡æ–¹è¯¯å·®ï¼‰ã€‚

------

## âœ… 2. PyTorch å®ç°ä»£ç ï¼ˆé€‚åˆå…¥é—¨å›å½’ä»»åŠ¡ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim

# å‡è®¾ä½ çš„è¾“å…¥ç‰¹å¾æœ‰ 10 ä¸ª
class DNNRegressor(nn.Module):
    def __init__(self, input_size):
        super(DNNRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)  # å›å½’è¾“å‡ºå±‚ï¼Œæ— æ¿€æ´»
        )

    def forward(self, x):
        return self.model(x)

# æ¨¡æ‹Ÿè¾“å…¥
input_size = 10
model = DNNRegressor(input_size)

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ç¤ºä¾‹è®­ç»ƒï¼ˆä¼ªæ•°æ®ï¼‰
x_train = torch.randn(100, input_size)
y_train = torch.randn(100, 1)

for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

------

## âœ… 3. è®­ç»ƒå»ºè®®

- **å½’ä¸€åŒ–è¾“å…¥æ•°æ®**ï¼ˆæ ‡å‡†åŒ–ç‰¹å¾ï¼Œèƒ½æ˜¾è‘—æå‡è®­ç»ƒæ•ˆæœï¼‰ï¼›
- **æ‰¹è®­ç»ƒï¼ˆBatch Trainingï¼‰**ï¼šä½¿ç”¨ `DataLoader` æŒ‰å°æ‰¹é‡è®­ç»ƒæ›´ç¨³å®šï¼›
- **å­¦ä¹ ç‡è°ƒä¼˜**ï¼šå¯ä»¥å°è¯•ä» `0.001` å¼€å§‹ï¼›
- **æ—©åœæ³• Early Stopping**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼›
- **è¯„ä¼°æŒ‡æ ‡**ï¼šé™¤äº† MSEï¼Œä½ è¿˜å¯ä»¥ç›‘æ§ MAEã€RÂ² åˆ†æ•°ç­‰ï¼›

------

## âœ… 4. ç¤ºä¾‹æ¨¡å‹ç»“æ„å›¾

```
Input (10 features)
    â†“
Linear(10 â†’ 64) + ReLU
    â†“
Linear(64 â†’ 32) + ReLU
    â†“
Linear(32 â†’ 16) + ReLU
    â†“
Linear(16 â†’ 1) â†’ è¾“å‡ºé¢„æµ‹å€¼
```

------

éœ€è¦æˆ‘å¸®ä½ åŠ ä¸Š `DataLoader` æˆ–æ”¯æŒ CSV æ–‡ä»¶è¯»å–ã€å®Œæ•´è®­ç»ƒ/éªŒè¯æµç¨‹å—ï¼Ÿ

éå¸¸å¥½ï¼ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ DNN å›å½’æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œ**æ”¯æŒè¯»å– CSV æ–‡ä»¶ã€ä½¿ç”¨ DataLoaderã€æ ‡å‡†åŒ–æ•°æ®ã€è®­ç»ƒå’ŒéªŒè¯æµç¨‹**ã€‚æˆ‘ä¼šé€æ®µè§£é‡Šæ¯éƒ¨åˆ†ä»£ç çš„ä½œç”¨ã€‚

------

## âœ… ç¯å¢ƒä¾èµ–

ä½ éœ€è¦å…ˆå®‰è£… PyTorch å’Œ Pandasï¼ˆç”¨äºè¯»å– CSV æ–‡ä»¶ï¼‰ï¼š

```bash
pip install torch pandas scikit-learn
```

------

## âœ… å®Œæ•´ä»£ç ï¼ˆå«è§£é‡Šï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

### âœ… è¯´æ˜ï¼š

- å¼•å…¥æ‰€éœ€çš„åº“ï¼›
- `torch.utils.data.Dataset` æ˜¯ç”¨äºå®šä¹‰è‡ªå®šä¹‰æ•°æ®é›†ï¼›
- `StandardScaler` ç”¨äºç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæå‡è®­ç»ƒæ•ˆæœï¼‰ã€‚

------

```python
# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # ä¿è¯è¾“å‡ºç»´åº¦ä¸€è‡´

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

### âœ… è¯´æ˜ï¼š

- æŠŠ `numpy` æ ¼å¼çš„ç‰¹å¾å’Œæ ‡ç­¾è½¬æ¢ä¸º `torch.Tensor`ï¼›
- å®ç°äº† `__getitem__`ï¼Œæ–¹ä¾¿ DataLoader è°ƒç”¨ã€‚

------

```python
# DNN æ¨¡å‹ç»“æ„
class DNNRegressor(nn.Module):
    def __init__(self, input_size):
        super(DNNRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)  # å›å½’ï¼Œè¾“å‡ºä¸€ä¸ªå€¼
        )

    def forward(self, x):
        return self.model(x)
```

### âœ… è¯´æ˜ï¼š

- 4 å±‚çº¿æ€§å±‚ï¼›
- æ¯å±‚ä½¿ç”¨ `ReLU` æ¿€æ´»ï¼›
- æœ€åä¸€å±‚æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºä¸€ä¸ªè¿ç»­æ•°å€¼ã€‚

------

```python
# è¯»å– CSV æ–‡ä»¶ï¼ˆä½ å¯ä»¥æ¢æˆè‡ªå·±çš„æ–‡ä»¶è·¯å¾„ï¼‰
df = pd.read_csv("your_data.csv")

# å‡è®¾ 'target' æ˜¯ä½ è¦é¢„æµ‹çš„åˆ—
X = df.drop(columns=["target"]).values
y = df["target"].values

# æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# æ„é€ æ•°æ®é›†å’ŒåŠ è½½å™¨
train_dataset = RegressionDataset(X_train, y_train)
val_dataset = RegressionDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
```

### âœ… è¯´æ˜ï¼š

- ä» CSV è¯»å–æ•°æ®ï¼›
- æŠŠ `target` ä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼›
- æ ‡å‡†åŒ–ç‰¹å¾ï¼›
- åˆ†å‡ºéªŒè¯é›†ï¼ˆ20%ï¼‰ï¼›
- ä½¿ç”¨ `DataLoader` æ–¹ä¾¿æ‰¹é‡è®­ç»ƒã€‚

------

```python
# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = DNNRegressor(input_size=X.shape[1]).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

### âœ… è¯´æ˜ï¼š

- å°†æ¨¡å‹æ”¾åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼›
- ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼›
- ä½¿ç”¨ Adam ä¼˜åŒ–å™¨è®­ç»ƒç½‘ç»œå‚æ•°ã€‚

------

```python
# è®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹
num_epochs = 50
for epoch in range(num_epochs):
    # Training
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        output = model(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * X_batch.size(0)

    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            output = model(X_batch)
            loss = criterion(output, y_batch)
            val_loss += loss.item() * X_batch.size(0)

    train_loss /= len(train_loader.dataset)
    val_loss /= len(val_loader.dataset)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
```

### âœ… è¯´æ˜ï¼š

- æ¯ä¸ª epoch åŒ…å«è®­ç»ƒå’ŒéªŒè¯ä¸¤ä¸ªé˜¶æ®µï¼›
- ä½¿ç”¨ `model.train()` å’Œ `model.eval()` æ§åˆ¶è¡Œä¸ºï¼›
- ç´¯è®¡æŸå¤±å€¼ï¼Œè®¡ç®—å¹³å‡è®­ç»ƒ/éªŒè¯è¯¯å·®ã€‚

------

## âœ… æ€»ç»“ç»“æ„å›¾

```text
CSV â†’ pandas â†’ æ ‡å‡†åŒ– â†’ DataLoader
                  â†“
              DNNRegressor
                  â†“
         MSELoss + Adamä¼˜åŒ–
                  â†“
         é¢„æµ‹ç»“æœ + éªŒè¯è¯„ä¼°
```

------

## âœ… å¯é€‰å¢å¼ºé¡¹ï¼ˆåç»­å¯åŠ ï¼‰ï¼š

- ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆ`torch.save`ï¼‰
- åŠ å…¥ EarlyStopping
- æ·»åŠ  TensorBoard æ—¥å¿—å¯è§†åŒ–
- å°è£…ä¸ºä¸€ä¸ªå¯å¤ç”¨ç±»/è®­ç»ƒå™¨

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥åŸºäºè¿™ä¸ªæ¨¡æ¿å¸®ä½ **å°è£…æˆä¸€ä¸ªè®­ç»ƒè„šæœ¬æˆ–æ¨¡å—**ï¼Œæˆ–è€…é›†æˆæ›´å¤æ‚çš„åŠŸèƒ½ï¼ˆå¦‚è‡ªåŠ¨è°ƒå‚ã€ç»˜å›¾ç­‰ï¼‰ã€‚ä½ å¸Œæœ›è¿è¡Œçš„ CSV æ–‡ä»¶æ ·ä¾‹é•¿ä»€ä¹ˆæ ·ï¼Ÿæˆ‘å¯ä»¥å¸®ä½ å†™ä¸€ä¸ªæ ·æœ¬ã€‚

éå¸¸å¥½ï¼ä¸‹é¢æˆ‘å°†ä¸ºä½ æ‰©å±• DNN å›å½’è®­ç»ƒæ¡†æ¶ï¼Œé›†æˆä»¥ä¸‹**å¤æ‚åŠŸèƒ½**ï¼Œé€‚åˆç”Ÿäº§çº§è®­ç»ƒï¼š

------

## âœ… **é›†æˆåŠŸèƒ½æ¦‚è§ˆ**

1. âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›† Loss æœ€ä½ï¼‰
2. âœ… Early Stoppingï¼ˆæå‰ç»ˆæ­¢è®­ç»ƒä»¥é¿å…è¿‡æ‹Ÿåˆï¼‰
3. âœ… TensorBoard å¯è§†åŒ–è®­ç»ƒæ—¥å¿—ï¼ˆè®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿ï¼‰
4. âœ… æ›´è§„èŒƒçš„è®­ç»ƒæµç¨‹å°è£…

------

## âœ… å®Œæ•´ä»£ç ç»“æ„

æˆ‘å°†å®Œæ•´è®­ç»ƒæµç¨‹æ‹†æˆå‡ ä¸ªéƒ¨åˆ†ï¼Œç»„ç»‡æ›´æ¸…æ™°ï¼š

```
train.py                # ä¸»ç¨‹åºå…¥å£
models.py               # DNN æ¨¡å‹ç»“æ„
dataset.py              # è‡ªå®šä¹‰æ•°æ®é›†ç±»
utils.py                # è®­ç»ƒå·¥å…·å‡½æ•°ï¼ˆearly stoppingã€ä¿å­˜æ¨¡å‹ç­‰ï¼‰
logs/                   # TensorBoard æ—¥å¿—ç›®å½•
checkpoints/            # æ¨¡å‹ä¿å­˜ç›®å½•
```

------

## âœ… æ ·ä¾‹ä»£ç å®ç°ï¼ˆä»¥å•æ–‡ä»¶å½¢å¼å±•ç¤ºï¼Œå®é™…å¯æŒ‰æ¨¡å—æ‹†åˆ†ï¼‰

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# ======================== æ•°æ®é›†å®šä¹‰ ========================
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ======================== æ¨¡å‹å®šä¹‰ ========================
class DNNRegressor(nn.Module):
    def __init__(self, input_size):
        super(DNNRegressor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.model(x)

# ======================== è®­ç»ƒè¾…åŠ©åŠŸèƒ½ ========================
class EarlyStopping:
    def __init__(self, patience=10, verbose=True, delta=1e-4):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_loss = np.Inf
        self.early_stop = False
        self.delta = delta

    def __call__(self, val_loss, model, path):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.counter = 0
            torch.save(model.state_dict(), path)
            if self.verbose:
                print(f"â†’ Validation loss improved, model saved.")
        else:
            self.counter += 1
            if self.verbose:
                print(f"â†’ No improvement. Early stop counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True

# ======================== ä¸»è®­ç»ƒå‡½æ•° ========================
def train():
    # å‚æ•°
    csv_path = "your_data.csv"
    target_column = "target"
    log_dir = "logs"
    model_path = "checkpoints/best_model.pt"
    os.makedirs("checkpoints", exist_ok=True)

    # åŠ è½½æ•°æ®
    df = pd.read_csv(csv_path)
    X = df.drop(columns=[target_column]).values
    y = df[target_column].values

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ‡åˆ†æ•°æ®
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    train_loader = DataLoader(RegressionDataset(X_train, y_train), batch_size=32, shuffle=True)
    val_loader = DataLoader(RegressionDataset(X_val, y_val), batch_size=32)

    # è®¾å¤‡é€‰æ‹©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆå§‹åŒ–æ¨¡å‹
    model = DNNRegressor(input_size=X.shape[1]).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    early_stopper = EarlyStopping(patience=10)
    writer = SummaryWriter(log_dir=log_dir)

    # è®­ç»ƒå¾ªç¯
    epochs = 100
    for epoch in range(epochs):
        model.train()
        total_train_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item() * X_batch.size(0)

        avg_train_loss = total_train_loss / len(train_loader.dataset)

        # éªŒè¯
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                loss = criterion(output, y_batch)
                total_val_loss += loss.item() * X_batch.size(0)

        avg_val_loss = total_val_loss / len(val_loader.dataset)

        # æ—¥å¿—è®°å½•
        writer.add_scalars("Loss", {
            "Train": avg_train_loss,
            "Validation": avg_val_loss
        }, epoch)

        print(f"[Epoch {epoch+1:03d}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        early_stopper(avg_val_loss, model, model_path)
        if early_stopper.early_stop:
            print("âœ‹ Early stopping triggered.")
            break

    writer.close()
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

if __name__ == "__main__":
    train()
```

------

## âœ… ä½¿ç”¨è¯´æ˜

1. å°†ä½ çš„ CSV æ–‡ä»¶æ”¾åœ¨åŒä¸€ç›®å½•ï¼Œå¹¶æ›¿æ¢è·¯å¾„ `your_data.csv`

2. `target_column` æ”¹ä¸ºä½ è¦é¢„æµ‹çš„åˆ—å

3. è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

   ```bash
   python train.py
   ```

4. è¿è¡Œ TensorBoard æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š

   ```bash
   tensorboard --logdir=logs
   ```

------

## âœ… å¯è§†åŒ–æ•ˆæœï¼ˆåœ¨ TensorBoardï¼‰

- æŸ¥çœ‹æŸå¤±æ›²çº¿æ”¶æ•›æƒ…å†µï¼›
- ç›‘æ§æ˜¯å¦è¿‡æ‹Ÿåˆï¼›
- ç›´è§‚å¯¹æ¯”ä¸åŒæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

------

## âœ… ä¸‹ä¸€æ­¥å»ºè®®

å¦‚æœä½ è¿˜éœ€è¦ï¼š

- åŠ å…¥éªŒè¯æŒ‡æ ‡ï¼ˆå¦‚ MAEã€RÂ²ï¼‰
- å°è£…æˆç±» / è®­ç»ƒå™¨ç»“æ„
- å¤šæ¨¡å‹å¯¹æ¯”è‡ªåŠ¨è°ƒå‚ï¼ˆå¦‚ GridSearchã€Optunaï¼‰

æˆ‘éƒ½å¯ä»¥ç»§ç»­å¸®ä½ æ‰©å±•ã€‚

ä½ æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ **è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªé¡¹ç›®ç»“æ„**æˆ–è€…åˆ›å»ºä¸€ä¸ª `.ipynb` çš„ notebook ç‰ˆæœ¬ç”¨äºè°ƒè¯•ï¼Ÿ

å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç»è¿‡å®Œæ•´å°è£…çš„ DNN å›å½’è®­ç»ƒæ¡†æ¶ï¼Œå…·å¤‡ä»¥ä¸‹é«˜çº§ç‰¹æ€§ï¼š

------

## âœ… åŠŸèƒ½åˆ—è¡¨

| åŠŸèƒ½                   | æè¿°                                                   |
| ---------------------- | ------------------------------------------------------ |
| **éªŒè¯æŒ‡æ ‡**           | æ·»åŠ  MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰ã€RÂ²ï¼ˆåˆ¤å®šç³»æ•°ï¼‰ä½œä¸ºè¯„ä¼°æŒ‡æ ‡   |
| **è®­ç»ƒå™¨ç»“æ„å°è£…**     | è®­ç»ƒè¿‡ç¨‹ã€éªŒè¯è¿‡ç¨‹ã€EarlyStopping å’Œä¿å­˜ç­‰å°è£…è¿›ç±»ä¸­   |
| **æ”¯æŒå¤šæ¨¡å‹ç»“æ„å¯¹æ¯”** | å¯ä¼ å…¥ä¸åŒæ¨¡å‹ç±»è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°                         |
| **æ”¯æŒè¶…å‚æ•°è°ƒä¼˜æ¥å£** | å¯é€šè¿‡å¤–éƒ¨è‡ªåŠ¨æœç´¢å™¨ï¼ˆå¦‚ Optunaã€Grid Searchï¼‰è¿›è¡Œè°ƒå‚ |

------

## âœ… ç»“æ„é¢„è§ˆï¼ˆå¯ä¸º `.py` æ¨¡å—æˆ– `.ipynb`ï¼‰

### 1. æ¨¡å‹å®šä¹‰ï¼ˆæ”¯æŒå¤šä¸ªï¼‰

```python
import torch.nn as nn

class DNN_V1(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)


class DNN_V2(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)
```

------

### 2. å›å½’è®­ç»ƒå™¨ï¼ˆ`RegressionTrainer` ç±»ï¼‰

```python
import torch
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np
import os

class RegressionTrainer:
    def __init__(self, model, optimizer, criterion, device, model_name="default"):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.writer = SummaryWriter(log_dir=f"logs/{model_name}")
        self.model_name = model_name
        self.best_val_loss = np.Inf
        self.patience = 10
        self.counter = 0
        os.makedirs("checkpoints", exist_ok=True)
        self.save_path = f"checkpoints/{model_name}.pt"

    def train(self, train_loader, val_loader, epochs=100):
        for epoch in range(epochs):
            train_loss = self._train_one_epoch(train_loader)
            val_loss, mae, r2 = self._validate(val_loader)

            self.writer.add_scalars("Loss", {
                "Train": train_loss,
                "Val": val_loss
            }, epoch)
            self.writer.add_scalar("MAE", mae, epoch)
            self.writer.add_scalar("R2", r2, epoch)

            print(f"[Epoch {epoch+1}] Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}")
            if self._check_early_stopping(val_loss):
                print("â†’ Early stopping.")
                break
        self.writer.close()

    def _train_one_epoch(self, loader):
        self.model.train()
        total_loss = 0
        for X, y in loader:
            X, y = X.to(self.device), y.to(self.device)
            self.optimizer.zero_grad()
            output = self.model(X)
            loss = self.criterion(output, y)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item() * X.size(0)
        return total_loss / len(loader.dataset)

    def _validate(self, loader):
        self.model.eval()
        total_loss = 0
        all_preds, all_targets = [], []
        with torch.no_grad():
            for X, y in loader:
                X, y = X.to(self.device), y.to(self.device)
                output = self.model(X)
                loss = self.criterion(output, y)
                total_loss += loss.item() * X.size(0)
                all_preds.append(output.cpu().numpy())
                all_targets.append(y.cpu().numpy())
        preds = np.concatenate(all_preds)
        targets = np.concatenate(all_targets)
        mae = mean_absolute_error(targets, preds)
        r2 = r2_score(targets, preds)
        return total_loss / len(loader.dataset), mae, r2

    def _check_early_stopping(self, val_loss):
        if val_loss < self.best_val_loss - 1e-4:
            self.best_val_loss = val_loss
            self.counter = 0
            torch.save(self.model.state_dict(), self.save_path)
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience
```

------

### 3. ä¸»è®­ç»ƒæµç¨‹

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim

# æ•°æ®é›†
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# æ•°æ®å‡†å¤‡
df = pd.read_csv("your_data.csv")
X = df.drop(columns=["target"]).values
y = df["target"].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2)

train_loader = DataLoader(RegressionDataset(X_train, y_train), batch_size=32, shuffle=True)
val_loader = DataLoader(RegressionDataset(X_val, y_val), batch_size=32)

# è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å¤šæ¨¡å‹æ¯”è¾ƒ
from models import DNN_V1, DNN_V2
from trainer import RegressionTrainer

models = {
    "DNN_V1": DNN_V1(X.shape[1]),
    "DNN_V2": DNN_V2(X.shape[1]),
}

for name, model in models.items():
    print(f"\nğŸ” Training {name}")
    trainer = RegressionTrainer(
        model=model,
        optimizer=optim.Adam(model.parameters(), lr=0.001),
        criterion=nn.MSELoss(),
        device=device,
        model_name=name
    )
    trainer.train(train_loader, val_loader)
```

------

## âœ… å¯é€‰ï¼šåŠ å…¥ Optuna è‡ªåŠ¨è°ƒå‚æ¥å£ï¼ˆç¤ºä¾‹ï¼‰

```python
import optuna

def objective(trial):
    hidden = trial.suggest_int("hidden", 32, 128)
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)

    model = nn.Sequential(
        nn.Linear(X.shape[1], hidden),
        nn.ReLU(),
        nn.Linear(hidden, 1)
    )

    trainer = RegressionTrainer(
        model=model,
        optimizer=optim.Adam(model.parameters(), lr=lr),
        criterion=nn.MSELoss(),
        device=device,
        model_name=f"trial_{trial.number}"
    )

    trainer.train(train_loader, val_loader)
    return trainer.best_val_loss

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=20)
```

------

## âœ… ç»“æœè¾“å‡ºä¸åˆ†æ

ä½ å°†ä¼šæ‹¥æœ‰ï¼š

- æ¯ä¸ªæ¨¡å‹å•ç‹¬çš„ TensorBoard æ—¥å¿—æ›²çº¿
- æ¯ä¸ªæ¨¡å‹è®­ç»ƒæ—¶éªŒè¯ loss / MAE / RÂ² çš„æ‰“å°è¾“å‡º
- æ¯ä¸ªæ¨¡å‹çš„ `.pt` æƒé‡æ–‡ä»¶ä¿å­˜åœ¨ `checkpoints/`

------

æ˜¯å¦éœ€è¦æˆ‘ï¼š

- æ•´ç†æˆ `.ipynb` æ–‡ä»¶ä¸Šä¼ ç»™ä½ ï¼Ÿ
- å¸®ä½ å®ç°å¤šçº¿ç¨‹/å¤šå¡å¹¶è¡Œæ¯”è¾ƒå¤šä¸ªæ¨¡å‹ï¼Ÿ
- è‡ªåŠ¨ç”Ÿæˆå®Œæ•´å·¥ç¨‹ç›®å½•å’Œå¯å¤ç”¨è„šæ‰‹æ¶ï¼Ÿ

ä½ æ›´å€¾å‘æœ¬åœ°è·‘è¿˜æ˜¯äº‘å¹³å°è·‘ï¼Ÿæˆ‘å¯ä»¥å¸®ä½ é€‚é…ã€‚

è¿™æ˜¯ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼åˆ¤æ–­ä¸€ä¸ª **DNN æ¨¡å‹æ˜¯å¦è®­ç»ƒæˆåŠŸ**ï¼Œä»¥åŠå®ƒçš„ **é¢„æµ‹æ˜¯å¦å‡†ç¡®**ï¼Œéœ€è¦ä»å¤šä¸ªè§’åº¦å…¥æ‰‹ã€‚ä¸‹é¢æ˜¯è¯¦ç»†è®²è§£ï¼š

------

## âœ… ä¸€ã€åˆ¤æ–­æ¨¡å‹æ˜¯å¦è®­ç»ƒæˆåŠŸ

å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢åˆ¤æ–­ï¼š

### 1. **è®­ç»ƒä¸éªŒè¯ Loss æ˜¯å¦ä¸‹é™**

- çœ‹ TensorBoard ä¸­çš„æ›²çº¿ï¼šæ˜¯å¦å¹³ç¨³ä¸‹é™ï¼Ÿ
- éªŒè¯é›† Loss æ˜¯å¦ä¹ŸåŒæ­¥ä¸‹é™ï¼ˆä»£è¡¨æ¨¡å‹æ²¡è¿‡æ‹Ÿåˆï¼‰ï¼Ÿ

ğŸ“‰ ç¤ºä¾‹ï¼ˆè‰¯å¥½æƒ…å†µï¼‰ï¼š

```
Epoch 1: Train Loss 1.203, Val Loss 1.180
Epoch 10: Train Loss 0.521, Val Loss 0.534
Epoch 50: Train Loss 0.312, Val Loss 0.318 âœ…
```

ğŸ“ˆ ç¤ºä¾‹ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼š

```
Epoch 1: Train Loss 1.2, Val Loss 1.1
Epoch 30: Train Loss 0.1, Val Loss 1.5 âŒï¼ˆè®­ç»ƒä¸‹é™ä½†éªŒè¯ä¸Šå‡ï¼‰
```

------

### 2. **Early Stopping æ˜¯å¦æå‰ç»ˆæ­¢**

- è‹¥æ¨¡å‹è‡ªåŠ¨åœæ­¢è®­ç»ƒï¼Œè¯´æ˜éªŒè¯é›†æ²¡æœ‰ç»§ç»­æå‡ï¼Œè¿™é€šå¸¸æ˜¯æ­£å¸¸çš„ï¼ˆè¡¨ç¤ºæ‰¾åˆ°äº†æœ€ä½³ç‚¹ï¼‰ï¼›
- è‹¥è®­ç»ƒæ»¡äº†å…¨éƒ¨è½®æ•°è¿˜æ²¡ early stopï¼Œè¯´æ˜å¯èƒ½æ²¡æ”¶æ•›ï¼Œä¹Ÿå¯èƒ½è¿˜éœ€è®­ç»ƒæ›´ä¹…ã€‚

------

## âœ… äºŒã€åˆ¤æ–­æ¨¡å‹æ˜¯å¦é¢„æµ‹å‡†ç¡®

ä¸»è¦ç”¨**å›å½’æŒ‡æ ‡**æ¥åˆ¤æ–­ï¼š

### 1. **MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰**

- è¡¨ç¤ºé¢„æµ‹ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³å‡ç»å¯¹å·®è·ï¼Œ**è¶Šå°è¶Šå¥½**ï¼›
- å¦‚æœä½ çš„ç›®æ ‡æ˜¯ä»·æ ¼æˆ–è¯„åˆ†ï¼Œè¿™ä¸ªå¾ˆç›´è§‚ã€‚

**ä¾‹å­**ï¼š

```
MAE = 2.5 â†’ å¹³å‡é¢„æµ‹è¯¯å·®æ˜¯ Â±2.5 å•ä½
```

------

### 2. **RÂ²ï¼ˆåˆ¤å®šç³»æ•°ï¼‰**

- è¡¡é‡æ¨¡å‹è§£é‡Šç›®æ ‡å˜é‡æ–¹å·®çš„èƒ½åŠ›ï¼ŒèŒƒå›´ä¸º **[0, 1]**ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼‰ï¼›
- RÂ² = 0ï¼šç­‰äºèƒ¡ä¹±çŒœï¼›
- RÂ² = 1ï¼šå®Œç¾é¢„æµ‹ï¼›
- RÂ² < 0ï¼šè¯´æ˜æ¨¡å‹æ¯”åŸºçº¿æ¨¡å‹è¿˜å·® âŒ

**ä¾‹å­**ï¼š

```
RÂ² = 0.92 â†’ æ¨¡å‹è§£é‡Šäº† 92% çš„æ–¹å·®ï¼Œéå¸¸å¥½
RÂ² = 0.5  â†’ æœ‰æ•ˆï¼Œä½†æ”¹è¿›ç©ºé—´å¾ˆå¤§
RÂ² < 0    â†’ æ¨¡å‹é¢„æµ‹æ¯”ç›´æ¥å–å¹³å‡å€¼è¿˜å·®
```

------

## âœ… ä¸‰ã€å¯è§†åŒ–éªŒè¯é¢„æµ‹å‡†ç¡®æ€§ï¼ˆå¼ºçƒˆæ¨èï¼‰

ä½ å¯ä»¥å°†æ¨¡å‹é¢„æµ‹ç»“æœç”»å›¾ä¸çœŸå®å€¼å¯¹æ¯”ï¼š

### ç¤ºä¾‹ä»£ç ï¼ˆæ·»åŠ åˆ°éªŒè¯é˜¶æ®µï¼‰ï¼š

```python
import matplotlib.pyplot as plt

# å–ä¸€æ‰¹éªŒè¯æ•°æ®é¢„æµ‹
X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
model.eval()
with torch.no_grad():
    y_pred = model(X_val_tensor).cpu().numpy()

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(8, 5))
plt.plot(y_val, label="çœŸå®å€¼", alpha=0.7)
plt.plot(y_pred, label="é¢„æµ‹å€¼", alpha=0.7)
plt.legend()
plt.title("é¢„æµ‹ vs çœŸå®")
plt.xlabel("æ ·æœ¬ç´¢å¼•")
plt.ylabel("ç›®æ ‡å€¼")
plt.grid(True)
plt.show()
```

ğŸ” **è‹¥é¢„æµ‹çº¿ä¸çœŸå®çº¿å½¢çŠ¶ç›¸ä¼¼ï¼Œè¯´æ˜æ¨¡å‹å­¦åˆ°äº†è¶‹åŠ¿ï¼›è‹¥åç¦»è¾ƒå¤§ï¼Œå°±éœ€è¦ä¼˜åŒ–æ¨¡å‹ç»“æ„æˆ–æ•°æ®å¤„ç†ã€‚**

------

## âœ… æ€»ç»“ï¼šè®­ç»ƒæˆåŠŸåˆ¤æ–­ Checklist

| æ£€æŸ¥é¡¹                       | æˆåŠŸåˆ¤æ–­æ ‡å‡†           |
| ---------------------------- | ---------------------- |
| è®­ç»ƒ/éªŒè¯ Loss æ˜¯å¦ä¸‹é™      | éƒ½ä¸‹é™ï¼ŒéªŒè¯ä¸åå¼¹     |
| MAE æ˜¯å¦è¶³å¤Ÿå°               | å’Œä¸šåŠ¡ç›®æ ‡è¯¯å·®è¦æ±‚ç›¸ç¬¦ |
| RÂ² æ˜¯å¦æ¥è¿‘ 1                | â‰¥ 0.8 æ˜¯ä¸é”™çš„æ¨¡å‹     |
| TensorBoard å¯è§†åŒ–æ˜¯å¦å¹³ç¨³   | Loss æ›²çº¿æ— å‰§çƒˆéœ‡è¡    |
| å¯è§†åŒ–é¢„æµ‹æ›²çº¿æ˜¯å¦æ‹ŸåˆçœŸå®å€¼ | è¶‹åŠ¿ä¸€è‡´ã€è¯¯å·®ç¨³å®š     |

------

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ å†™ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°ä¸å¯è§†åŒ–è„šæœ¬ï¼Œåªéœ€åŠ è½½æ¨¡å‹å°±èƒ½çœ‹åˆ°é¢„æµ‹å›¾å’Œè¯„åˆ†ï¼Ÿæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å°†æ¨¡å‹æ‰“åŒ…æˆ `.pt` å¹¶ç¼–å†™æ¨ç†æ¥å£ã€‚ä½ ç°åœ¨æ˜¯å¦å·²ç»æœ‰è‡ªå·±çš„æ•°æ®æˆ–æƒ³åšæ¨¡å‹éƒ¨ç½²ï¼Ÿ

ä¸‹é¢æ˜¯ä¸€ä¸ª **è‡ªåŠ¨è¯„ä¼°ä¸å¯è§†åŒ–è„šæœ¬**ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒæ¥ï¼š

âœ… åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå¦‚ `.pt` æ–‡ä»¶ï¼‰
 âœ… åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
 âœ… è‡ªåŠ¨è®¡ç®— MAEã€RÂ² ç­‰æŒ‡æ ‡
 âœ… ç”»å‡ºé¢„æµ‹å€¼ vs çœŸå®å€¼çš„å¯¹æ¯”å›¾

------

## âœ… ä¸€ã€è¯„ä¼°è„šæœ¬ `evaluate_model.py`

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, r2_score
from torch.utils.data import DataLoader, Dataset

# ä½ çš„æ¨¡å‹å®šä¹‰ï¼Œä¾‹å¦‚ DNN_V1
class DNN_V1(torch.nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.model(x)

# æ•°æ®é›†å°è£…
class RegressionDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# è¯„ä¼°å‡½æ•°
def evaluate_model(model_class, model_path, X_val, y_val, batch_size=32, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ¨¡å‹
    model = model_class(X_val.shape[1]).to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()

    val_dataset = RegressionDataset(X_val, y_val)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)

    preds, targets = [], []

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch = X_batch.to(device)
            output = model(X_batch).cpu().numpy()
            preds.append(output)
            targets.append(y_batch.numpy())

    preds = np.vstack(preds)
    targets = np.vstack(targets)

    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(targets, preds)
    r2 = r2_score(targets, preds)
    print(f"MAE: {mae:.4f}, RÂ²: {r2:.4f}")

    # å¯è§†åŒ–é¢„æµ‹ vs çœŸå®
    plt.figure(figsize=(10, 5))
    plt.plot(targets, label="çœŸå®å€¼", alpha=0.7)
    plt.plot(preds, label="é¢„æµ‹å€¼", alpha=0.7)
    plt.title("æ¨¡å‹é¢„æµ‹ç»“æœå¯¹æ¯”")
    plt.xlabel("æ ·æœ¬ç´¢å¼•")
    plt.ylabel("ç›®æ ‡å€¼")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
```

------

## âœ… äºŒã€ä½¿ç”¨ç¤ºä¾‹

ä½ å¯ä»¥åƒè¿™æ ·è°ƒç”¨å®ƒï¼ˆå‡è®¾ä½ åœ¨ `train.py` ä¸­å·²ç»è®­ç»ƒè¿‡æ¨¡å‹ï¼‰ï¼š

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from evaluate_model import evaluate_model, DNN_V1

# åŠ è½½åŸå§‹æ•°æ®
df = pd.read_csv("your_data.csv")
X = df.drop(columns=["target"]).values
y = df["target"].values

# åŒæ ·çš„é¢„å¤„ç†ï¼ˆå¾ˆé‡è¦ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# æ‹†åˆ†éªŒè¯é›†
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2)

# è·¯å¾„ä¸ºè®­ç»ƒæ—¶ä¿å­˜çš„æ¨¡å‹
evaluate_model(
    model_class=DNN_V1,
    model_path="checkpoints/DNN_V1.pt",
    X_val=X_val,
    y_val=y_val
)
```

------

## âœ… ä¸‰ã€è¾“å‡ºç¤ºæ„

```bash
MAE: 2.1347, RÂ²: 0.9183
```

ğŸ“ˆ ç„¶åä¼šå¼¹å‡ºå›¾åƒçª—å£æ˜¾ç¤ºé¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å¯¹æ¯”æ›²çº¿ã€‚

------

æ˜¯å¦éœ€è¦æˆ‘è¿›ä¸€æ­¥å°è£…æˆå‘½ä»¤è¡Œå·¥å…·ï¼ˆå¦‚ `python evaluate.py --model DNN_V1 --checkpoint path/to.pt`ï¼‰ï¼Ÿ
 æˆ–è€…ä½ å¸Œæœ›æˆ‘æ‰©å±•æ”¯æŒå¤šä¸ªæ¨¡å‹æ‰¹é‡è¯„ä¼°ï¼Ÿ
