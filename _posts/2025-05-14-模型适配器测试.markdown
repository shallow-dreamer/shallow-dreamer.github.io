---
layout:     post
title:      "æ¨¡å‹é€‚é…å™¨æµ‹è¯•"
subtitle:   ""
date:       2025-05-14
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - torch
---

ä¸ºäº†è®¾è®¡ä¸€ä¸ªé€šç”¨çš„ **æ¨¡å‹è®­ç»ƒæµç¨‹æ¡†æ¶**ï¼Œèƒ½å¤Ÿé€‚é…å¤šç§æ¨¡å‹ç±»å‹ï¼ˆå¦‚çº¿æ€§å›å½’ã€æ•°æ®åˆ†æã€å¤§è¯­è¨€æ¨¡å‹ç­‰ï¼‰ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨ **â€œé€‚é…å™¨æ¨¡å¼ï¼ˆAdapter Patternï¼‰â€** æ¥å°è£…ä¸åŒç±»å‹æ¨¡å‹çš„å·®å¼‚ï¼Œä½¿å®ƒä»¬ç»Ÿä¸€å®ç°ä¸€å¥—è®­ç»ƒå’Œè¯„ä¼°æ¥å£ï¼Œä¾¿äºè°ƒåº¦ã€ç®¡ç†ã€æ—¥å¿—è®°å½•ã€ç»“æœå±•ç¤ºç­‰ã€‚

------

## ğŸ§© ä¸€ã€æ ¸å¿ƒè®¾è®¡ç†å¿µ

- **ç»Ÿä¸€æ¥å£**ï¼šå®šä¹‰ä¸€ä¸ªæŠ½è±¡åŸºç±» `BaseModelAdapter`ï¼Œæ‰€æœ‰æ¨¡å‹é€‚é…å™¨ç»§æ‰¿å®ƒï¼Œå®ç°ç»Ÿä¸€çš„æ–¹æ³•ã€‚
- **é€‚é…å™¨ç±»**ï¼šé’ˆå¯¹ä¸åŒæ¨¡å‹ç±»å‹ï¼ˆå¦‚çº¿æ€§å›å½’ã€å¤§è¯­è¨€æ¨¡å‹ç­‰ï¼‰å®šä¹‰å¯¹åº”çš„é€‚é…å™¨ç±»ã€‚
- **è®­ç»ƒç®¡ç†å™¨**ï¼šæä¾›ç»Ÿä¸€çš„æ¨¡å‹è°ƒåº¦ã€è®­ç»ƒæ‰§è¡Œã€æ—¥å¿—æ”¶é›†ã€çŠ¶æ€ä¿å­˜ç­‰åŠŸèƒ½ã€‚
- **å¯æ‰©å±•æ€§å¼º**ï¼šæ–°å¢æ¨¡å‹ç±»å‹æ—¶ï¼Œåªéœ€æ–°å¢é€‚é…å™¨å¹¶æ³¨å†Œå³å¯ï¼Œæ— éœ€æ”¹åŠ¨ä¸»æµç¨‹ã€‚

------

## ğŸ“¦ äºŒã€ç›®å½•ç»“æ„å»ºè®®ï¼ˆDjango + Pythonï¼‰

```
ai_platform/
â”œâ”€â”€ adapters/                 # æ‰€æœ‰æ¨¡å‹é€‚é…å™¨
â”‚   â”œâ”€â”€ base.py              # BaseModelAdapter æŠ½è±¡ç±»
â”‚   â”œâ”€â”€ linear.py            # çº¿æ€§å›å½’é€‚é…å™¨
â”‚   â”œâ”€â”€ analysis.py          # æ•°æ®åˆ†æé€‚é…å™¨
â”‚   â””â”€â”€ llm.py               # å¤§è¯­è¨€æ¨¡å‹é€‚é…å™¨
â”œâ”€â”€ services/
â”‚   â””â”€â”€ trainer.py           # ç»Ÿä¸€è®­ç»ƒå™¨/ä»»åŠ¡ç®¡ç†å™¨
â”œâ”€â”€ models/
â”‚   â””â”€â”€ task.py              # ModelTrainTask ç­‰ä»»åŠ¡æ¨¡å‹
â””â”€â”€ utils/
    â””â”€â”€ logger.py            # æ—¥å¿—å·¥å…·
```

------

## ğŸ§± ä¸‰ã€æ ¸å¿ƒæ¥å£å®šä¹‰

### âœ… `BaseModelAdapter`ï¼ˆæŠ½è±¡åŸºç±»ï¼‰

```python
# adapters/base.py
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    def __init__(self, config, dataset):
        self.config = config      # ç”¨æˆ·é…ç½®å‚æ•°
        self.dataset = dataset    # æ•°æ®é›†å¯¹è±¡æˆ–è·¯å¾„

    @abstractmethod
    def preprocess(self):
        """é¢„å¤„ç†æ•°æ®"""
        pass

    @abstractmethod
    def train(self):
        """æ¨¡å‹è®­ç»ƒä¸»é€»è¾‘"""
        pass

    @abstractmethod
    def evaluate(self):
        """æ¨¡å‹è¯„ä¼°é€»è¾‘"""
        pass

    @abstractmethod
    def save_model(self, output_path):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶/æ•°æ®åº“"""
        pass
```

------

## ğŸ§  å››ã€æ¨¡å‹é€‚é…å™¨ç¤ºä¾‹

### 1ï¸âƒ£ çº¿æ€§å›å½’é€‚é…å™¨

```python
# adapters/linear.py
from .base import BaseModelAdapter
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import joblib

class LinearRegressionAdapter(BaseModelAdapter):
    def preprocess(self):
        self.X = self.dataset.drop(columns=['target'])
        self.y = self.dataset['target']

    def train(self):
        self.model = LinearRegression()
        self.model.fit(self.X, self.y)

    def evaluate(self):
        y_pred = self.model.predict(self.X)
        return {"mse": mean_squared_error(self.y, y_pred)}

    def save_model(self, output_path):
        joblib.dump(self.model, output_path)
```

------

### 2ï¸âƒ£ å¤§è¯­è¨€æ¨¡å‹é€‚é…å™¨ï¼ˆå¦‚è°ƒç”¨ OpenAI GPTï¼‰

```python
# adapters/llm.py
from .base import BaseModelAdapter
import openai

class LLMAdapter(BaseModelAdapter):
    def preprocess(self):
        self.prompt = self.config.get("prompt", "")

    def train(self):
        # é€šå¸¸å¤§è¯­è¨€æ¨¡å‹æ˜¯ zero/few-shotï¼Œæ— éœ€è®­ç»ƒï¼Œåªæ¨ç†
        self.output = openai.ChatCompletion.create(
            model=self.config.get("model", "gpt-4"),
            messages=[{"role": "user", "content": self.prompt}],
            temperature=self.config.get("temperature", 0.7)
        )

    def evaluate(self):
        return {"output": self.output["choices"][0]["message"]["content"]}

    def save_model(self, output_path):
        # LLM æ¨¡å‹æ— æ³•æœ¬åœ°ä¿å­˜ï¼Œä»…è®°å½• prompt & output
        with open(output_path, 'w') as f:
            f.write(self.output["choices"][0]["message"]["content"])
```

------

## âš™ï¸ äº”ã€è®­ç»ƒä»»åŠ¡ç®¡ç†å™¨ï¼ˆè®­ç»ƒå…¥å£ï¼‰

```python
# services/trainer.py
from adapters.linear import LinearRegressionAdapter
from adapters.llm import LLMAdapter

# æ³¨å†Œé€‚é…å™¨
ADAPTER_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "llm": LLMAdapter,
    # å¯æ‹“å±•æ›´å¤šæ¨¡å‹
}

class ModelTrainer:
    def __init__(self, task_config, dataset):
        self.task_type = task_config["task_type"]
        self.adapter_class = ADAPTER_REGISTRY.get(self.task_type)

        if not self.adapter_class:
            raise ValueError(f"Unsupported model type: {self.task_type}")

        self.adapter = self.adapter_class(task_config, dataset)

    def run(self):
        self.adapter.preprocess()
        self.adapter.train()
        results = self.adapter.evaluate()
        self.adapter.save_model(self._output_path())
        return results

    def _output_path(self):
        return f"/tmp/models/{self.task_type}_result.pkl"
```

------

## ğŸ”Œ å…­ã€ä½¿ç”¨æ–¹å¼ï¼ˆDjango è§†å›¾æˆ– Celery ä¸­è°ƒç”¨ï¼‰

```python
from services.trainer import ModelTrainer
from datasets.utils import load_dataset  # è‡ªå®šä¹‰å‡½æ•°ï¼šä»æ•°æ®åº“æˆ– S3 è½½å…¥æ•°æ®

def train_model(task_id):
    task_config = {
        "task_type": "linear_regression",
        "learning_rate": 0.01,
        "prompt": "æ€»ç»“è¿™æ®µæ–‡æœ¬...",
    }
    dataset = load_dataset(task_id)
    trainer = ModelTrainer(task_config, dataset)
    result = trainer.run()
    return result
```

------

## ğŸ§® ä¸ƒã€æ¨¡å‹é…ç½®å­—æ®µå»ºè®®

| å­—æ®µå           | å«ä¹‰                                          | æ¨¡å‹ç±»å‹   |
| ---------------- | --------------------------------------------- | ---------- |
| `task_type`      | æ¨¡å‹ä»»åŠ¡ç±»å‹ï¼ˆå¦‚ `llm`, `linear_regression`ï¼‰ | æ‰€æœ‰       |
| `learning_rate`  | å­¦ä¹ ç‡                                        | ML/DL æ¨¡å‹ |
| `prompt`         | è¾“å…¥æç¤ºæ–‡æœ¬                                  | LLM        |
| `temperature`    | æ§åˆ¶ LLM è¾“å‡ºçš„éšæœºæ€§                         | LLM        |
| `max_iterations` | æœ€å¤§è®­ç»ƒè½®æ•°                                  | ML æ¨¡å‹    |
| `target_column`  | æ ‡ç­¾åˆ—å                                      | ML æ¨¡å‹    |
| `model_name`     | é¢„è®¾æ¨¡å‹åï¼ˆå¦‚ `gpt-4`, `glm` ç­‰ï¼‰            | LLM        |

------

## âœ… å…«ã€ä¼˜åŠ¿æ€»ç»“

- **è§£è€¦å¼º**ï¼šæ¯ç§æ¨¡å‹é€‚é…å™¨ç‹¬ç«‹å®ç°ï¼Œä¸ä¼šç›¸äº’å½±å“ã€‚
- **æ˜“æ‰©å±•**ï¼šæ–°å¢æ¨¡å‹åªéœ€æ·»åŠ ä¸€ä¸ªç±»å¹¶æ³¨å†Œã€‚
- **æ¥å£ç»Ÿä¸€**ï¼šä¸»è®­ç»ƒé€»è¾‘å’Œè°ƒç”¨æ–¹å¼ä¸å˜ï¼Œä¾¿äºç»Ÿä¸€æ—¥å¿—ã€çŠ¶æ€ç®¡ç†ã€èµ„æºè°ƒåº¦ã€‚
- **é€‚åˆå¹³å°åŒ–**ï¼šéå¸¸é€‚ç”¨äº AI å¹³å°ã€AutoML ç³»ç»Ÿã€è®­ç»ƒå¹³å°æ„å»ºã€‚

------

å¦‚æœä½ æœ‰ç‰¹å®šæ¨¡å‹ã€ç‰¹å®šè¾“å…¥/è¾“å‡ºéœ€æ±‚ï¼ˆæ¯”å¦‚å›¾åƒã€å¤šåˆ†ç±»ã€æ—¶åºé¢„æµ‹ç­‰ï¼‰ï¼Œæˆ‘å¯ä»¥å¸®ä½ è¿›ä¸€æ­¥æ‰©å±•å¯¹åº”çš„é€‚é…å™¨ç±»ã€‚

éœ€è¦æˆ‘ç»§ç»­å¸®ä½ å®Œå–„æ•°æ®åº“æ¨¡å‹è®¾è®¡ã€Celery ä»»åŠ¡å°è£…ã€å‰ç«¯å¯¹æ¥å»ºè®®å—ï¼Ÿ

äº†è§£äº†ï¼Œæˆ‘å°†åŸºäºä½ çš„è¦æ±‚é‡æ–°è®¾è®¡å¹¶ç²¾ç®€è®­ç»ƒä¸é¢„æµ‹æµç¨‹ï¼š

- ä»æ•°æ®åº“ä¸­çš„æ¨¡å‹æ¨¡æ¿è¡¨åŠ è½½æŒ‡å®šæ¨¡å‹ç±»å‹çš„é»˜è®¤å‚æ•°ï¼›
- ä¸å‰ç«¯ä¼ å…¥çš„æ¨¡å‹å‚æ•°è¿›è¡Œåˆå¹¶ï¼ˆå‰ç«¯å€¼ä¼˜å…ˆï¼‰ï¼›
- ä½¿ç”¨é€‚é…å™¨æ¨¡å¼å®Œæˆæ¸…æ´—ã€é¢„å¤„ç†ã€è®­ç»ƒã€ä¿å­˜ã€å‘å¸ƒã€é¢„æµ‹çš„å…¨æµç¨‹ï¼›
- æ‰€æœ‰ç»“æ„è§£è€¦ã€æ¥å£æ¸…æ™°ã€é€»è¾‘æ˜“è¯»å¯ç»´æŠ¤ã€‚

æˆ‘ä¼šè¾“å‡ºå…³é”®æ¨¡å—ä»£ç å’Œè¯´æ˜ï¼Œç¡®ä¿ä½ èƒ½ç›´æ¥é›†æˆä½¿ç”¨ã€‚è¯·ç¨ç­‰ï¼Œæˆ‘æ•´ç†å®Œå°±å‘Šè¯‰ä½ ã€‚

# åç«¯æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹æµç¨‹è®¾è®¡

## æ¶æ„æ¦‚è¿°

æ•°æ®ç®¡é“æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–çš„æ•°æ®å¤„ç†å·¥å…·ï¼Œå®ƒå¯ä»¥ä»å¤šä¸ªæºè·å–æ•°æ®ï¼Œå¯¹å…¶è¿›è¡Œæ¸…æ´—ã€è½¬æ¢å’Œæ•´åˆï¼Œç„¶åå°†å…¶æä¾›ç»™æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åœ¨æœ¬æ–¹æ¡ˆä¸­ï¼Œæµç¨‹åŒ…æ‹¬ï¼š

- **æ•°æ®æ¸…æ´—**ï¼šåˆå¹¶å¤šä¸ª CSV æ•°æ®é›†ã€æŒ‰å› å­ç­›é€‰å­—æ®µå¹¶å»é‡ï¼›
- **æ•°æ®é¢„å¤„ç†**ï¼šå¯¹æ ‡è®°ä¸ºå½’ä¸€åŒ–çš„æ•°å€¼åˆ—è¿›è¡Œæ ‡å‡†åŒ–ï¼›
- **æ¨¡å‹è®­ç»ƒ**ï¼šæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„é€‚é…å™¨è¿›è¡Œè®­ç»ƒï¼›
- **æ¨¡å‹ä¿å­˜ä¸å‘å¸ƒ**ï¼šè®­ç»ƒç»“æŸåå°†æ¨¡å‹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œç”¨æˆ·å¯é€‰æ‹©å°†å…¶ä¸Šä¼ åˆ° S3ï¼›
- **æ¨¡å‹é¢„æµ‹**ï¼šå…¶ä»–ç”¨æˆ·æä¾›è¾“å…¥å­—æ®µï¼Œæ ¹æ®å·²å‘å¸ƒæ¨¡å‹çš„ ID å’Œç±»å‹åŠ è½½æ¨¡å‹å¹¶æ¨ç†ã€‚

## æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†æ¨¡å—

é¦–å…ˆè¿›è¡Œæ•°æ®æ¸…æ´—ï¼šè¯»å–å¹¶åˆå¹¶å‰ç«¯ä¼ å…¥çš„å¤šä¸ª CSV å­—ç¬¦ä¸²ï¼ŒæŒ‰å› å­åˆ—è¡¨ç­›é€‰æ‰€éœ€å­—æ®µï¼Œæœ€åå»é™¤é‡å¤è¡Œã€‚å®Œæˆæ•°æ®æ¸…æ´—åï¼Œå¯¹éœ€è¦å½’ä¸€åŒ–çš„æ•°å€¼åˆ—è¿›è¡Œå¤„ç†ï¼Œå®ç°åŸºæœ¬çš„æ ‡å‡†åŒ–ã€‚ä»¥ä¸‹æ˜¯ç¤ºä¾‹ä»£ç ç»“æ„ï¼š

```python
import pandas as pd
from typing import List
import io

class DataCleaner:
    @staticmethod
    def merge_datasets(csv_contents: List[str]) -> pd.DataFrame:
        """
        åˆå¹¶å¤šä¸ª CSV æ–‡æœ¬æ•°æ®ä¸ºä¸€ä¸ª DataFrame
        """
        df_list = []
        for csv in csv_contents:
            df_list.append(pd.read_csv(io.StringIO(csv)))
        df = pd.concat(df_list, ignore_index=True)
        return df

    @staticmethod
    def filter_fields(df: pd.DataFrame, field_names: List[str]) -> pd.DataFrame:
        """
        æ ¹æ®å­—æ®µåç­›é€‰åˆ—
        """
        return df[field_names]

    @staticmethod
    def drop_duplicates(df: pd.DataFrame) -> pd.DataFrame:
        """
        å»é™¤é‡å¤è¡Œ
        """
        return df.drop_duplicates()
import pandas as pd
from typing import Dict

class DataPreprocessor:
    @staticmethod
    def normalize(df: pd.DataFrame, factors: List[Dict]) -> pd.DataFrame:
        """
        å¯¹éœ€è¦å½’ä¸€åŒ–çš„æ•°å€¼åˆ—è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
        """
        for factor in factors:
            name = factor['name']
            if factor.get('normalize') and pd.api.types.is_numeric_dtype(df[name]):
                # ä½¿ç”¨å‡å€¼-æ ‡å‡†å·®æ ‡å‡†åŒ–
                df[name] = (df[name] - df[name].mean()) / df[name].std()
        return df
```

## æ¨¡å‹é€‚é…å™¨æ¥å£ä¸å®ç°

ä½¿ç”¨é€‚é…å™¨æ¨¡å¼è®¾è®¡ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ã€‚å®šä¹‰ä¸€ä¸ª `BaseModelAdapter` æŠ½è±¡ç±»ï¼Œè§„å®š `train`ã€`predict`ã€`save`ã€`load` ç­‰æ–¹æ³•ï¼Œæ‰€æœ‰æ¨¡å‹ç±»å‹çš„é€‚é…å™¨éƒ½ç»§æ‰¿è¯¥æ¥å£ã€‚ä¾‹å¦‚ï¼š

```python
import pandas as pd
from typing import Dict, Any
import joblib
from abc import ABC, abstractmethod

class BaseModelAdapter(ABC):
    @abstractmethod
    def train(self, X: pd.DataFrame, y: pd.Series, params: Dict):
        pass

    @abstractmethod
    def predict(self, X: pd.DataFrame) -> Any:
        pass

    def save(self, filepath: str):
        """
        å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶
        """
        joblib.dump(self.model, filepath)

    @classmethod
    def load(cls, filepath: str):
        """
        ä»æ–‡ä»¶åŠ è½½æ¨¡å‹
        """
        adapter = cls()
        adapter.model = joblib.load(filepath)
        return adapter
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor

class LinearRegressionAdapter(BaseModelAdapter):
    def __init__(self):
        self.model = LinearRegression()

    def train(self, X, y, params):
        # åˆå§‹åŒ–æ¨¡å‹æ—¶åº”ç”¨å‚æ•°ï¼ˆå¦‚æ­£åˆ™åŒ–ç³»æ•°ç­‰ï¼‰
        self.model = LinearRegression(**params)
        self.model.fit(X, y)

    def predict(self, X):
        return self.model.predict(X)

class XGBoostAdapter(BaseModelAdapter):
    def __init__(self):
        self.model = XGBRegressor()

    def train(self, X, y, params):
        self.model = XGBRegressor(**params)
        self.model.fit(X, y)

    def predict(self, X):
        return self.model.predict(X)

# è‹¥æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ï¼Œå¯å®ç°å¯¹åº”é€‚é…å™¨
class LLMAdapter(BaseModelAdapter):
    def __init__(self):
        self.model = None  # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼‰

    def train(self, X, y, params):
        # å¯¹äº LLM å¯æ‰§è¡Œå¾®è°ƒæˆ–å…¶å®ƒè®­ç»ƒé€»è¾‘
        pass

    def predict(self, X):
        # ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†
        return self.model.generate(X)
```

## è®­ç»ƒè°ƒåº¦ä¸å‚æ•°ç®¡ç†

è®­ç»ƒæ¨¡å—è´Ÿè´£åè°ƒæ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚é¦–å…ˆï¼Œæ ¹æ®å‰ç«¯ä¼ å…¥çš„æ¨¡å‹ç±»å‹ï¼Œä»æ¨¡æ¿è¡¨åŠ è½½é»˜è®¤å‚æ•°ï¼Œç„¶åä¸å‰ç«¯ç»™å®šçš„å‚æ•°åˆå¹¶ï¼ˆå‰ç«¯ä¼˜å…ˆï¼‰ã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
class ModelTemplateRepo:
    """
    æ¨¡å‹æ¨¡æ¿è¡¨ï¼Œå­˜å‚¨æ¯ç§æ¨¡å‹ç±»å‹çš„é»˜è®¤å‚æ•°
    """
    TEMPLATE = {
        "linear_regression": {"fit_intercept": True, "normalize": False},
        "xgboost": {"max_depth": 5, "learning_rate": 0.1},
        # å¯ä»¥æ·»åŠ å…¶ä»–æ¨¡å‹ç±»å‹åŠå…¶é»˜è®¤å‚æ•°
    }

    @staticmethod
    def get_default_params(model_type: str) -> Dict:
        return ModelTemplateRepo.TEMPLATE.get(model_type, {})

# é€‚é…å™¨æ³¨å†Œè¡¨ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è·å–å¯¹åº”çš„ Adapter ç±»
ADAPTER_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "xgboost": XGBoostAdapter,
    "llm": LLMAdapter
}

class TrainingScheduler:
    def __init__(self):
        pass

    def train_model(self, factors: List[Dict], model_type: str,
                    user_params: Dict, csv_contents: List[str]) -> str:
        # æ•°æ®æ¸…æ´—
        df = DataCleaner.merge_datasets(csv_contents)
        df = DataCleaner.filter_fields(df, [f['name'] for f in factors])
        df = DataCleaner.drop_duplicates(df)
        # æ•°æ®é¢„å¤„ç†
        df = DataPreprocessor.normalize(df, factors)
        # å‡è®¾æœ€åä¸€ä¸ªå­—æ®µä¸ºç›®æ ‡å˜é‡
        X = df[[f['name'] for f in factors if f['name'] != factors[-1]['name']]]
        y = df[factors[-1]['name']]
        # å‚æ•°åˆå¹¶ï¼ˆæ¨¡æ¿é»˜è®¤å‚æ•° + ç”¨æˆ·å‚æ•°ï¼‰
        default_params = ModelTemplateRepo.get_default_params(model_type)
        params = {**default_params, **user_params}
        # è®­ç»ƒæ¨¡å‹
        adapter_cls = ADAPTER_REGISTRY.get(model_type)
        adapter = adapter_cls()
        adapter.train(X, y, params)
        # ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°
        model_path = f"{model_type}_model.pkl"
        adapter.save(model_path)
        return model_path  # è¿”å›æ¨¡å‹æ–‡ä»¶è·¯å¾„æˆ– ID
```

## æ¨¡å‹ä¿å­˜ä¸å‘å¸ƒ

æ¨¡å‹è®­ç»ƒç»“æŸåï¼Œæ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨æœ¬åœ°ã€‚ç”¨æˆ·ç¡®è®¤åï¼Œå¯å°†æ¨¡å‹ä¸Šä¼ è‡³ S3 ç­‰å¤–éƒ¨å­˜å‚¨ï¼Œä¾¿äºåç»­è°ƒç”¨ã€‚ä¾‹å¦‚ï¼š

```python
import boto3

class ModelRegistry:
    @staticmethod
    def upload_to_s3(local_path: str, bucket_name: str, model_id: str):
        """
        å°†æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸Šä¼ åˆ° S3
        """
        s3 = boto3.client('s3')
        s3.upload_file(local_path, bucket_name, f"{model_id}.pkl")
```

## æ¨¡å‹é¢„æµ‹æ¨¡å—

é¢„æµ‹æ¨¡å—æ ¹æ®ä¼ å…¥çš„å·²å‘å¸ƒæ¨¡å‹ ID å’Œç±»å‹åŠ è½½æ¨¡å‹ï¼Œå¹¶å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ï¼š

```python
from pandas import DataFrame

class Predictor:
    def __init__(self, adapter_registry):
        self.adapter_registry = adapter_registry

    def predict(self, model_type: str, model_path: str, input_data: DataFrame):
        """
        åŠ è½½æŒ‡å®šæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
        """
        adapter_cls = self.adapter_registry.get(model_type)
        adapter = adapter_cls.load(model_path)
        return adapter.predict(input_data)

# ä½¿ç”¨ç¤ºä¾‹
adapter_registry = ADAPTER_REGISTRY
predictor = Predictor(adapter_registry)
# å…¶ä»–ç”¨æˆ·ä¼ å…¥çš„é¢„æµ‹æ•°æ®ç¤ºä¾‹
# input_df = pd.DataFrame([...])
# preds = predictor.predict("linear_regression", "linear_regression_model.pkl", input_df)
```

ä»¥ä¸Šä»£ç ç»“æ„å®ç°äº†ä»æ•°æ®æ¸…æ´—åˆ°æ¨¡å‹è®­ç»ƒå†åˆ°é¢„æµ‹çš„å®Œæ•´æµç¨‹ã€‚å„æ¨¡å—è®¾è®¡é€šç”¨ç®€æ´ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤šæ¨¡å‹ç±»å‹å’ŒåŠŸèƒ½ã€‚

**å‚è€ƒæ–‡çŒ®ï¼š** æ•°æ®ç®¡é“æ ¸å¿ƒæ¦‚å¿µï¼›å¤šæºæ•°æ®åˆå¹¶å»é‡ï¼›é€‚é…å™¨æ¨¡å¼ä»‹ç»ã€‚

æ˜ç™½äº†ã€‚æˆ‘å°†ä¸ºä½ è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„åç«¯æ¡†æ¶ï¼Œä½¿ç”¨é€‚é…å™¨æ¨¡å¼æ¥æ”¯æŒå¤šç§æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹æµç¨‹ï¼ŒåŒ…å«æ•°æ®æ¸…æ´—ã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹å‘å¸ƒï¼ˆS3ï¼‰ã€é¢„æµ‹å‚æ•°ä¼ é€’ç­‰åŠŸèƒ½ï¼Œç¡®ä¿ç»“æ„è§£è€¦ã€æ¸…æ™°å¯æ‰©å±•ã€‚

æˆ‘ä¼šåœ¨å®Œæˆåæä¾›å„ä¸ªæ¨¡å—çš„ä»£ç ç»“æ„ä¸é€»è¾‘è¯´æ˜ï¼Œä¾›ä½ ç›´æ¥é›†æˆã€‚è¯·ç¨ç­‰ï¼Œæˆ‘é©¬ä¸Šæ•´ç†å®Œæ¯•ã€‚

# åç«¯æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹æ¡†æ¶è®¾è®¡

## ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

- **æ•°æ®å¤„ç†æ¨¡å—**ï¼šè´Ÿè´£åˆå¹¶å¤šä¸ªæ•°æ®é›†ï¼ˆCSV å†…å®¹ï¼‰ã€æŒ‰å› å­å‚æ•°é€‰å–ç‰¹å¾åˆ—ã€å»é‡ï¼Œå¹¶æ ¹æ®é…ç½®å¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ç­‰é¢„å¤„ç†ã€‚
- **æ¨¡å‹é€‚é…å™¨æ¥å£**ï¼šå®šä¹‰ç»Ÿä¸€çš„è®­ç»ƒä¸é¢„æµ‹æ¥å£ï¼ˆ`train`ã€`predict` ç­‰æ–¹æ³•ï¼‰ï¼Œå„æ¨¡å‹ç±»å‹é€šè¿‡å®ç°è¯¥æ¥å£é€‚é…å™¨æ¥è§£è€¦å…·ä½“ç®—æ³•ã€‚
- **ç¤ºä¾‹é€‚é…å™¨**ï¼šé’ˆå¯¹çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€XGBoostã€é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ç­‰ä¸åŒæ¨¡å‹ï¼Œå®ç°å…·ä½“çš„é€‚é…å™¨ç±»ï¼Œè´Ÿè´£è°ƒç”¨ç›¸åº”æ¡†æ¶è®­ç»ƒ/æ¨ç†å¹¶ä¿å­˜æ¨¡å‹ã€‚
- **è®­ç»ƒè°ƒåº¦å™¨**ï¼šè´Ÿè´£æ¥æ”¶å‰ç«¯è¯·æ±‚ï¼ˆå› å­å‚æ•°ã€æ¨¡å‹å‚æ•°ã€æ•°æ®æ–‡ä»¶å†…å®¹ï¼‰ï¼Œä¾æ¬¡è°ƒç”¨æ•°æ®å¤„ç†ã€é€‰æ‹©é€‚é…å™¨è®­ç»ƒï¼Œå¹¶è°ƒç”¨æ¨¡å‹æ³¨å†Œä¸å‘å¸ƒé€»è¾‘è¿›è¡Œä¿å­˜å’Œæ ‡è®°ã€‚
- **æ¨¡å‹æ³¨å†Œä¸å‘å¸ƒæ¨¡å—**ï¼šç®¡ç†è®­ç»ƒåçš„æ¨¡å‹å…ƒæ•°æ®ï¼ˆå¦‚æ¨¡å‹ IDã€æœ¬åœ°è·¯å¾„ã€çŠ¶æ€ï¼‰ï¼Œåœ¨ç”¨æˆ·ç¡®è®¤å‘å¸ƒåé€šè¿‡ boto3 ä¸Šä¼ è‡³ S3ï¼Œå¹¶æ›´æ–°æ¨¡å‹å‘å¸ƒçŠ¶æ€ã€‚
- **é¢„æµ‹æœåŠ¡**ï¼šå…è®¸å…¶ä»–ç”¨æˆ·è°ƒç”¨å·²å‘å¸ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ŒåŠ¨æ€æ¥æ”¶è¾“å…¥å‚æ•°ï¼ŒåŠ è½½æ¨¡å‹å¹¶è¾“å‡ºé¢„æµ‹ç»“æœã€‚

å„æ¨¡å—é€šè¿‡æ¥å£è§£è€¦ï¼Œä¾‹å¦‚è®­ç»ƒè°ƒåº¦å™¨åªä¾èµ–äºæ•°æ®å¤„ç†å’Œé€‚é…å™¨æ¥å£ï¼Œä¸ç›´æ¥ä¾èµ–å…·ä½“æ¨¡å‹åº“ï¼›æ–°å¢æ¨¡å‹ç±»å‹æ—¶ï¼Œåªéœ€æ–°å¢å¯¹åº”çš„é€‚é…å™¨ç±»å¹¶æ³¨å†Œåˆ°å·¥å‚å³å¯ï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒæµç¨‹ã€‚

## æ¨¡å‹é€‚é…å™¨æ¥å£

é‡‡ç”¨**é€‚é…å™¨æ¨¡å¼**ä¸ºå„æ¨¡å‹ç±»å‹å®šä¹‰ç»Ÿä¸€æ¥å£ï¼Œä»¥ä¾¿è®­ç»ƒè°ƒåº¦å™¨å’Œé¢„æµ‹æœåŠ¡å¯ä»¥é€æ˜è°ƒç”¨ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªæŠ½è±¡åŸºç±» `ModelAdapter`ï¼Œå®šä¹‰ç»Ÿä¸€çš„è®­ç»ƒã€é¢„æµ‹ã€æ¨¡å‹ä¿å­˜/åŠ è½½æ¥å£ï¼š

```python
from abc import ABC, abstractmethod

class ModelAdapter(ABC):
    """
    æ¨¡å‹é€‚é…å™¨åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€çš„è®­ç»ƒå’Œé¢„æµ‹æ¥å£ã€‚
    """

    def __init__(self, model_params: dict):
        """
        åˆå§‹åŒ–é€‚é…å™¨æ—¶ï¼Œå¯ä¼ å…¥æ¨¡å‹é…ç½®å‚æ•°ã€‚
        """
        self.model_params = model_params
        self.model = None  # åœ¨è®­ç»ƒåä¿å­˜æ¨¡å‹å®ä¾‹

    @abstractmethod
    def train(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹ã€‚X ä¸ºç‰¹å¾æ•°æ®é›†ï¼ˆDataFrame æˆ– ndarrayï¼‰ï¼Œy ä¸ºç›®æ ‡æ ‡ç­¾ã€‚
        """
        pass

    @abstractmethod
    def predict(self, X):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚X ä¸ºè¾“å…¥ç‰¹å¾ï¼Œå¯ä¸ºå•æ¡æˆ–å¤šæ¡æ•°æ®ã€‚
        è¿”å›é¢„æµ‹ç»“æœã€‚
        """
        pass

    @abstractmethod
    def save_model(self, file_path: str):
        """
        å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„ã€‚
        """
        pass

    @abstractmethod
    def load_model(self, file_path: str):
        """
        ä»æŒ‡å®šè·¯å¾„åŠ è½½æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ—¶ä½¿ç”¨ã€‚
        """
        pass
```

- **è§£è€¦è®¾è®¡**ï¼šè®­ç»ƒè°ƒåº¦å™¨å’Œé¢„æµ‹æœåŠ¡é€šè¿‡è¯¥æ¥å£ä¸å…·ä½“æ¨¡å‹é€‚é…å™¨äº¤äº’ï¼Œä¸éœ€è¦çŸ¥é“æ¨¡å‹ç»†èŠ‚ã€‚æ–°å¢æ¨¡å‹ç±»å‹æ—¶ï¼Œåªéœ€å®ç°è¯¥æ¥å£å¹¶æ³¨å†Œï¼Œæ— éœ€ä¿®æ”¹å·²æœ‰ä»£ç ã€‚
- **åŠ¨æ€æ‰©å±•**ï¼šå¯é€šè¿‡å·¥å‚æ¨¡å¼æˆ–æ³¨å†Œè¡¨ç»´æŠ¤æ¨¡å‹ç±»å‹ä¸é€‚é…å™¨ç±»çš„æ˜ å°„ï¼Œå¦‚ `"linear_regression" -> LinearRegressionAdapter`ï¼Œæ–¹ä¾¿åŠ¨æ€é€‰æ‹©é€‚é…å™¨ã€‚
- **ç»Ÿä¸€è°ƒç”¨**ï¼šç»Ÿä¸€çš„ `train/predict` æ¥å£ï¼Œå‚æ•°ä¸ºç‰¹å¾æ•°æ®å’Œè¾“å…¥å€¼ï¼Œæ”¯æŒåŠ¨æ€å‚æ•°è¾“å…¥ã€‚

### ç¤ºä¾‹é€‚é…å™¨

ä»¥ä¸‹æä¾›ä¸¤ä¸ªç¤ºä¾‹é€‚é…å™¨ï¼Œä¸€ä¸ªç”¨äºçº¿æ€§å›å½’ï¼Œä¸€ä¸ªç”¨äºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥å±•ç¤ºä¸åŒæ¨¡å‹ç±»å‹çš„é€‚é…å™¨å®ç°æ€è·¯ã€‚

- **çº¿æ€§å›å½’é€‚é…å™¨**ï¼ˆä½¿ç”¨ scikit-learnï¼‰ï¼š

```python
from sklearn.linear_model import LinearRegression
import joblib

class LinearRegressionAdapter(ModelAdapter):
    def train(self, X, y):
        self.model = LinearRegression(**self.model_params.get('params', {}))
        self.model.fit(X, y)
        return self.model

    def predict(self, X):
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–åŠ è½½")
        return self.model.predict(X)

    def save_model(self, file_path: str):
        if self.model is None:
            raise RuntimeError("æ— æ¨¡å‹å¯ä¿å­˜")
        joblib.dump(self.model, file_path)

    def load_model(self, file_path: str):
        self.model = joblib.load(file_path)
        return self.model
```

- **LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰é€‚é…å™¨**ï¼ˆä½¿ç”¨ Hugging Face Transformersï¼‰ï¼š

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
import torch

class LLMAdapter(ModelAdapter):
    def train(self, train_texts, train_labels):
        """
        ç®€è¦ç¤ºä¾‹ï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸‹ï¼Œç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
        å®é™…å®ç°å¯ä½¿ç”¨ Trainer æˆ–è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚
        """
        model_name = self.model_params.get('model_name', 'bert-base-uncased')
        num_labels = self.model_params.get('num_labels', 2)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        # è¿™é‡Œå¯è¿›è¡Œå¾®è°ƒï¼ˆä»£ç ç®€ç•¥ï¼Œå®é™…å¯ç”¨ Trainerï¼‰
        self.tokenizer = tokenizer
        self.model = model
        return model

    def predict(self, texts):
        """
        ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»é¢„æµ‹ã€‚texts å¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒæˆ–åŠ è½½")
        # ä½¿ç”¨pipelineç®€åŒ–æ¨ç†
        classifier = pipeline("text-classification", model=self.model, tokenizer=self.tokenizer)
        results = classifier(texts)
        return results

    def save_model(self, dir_path: str):
        """
        ä¿å­˜æ•´ä¸ªæ¨¡å‹åŠtokenizeråˆ°æ–‡ä»¶å¤¹ã€‚
        """
        if self.model is None:
            raise RuntimeError("æ— æ¨¡å‹å¯ä¿å­˜")
        self.model.save_pretrained(dir_path)
        self.tokenizer.save_pretrained(dir_path)

    def load_model(self, dir_path: str):
        """
        ä»æ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹åŠtokenizerã€‚
        """
        model_name = self.model_params.get('model_name', 'bert-base-uncased')
        self.tokenizer = AutoTokenizer.from_pretrained(dir_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(dir_path)
        return self.model
```

è¿™ä¸¤ä¸ªç¤ºä¾‹å±•ç¤ºäº†ï¼š`LinearRegressionAdapter` è´Ÿè´£è°ƒç”¨ sklearn æ¥å£è®­ç»ƒã€é¢„æµ‹ï¼Œå¹¶ç”¨ `joblib` ä¿å­˜æ¨¡å‹ï¼›`LLMAdapter` åˆ™ä½¿ç”¨ Hugging Face è¿›è¡Œæ–‡æœ¬åˆ†ç±»ç¤ºä¾‹ï¼Œå±•ç¤ºå¯¹å¾®è°ƒå’Œæ¨ç†çš„åŒ…è£…ã€‚å…¶å®ƒæ¨¡å‹ç±»å‹ï¼ˆå¦‚é€»è¾‘å›å½’ã€XGBoost ç­‰ï¼‰ä¹Ÿå¯ç±»ä¼¼å®ç°å„è‡ªçš„é€‚é…å™¨ç±»ï¼Œå¤ç”¨ç›¸åŒçš„ç»Ÿä¸€æ¥å£ã€‚

## æ•°æ®å¤„ç†æ¨¡å—

æ•°æ®å¤„ç†æ¨¡å—è´Ÿè´£ä»å‰ç«¯æ¥æ”¶çš„æ•°æ®æ–‡ä»¶å†…å®¹å¼€å§‹ï¼Œè¿›è¡Œæ¸…æ´—å’Œé¢„å¤„ç†ï¼Œæœ€ç»ˆç”Ÿæˆå¯ä¾›æ¨¡å‹è®­ç»ƒçš„ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾ã€‚è®¾è®¡ä¸­å°†å…¶è§£è€¦ä¸ºå•ç‹¬çš„ç±»ï¼Œæ¯”å¦‚ `DataProcessor`ã€‚ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

1. **æ•°æ®æ¸…æ´—**ï¼šåˆå¹¶å‰ç«¯ä¼ æ¥çš„å¤šä¸ª CSV æ–‡ä»¶å†…å®¹ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ï¼Œæ ¹æ®å› å­åˆ—è¡¨æå–å¯¹åº”åˆ—ï¼Œå¹¶å»é™¤é‡å¤è¡Œã€‚
2. **å½’ä¸€åŒ–ç­‰é¢„å¤„ç†**ï¼šéå†å› å­é…ç½®ï¼Œå¦‚æœæŸåˆ—æ ‡è®°éœ€è¦å½’ä¸€åŒ–ä¸”ä¸ºæ•°å€¼ç±»å‹ï¼Œåˆ™å¯¹è¯¥åˆ—è¿›è¡Œ Min-Max å½’ä¸€åŒ–æˆ–å…¶ä»–æ–¹å¼è½¬æ¢ã€‚

ä¸‹é¢ç»™å‡ºä¸€ä¸ªç¤ºä¾‹å®ç°ï¼š

```python
import pandas as pd
from io import StringIO
from sklearn.preprocessing import MinMaxScaler

class DataProcessor:
    def clean(self, file_contents: list, factors: list) -> pd.DataFrame:
        """
        åˆå¹¶å¤šä¸ª CSV å†…å®¹ï¼Œæå–å› å­ç›¸å…³çš„åˆ—ï¼Œå¹¶å»é™¤é‡å¤å€¼ã€‚
        - file_contents: åŒ…å«å¤šä¸ª CSV æ ¼å¼å­—ç¬¦ä¸²çš„åˆ—è¡¨
        - factors: å› å­é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå› å­åŒ…å« 'name' (åˆ—å) å’Œå…¶ä»–å±æ€§
        """
        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV å†…å®¹
        dfs = [pd.read_csv(StringIO(content)) for content in file_contents]
        df = pd.concat(dfs, ignore_index=True)

        # æå–å› å­æŒ‡å®šçš„åˆ—
        selected_cols = [f['name'] for f in factors]
        df = df.loc[:, selected_cols]

        # å»é™¤å®Œå…¨é‡å¤çš„è¡Œ
        df = df.drop_duplicates().reset_index(drop=True)
        return df

    def preprocess(self, df: pd.DataFrame, factors: list) -> pd.DataFrame:
        """
        å¯¹æ•°å€¼åˆ—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆå¦‚é…ç½®äº† normalize=Trueï¼‰ã€‚
        - df: ä¸Šä¸€æ­¥æ¸…æ´—åçš„ DataFrame
        - factors: å› å­é…ç½®ï¼ŒåŒ…å«ç±»å‹å’Œæ˜¯å¦å½’ä¸€åŒ–çš„æ ‡å¿—
        """
        for f in factors:
            col = f['name']
            if f.get('type') == 'numeric' and f.get('normalize', False):
                scaler = MinMaxScaler()
                df[col] = scaler.fit_transform(df[[col]])
        return df
```

- **è§£è€¦è®¾è®¡**ï¼š`TrainingScheduler` ä¼šè°ƒç”¨ `DataProcessor`ï¼Œè€Œä¸ä¼šå…³å¿ƒå…·ä½“çš„æ¸…æ´—é€»è¾‘ç»†èŠ‚ã€‚è‹¥éœ€è¦å¢åŠ æ–°é¢„å¤„ç†æ–¹æ³•ï¼ˆå¦‚æ ‡å‡†åŒ–ã€ç¼ºå¤±å€¼å¡«å……ç­‰ï¼‰ï¼Œåªè¦åœ¨ `DataProcessor` ä¸­æ‰©å±•å³å¯ï¼Œä¸å½±å“å…¶ä»–æ¨¡å—ã€‚
- **æ•°æ®è¯»å–**ï¼šä½¿ç”¨ `StringIO` è¯»å– CSV å†…å®¹å­—ç¬¦ä¸²ï¼Œå®ç°â€œæ–‡ä»¶å†…å®¹â€è€Œéè·¯å¾„ä¼ é€’çš„è¯»å–æ–¹å¼ã€‚
- **å»é‡å’Œç­›é€‰**ï¼šç›´æ¥ä½¿ç”¨ `pandas` çš„ `drop_duplicates()` ä»¥åŠåˆ—é€‰æ‹©åŠŸèƒ½å®ç°ã€‚

## è®­ç»ƒè°ƒåº¦å™¨

è®­ç»ƒè°ƒåº¦å™¨ï¼ˆ`TrainingScheduler`ï¼‰è´Ÿè´£åè°ƒæ•´ä¸ªè®­ç»ƒæµç¨‹ï¼Œå°†å‰ç«¯ä¼ å…¥çš„å‚æ•°å’Œæ•°æ®åˆ†å‘ç»™å„å­æ¨¡å—ï¼Œå¹¶ç®¡ç†æ¨¡å‹è®­ç»ƒä¸ä¿å­˜çš„æµç¨‹ã€‚ä¸»è¦é€»è¾‘åŒ…æ‹¬ï¼š

- æ¥æ”¶å‰ç«¯å‚æ•°ï¼šå› å­é…ç½®åˆ—è¡¨ `factor_params`ã€æ¨¡å‹é…ç½® `model_params`ã€ä»¥åŠä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®æ–‡ä»¶å†…å®¹ `data_contents`ã€‚
- è°ƒç”¨æ•°æ®å¤„ç†æ¨¡å—ï¼šå…ˆè¿›è¡Œæ•°æ®æ¸…æ´— (`clean`)ï¼Œå†è¿›è¡Œé¢„å¤„ç† (`preprocess`)ã€‚
- é€‰æ‹©æ¨¡å‹é€‚é…å™¨ï¼šæ ¹æ® `model_params['type']`ï¼Œä»å·¥å‚æˆ–æ³¨å†Œè¡¨ä¸­è·å–å¯¹åº”çš„é€‚é…å™¨å®ä¾‹ã€‚
- è°ƒç”¨é€‚é…å™¨è®­ç»ƒï¼šä¼ å…¥ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°ä¸´æ—¶è·¯å¾„ã€‚
- æ³¨å†Œæ¨¡å‹å…ƒæ•°æ®ï¼šå°†æ¨¡å‹ä¿å­˜è·¯å¾„ã€å‚æ•°ç­‰ä¿¡æ¯æ³¨å†Œåˆ°æ¨¡å‹æ³¨å†Œè¡¨ï¼Œå¹¶ç”Ÿæˆæ¨¡å‹ ID ã€‚
- ç­‰å¾…å‘å¸ƒï¼šè®­ç»ƒå®Œæˆåæ¨¡å‹å…ˆå¤„äºâ€œå·²è®­ç»ƒæœªå‘å¸ƒâ€çŠ¶æ€ï¼Œå¾…ç”¨æˆ·ç¡®è®¤åä¸Šä¼ åˆ° S3ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹å®ç°ï¼š

```python
import uuid
import os

class TrainingScheduler:
    def __init__(self, data_processor, adapter_factory, model_registry):
        self.data_processor = data_processor
        self.adapter_factory = adapter_factory
        self.model_registry = model_registry

    def schedule(self, factor_params: list, model_params: dict, data_contents: list) -> str:
        """
        è°ƒåº¦è®­ç»ƒæµç¨‹ï¼Œè¿”å›æ¨¡å‹IDï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰ã€‚
        """
        # 1. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
        df_clean = self.data_processor.clean(data_contents, factor_params)
        df_processed = self.data_processor.preprocess(df_clean, factor_params)

        # æå–ç‰¹å¾çŸ©é˜µXå’Œæ ‡ç­¾y
        target_col = model_params['target']  # å‡è®¾æ¨¡å‹å‚æ•°é‡ŒæŒ‡å®šäº†ç›®æ ‡åˆ—
        feature_cols = [f['name'] for f in factor_params if f['name'] != target_col]
        X = df_processed[feature_cols]
        y = df_processed[target_col]

        # 2. è·å–å¯¹åº”çš„æ¨¡å‹é€‚é…å™¨å¹¶è®­ç»ƒ
        model_type = model_params['type']
        adapter = self.adapter_factory.get_adapter(model_type, model_params)
        adapter.train(X, y)

        # 3. æ¨¡å‹ä¿å­˜ï¼ˆæœ¬åœ°ï¼‰
        model_id = str(uuid.uuid4())
        local_dir = f"/tmp/model_{model_id}"
        os.makedirs(local_dir, exist_ok=True)
        adapter.save_model(local_dir)  # ä¿å­˜æ¨¡å‹æˆ–æ¨¡å‹æ–‡ä»¶å¤¹

        # 4. æ³¨å†Œæ¨¡å‹ä¿¡æ¯
        self.model_registry.register(model_id, {
            'type': model_type,
            'local_path': local_dir,
            'params': model_params,
            'published': False
        })
        return model_id

    def publish_model(self, model_id: str):
        """
        ç”¨æˆ·ç¡®è®¤å‘å¸ƒåè°ƒç”¨æ­¤æ–¹æ³•ï¼Œå°†æ¨¡å‹ä¸Šä¼ åˆ° S3 å¹¶æ›´æ–°çŠ¶æ€ã€‚
        """
        record = self.model_registry.get(model_id)
        if not record:
            raise ValueError("æ¨¡å‹IDä¸å­˜åœ¨")
        if record['published']:
            return  # å·²å‘å¸ƒæ— éœ€é‡å¤
        local_path = record['local_path']
        # æ„é€  S3 keyï¼Œå¯æ ¹æ®éœ€æ±‚ä½¿ç”¨æ¨¡å‹IDç­‰ä¿¡æ¯å‘½å
        s3_key = f"models/{model_id}.zip"
        uploader = S3Uploader(bucket_name="your-bucket-name")
        # å‡è®¾æ¨¡å‹ä¿å­˜ä¸ºä¸€ä¸ªç›®å½•ï¼Œè¿™é‡Œå…ˆå‹ç¼©ä¸ºzipä¸Šä¼ ï¼Œç¤ºä¾‹ä»£ç 
        zip_path = f"{local_path}.zip"
        os.system(f"zip -r {zip_path} {local_path}")
        uploader.upload_file(zip_path, s3_key)
        # æ›´æ–°å‘å¸ƒçŠ¶æ€
        record['published'] = True
        record['s3_key'] = s3_key
        self.model_registry.update(model_id, record)
```

- **æ¨¡å‹ID**ï¼šç”Ÿæˆå”¯ä¸€ `model_id`ï¼ˆå¦‚ä½¿ç”¨ `uuid`ï¼‰ï¼Œä¾¿äºåç»­å¼•ç”¨å’ŒçŠ¶æ€ç®¡ç†ã€‚
- **æœ¬åœ°ä¿å­˜**ï¼šå…ˆå°†æ¨¡å‹ä¿å­˜åˆ°åç«¯æœ¬åœ°è·¯å¾„ï¼ˆæˆ–ä¸´æ—¶ç›®å½•ï¼‰ï¼Œå†åœ¨å‘å¸ƒæ—¶ç»Ÿä¸€ä¸Šä¼ ã€‚
- **å¾…å‘å¸ƒçŠ¶æ€**ï¼šè®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹åœ¨æ³¨å†Œè¡¨ä¸­æ ‡è®°ä¸ºæœªå‘å¸ƒçŠ¶æ€ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤å†è§¦å‘ S3 ä¸Šä¼ ã€‚
- **S3 ä¸Šä¼ **ï¼šç¤ºä¾‹ä¸­ç”¨ `S3Uploader`ï¼ˆè§ä¸‹æ–‡ï¼‰è¿›è¡Œä¸Šä¼ ï¼Œè¿™é‡Œå‡è®¾æ¨¡å‹å…ˆå‹ç¼©åä¸Šä¼ åˆ°æŒ‡å®š bucketã€‚å®é™…å¯æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„æ‰“åŒ…æ–¹å¼ã€‚

## æ¨¡å‹æ³¨å†Œä¸å‘å¸ƒ

`ModelRegistry` è´Ÿè´£ç»´æŠ¤æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬çŠ¶æ€ï¼ˆå·²è®­ç»ƒ/å·²å‘å¸ƒï¼‰ã€æœ¬åœ°è·¯å¾„ã€S3 è·¯å¾„ç­‰ä¿¡æ¯ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„æ³¨å†Œè¡¨è®¾è®¡ï¼š

```python
class ModelRegistry:
    def __init__(self):
        # å­˜å‚¨æ¨¡å‹å…ƒæ•°æ®ï¼Œkey ä¸º model_id
        self._store = {}

    def register(self, model_id: str, metadata: dict):
        """
        æ³¨å†Œæ–°æ¨¡å‹ä¿¡æ¯ã€‚
        """
        self._store[model_id] = metadata

    def update(self, model_id: str, metadata: dict):
        """
        æ›´æ–°æ¨¡å‹å…ƒæ•°æ®ï¼ˆå¦‚å‘å¸ƒçŠ¶æ€ã€S3è·¯å¾„ç­‰ï¼‰ã€‚
        """
        if model_id in self._store:
            self._store[model_id].update(metadata)
        else:
            raise KeyError("æ¨¡å‹IDæœªæ‰¾åˆ°")

    def get(self, model_id: str) -> dict:
        """
        è·å–æ¨¡å‹å…ƒæ•°æ®ã€‚
        """
        return self._store.get(model_id)
```

- **å…ƒæ•°æ®ç®¡ç†**ï¼š`metadata` åŒ…å«äº†æ¨¡å‹ç±»å‹ã€å‚æ•°ã€æœ¬åœ°å­˜å‚¨è·¯å¾„ã€æ˜¯å¦å‘å¸ƒã€S3 å­˜å‚¨é”®ç­‰ä¿¡æ¯ã€‚å®é™…ç³»ç»Ÿä¸­å¯ä»¥ç»“åˆæ•°æ®åº“æˆ–æŒä¹…åŒ–å­˜å‚¨ï¼Œæ­¤å¤„ç®€åŒ–ä¸ºå†…å­˜å­—å…¸ã€‚
- **å‘å¸ƒé€»è¾‘**ï¼šå½“è°ƒç”¨ `TrainingScheduler.publish_model(model_id)` æ—¶ï¼Œä¼šä¸Šä¼ æ¨¡å‹è‡³ S3ï¼Œå¹¶æ›´æ–° `metadata['published'] = True` åŠç›¸åº”çš„ S3 å­˜å‚¨è·¯å¾„ã€‚è¿™æ ·é¢„æµ‹æœåŠ¡å¯ä¾æ® `published` çŠ¶æ€å†³å®šæ˜¯å¦å…è®¸é¢„æµ‹è°ƒç”¨ã€‚
- **åç»­æ‰©å±•**ï¼šå¯å¢åŠ ç‰ˆæœ¬æ§åˆ¶ã€è®¿é—®æƒé™ã€æ¨¡å‹æè¿°ç­‰å­—æ®µï¼Œä½¿æ¨¡å‹æ³¨å†Œè¡¨æ›´åŠ å®Œå–„ã€‚

## S3 ä¸Šä¼ å·¥å…·

ä½¿ç”¨ `boto3` å®¢æˆ·ç«¯å®ç°æ¨¡å‹æ–‡ä»¶ä¸Šä¼ åˆ° S3ã€‚ç¤ºä¾‹å·¥å…·ç±»å¦‚ä¸‹ï¼š

```python
import boto3

class S3Uploader:
    def __init__(self, bucket_name: str):
        self.s3 = boto3.client('s3')
        self.bucket = bucket_name

    def upload_file(self, file_path: str, key: str):
        """
        ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ°æŒ‡å®š S3 bucket çš„ key è·¯å¾„ã€‚
        """
        self.s3.upload_file(file_path, self.bucket, key)
```

- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œéœ€è¦é…ç½® AWS çš„è®¿é—®å¯†é’¥æˆ–ä½¿ç”¨ IAM è§’è‰²è¿›è¡Œè®¤è¯ã€‚
- `file_path` æ”¯æŒæœ¬åœ°è·¯å¾„ï¼Œ`key` å®šä¹‰äº†å­˜å‚¨åœ¨ S3 ä¸Šçš„æ–‡ä»¶åï¼ˆå¯åŒ…å«ç›®å½•ï¼‰ã€‚
- ä¸Šä¼ å®Œæˆåï¼Œæ³¨å†Œè¡¨ä¸­ä¿å­˜å¯¹åº”çš„ S3 è·¯å¾„ä¿¡æ¯ï¼Œä»¥ä¾¿åç»­åŠ è½½ä½¿ç”¨ã€‚

## æ¨¡å‹é¢„æµ‹æœåŠ¡

é¢„æµ‹æœåŠ¡å…è®¸å…¶ä»–ç”¨æˆ·æŒ‡å®šå·²å‘å¸ƒçš„æ¨¡å‹ ID å¹¶è¾“å…¥ç‰¹å¾è¿›è¡Œé¢„æµ‹ã€‚ä¸»è¦æ­¥éª¤ï¼š

1. **åŠ è½½æ¨¡å‹**ï¼šæ ¹æ®æ¨¡å‹ ID ä»æ³¨å†Œè¡¨è·å–æ¨¡å‹å…ƒæ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦å·²å‘å¸ƒã€‚è‹¥æ¨¡å‹æ–‡ä»¶å°šåœ¨æœ¬åœ°ï¼Œå¯ç›´æ¥ `load_model`ï¼›è‹¥ä»…åœ¨ S3 ä¸Šï¼Œåˆ™å…ˆä¸‹è½½ï¼ˆæˆ–é€šè¿‡ `load_model` æ”¯æŒç›´æ¥S3 URLï¼‰ã€‚
2. **åŠ¨æ€è¾“å…¥**ï¼šæ¥æ”¶å‰ç«¯ä¼ å…¥çš„é¢„æµ‹å‚æ•°ï¼Œé€šå¸¸ä¸ºé”®å€¼å¯¹å½¢å¼çš„ç‰¹å¾å€¼ã€‚åœ¨ä½¿ç”¨æ—¶éœ€è¦å°†å…¶è½¬æ¢ä¸ºæ¨¡å‹é€‚é…å™¨èƒ½æ¥å—çš„æ ¼å¼ï¼ˆå¦‚ DataFrame æˆ–çŸ©é˜µï¼‰ã€‚
3. **è°ƒç”¨é¢„æµ‹**ï¼šä½¿ç”¨é€‚é…å™¨çš„ `predict` æ–¹æ³•è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¿”å›ç»“æœç»™å‰ç«¯ã€‚

ç¤ºä¾‹å®ç°å¦‚ä¸‹ï¼š

```python
class PredictionService:
    def __init__(self, adapter_factory, model_registry):
        self.adapter_factory = adapter_factory
        self.model_registry = model_registry

    def predict(self, model_id: str, input_data: dict):
        """
        ä½¿ç”¨å·²å‘å¸ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚
        - model_id: æ³¨å†Œè¡¨ä¸­çš„æ¨¡å‹ID
        - input_data: åŒ…å«ç‰¹å¾åç§°å’Œå€¼çš„å­—å…¸ï¼ˆåŠ¨æ€å‚æ•°è¾“å…¥ï¼‰
        """
        record = self.model_registry.get(model_id)
        if not record or not record.get('published', False):
            raise ValueError("æ¨¡å‹æœªå‘å¸ƒæˆ–ä¸å­˜åœ¨")

        model_type = record['type']
        model_params = record['params']
        # è·å–å¯¹åº”é€‚é…å™¨
        adapter = self.adapter_factory.get_adapter(model_type, model_params)

        # è‹¥éœ€è¦ï¼Œä» S3 ä¸‹è½½æ¨¡å‹ï¼Œè¿™é‡Œå‡è®¾æœ¬åœ°å·²æœ‰æ¨¡å‹ç›®å½•ï¼Œå¦åˆ™å®ç°ä¸‹è½½é€»è¾‘
        local_path = record['local_path']
        adapter.load_model(local_path)

        # å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸º DataFrame (æˆ–é€‚é…å™¨æ”¯æŒçš„æ ¼å¼)
        import pandas as pd
        input_df = pd.DataFrame([input_data])

        # é¢„æµ‹
        predictions = adapter.predict(input_df)
        return predictions
```

- **åŠ¨æ€å‚æ•°**ï¼šå‰ç«¯å¯ä¼ å…¥ä»»æ„éœ€è¦çš„ç‰¹å¾å­—æ®µå’Œå€¼ï¼ŒæœåŠ¡ç«¯å°†å…¶åŒ…è£…ä¸ºå•è¡Œçš„ `DataFrame` è¿›è¡Œé¢„æµ‹ã€‚è¿™æ · `predict` æ¥å£æ— éœ€å›ºå®šè¾“å…¥ç»´åº¦ã€‚
- **æ¨¡å‹åŠ è½½**ï¼šå‡è®¾æ¨¡å‹å·²ç»ä¸Šä¼ å¹¶ä¸‹è½½è‡³æœ¬åœ°ï¼ˆæˆ–æœ¬åœ°ä¿ç•™è®­ç»ƒç»“æœï¼‰ï¼Œ`load_model` è´Ÿè´£åŠ è½½æ¨¡å‹å®ä¾‹ã€‚è‹¥æ¨¡å‹ä»…åœ¨ S3ï¼Œå¯åœ¨æ­¤å¤„åŠ ä¸‹è½½ä»£ç ï¼ˆå¦‚ `boto3.download_file`ï¼‰åå†åŠ è½½ã€‚
- **è¿”å›ç»“æœ**ï¼šå°†æ¨¡å‹è¾“å‡ºç›´æ¥è¿”å›ç»™å‰ç«¯ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡å¯èƒ½éœ€è¦æ˜ å°„ç±»åˆ«æ ‡ç­¾ï¼Œå›å½’åˆ™è¿”å›æ•°å€¼ã€‚

## ä»£ç è°ƒç”¨ç¤ºä¾‹

ä¸‹é¢ç»™å‡ºä¸€ä¸ªç®€å•çš„æµç¨‹ç¤ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•ä½¿ç”¨ä¸Šè¿°ç»„ä»¶å®Œæˆè®­ç»ƒå’Œé¢„æµ‹ï¼š

```python
# å‡è®¾å‰ç«¯ä¼ é€’ï¼š
factor_params = [
    {'name': 'age', 'type': 'numeric', 'normalize': True},
    {'name': 'salary', 'type': 'numeric', 'normalize': True},
    {'name': 'gender', 'type': 'categorical', 'normalize': False},
    {'name': 'income', 'type': 'numeric', 'normalize': False},  # ç›®æ ‡åˆ—
]
model_params = {
    'type': 'linear_regression',
    'target': 'income',
    'params': {}  # çº¿æ€§å›å½’è¶…å‚æ•°
}
data_contents = [csv_string1, csv_string2]  # CSVæ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²åˆ—è¡¨

# åˆå§‹åŒ–ç»„ä»¶
data_processor = DataProcessor()
adapter_factory = ModelAdapterFactory()  # å‡è®¾å·¥å‚å·²å®ç°ç±»å‹->é€‚é…å™¨æ˜ å°„
model_registry = ModelRegistry()
scheduler = TrainingScheduler(data_processor, adapter_factory, model_registry)

# è°ƒåº¦è®­ç»ƒ
model_id = scheduler.schedule(factor_params, model_params, data_contents)
print(f"æ¨¡å‹å·²è®­ç»ƒå®Œæˆï¼ŒID = {model_id}ï¼Œç­‰å¾…å‘å¸ƒã€‚")

# ç”¨æˆ·ç¡®è®¤å‘å¸ƒåè°ƒç”¨
scheduler.publish_model(model_id)
print("æ¨¡å‹å·²å‘å¸ƒè‡³ S3ã€‚")

# é¢„æµ‹ç¤ºä¾‹
prediction_service = PredictionService(adapter_factory, model_registry)
input_features = {'age': 30, 'salary': 60000, 'gender': 'Male'}
result = prediction_service.predict(model_id, input_features)
print("é¢„æµ‹ç»“æœï¼š", result)
```

ä»¥ä¸Šç¤ºä¾‹æ¼”ç¤ºäº†æ•´ä¸ªæµç¨‹ï¼šå‰ç«¯ä¼ å…¥æ•°æ®åï¼Œ**è®­ç»ƒè°ƒåº¦å™¨**å®Œæˆæ•°æ®å¤„ç†å’Œæ¨¡å‹è®­ç»ƒï¼›è®­ç»ƒå®Œæˆåï¼Œç®¡ç†å‘˜ç¡®è®¤å‘å¸ƒæ¨¡å‹ï¼›å…¶ä»–ç”¨æˆ·å†è°ƒç”¨**é¢„æµ‹æœåŠ¡**ï¼Œé€šè¿‡æ¨¡å‹ ID å’ŒåŠ¨æ€è¾“å…¥å‚æ•°è·å–é¢„æµ‹ç»“æœã€‚æ•´ä¸ªè¿‡ç¨‹ä¸­çš„å„æ¨¡å—èŒè´£æ¸…æ™°ï¼Œä½¿ç”¨**é€‚é…å™¨æ¨¡å¼**ä¿è¯äº†å¯¹ä¸åŒæ¨¡å‹ç±»å‹çš„ä¸€è‡´è°ƒç”¨æ¥å£ï¼Œå¹¶é€šè¿‡å·¥å‚/æ³¨å†Œè¡¨æ”¯æŒçµæ´»æ‰©å±•æ–°çš„æ¨¡å‹ç±»å‹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ª**é«˜åº¦è§£è€¦ã€å¯æ‰©å±•**çš„**AI æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹é€šç”¨æµç¨‹**å®ç°ï¼Œé‡‡ç”¨**é€‚é…å™¨æ¨¡å¼**æ¥æ”¯æŒå¤šç§æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ã€XGBoostã€å¤§è¯­è¨€æ¨¡å‹ç­‰ï¼‰ï¼Œå¹¶å›´ç»•ä½ çš„éœ€æ±‚ï¼ˆæ•°æ®æ¸…æ´—ã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€ä¿å­˜ã€å‘å¸ƒã€é¢„æµ‹ï¼‰è¿›è¡Œæ¨¡å—åŒ–æ‹†åˆ†ã€‚

------

## ğŸ§© 1. ç›®å½•ç»“æ„å»ºè®®

```
ai_platform/
â”œâ”€â”€ adapters/                 # å„ç±»æ¨¡å‹é€‚é…å™¨
â”‚   â”œâ”€â”€ base.py              # æ¨¡å‹é€‚é…å™¨æ¥å£
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â””â”€â”€ llm_model.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ cleaner.py           # æ•°æ®æ¸…æ´—
â”‚   â”œâ”€â”€ preprocessor.py      # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ scheduler.py         # è®­ç»ƒè°ƒåº¦å™¨
â”‚   â”œâ”€â”€ registry.py          # æ¨¡å‹ä¿å­˜å’Œå‘å¸ƒ
â”‚   â””â”€â”€ predictor.py         # é¢„æµ‹æ¨¡å—
â”œâ”€â”€ config/
â”‚   â””â”€â”€ registry.py          # æ¨¡å‹ç±»å‹æ³¨å†Œè¡¨
â””â”€â”€ main.py                  # è®­ç»ƒ/é¢„æµ‹å…¥å£
```

------

## ğŸ§  2. é€‚é…å™¨æ¥å£å®šä¹‰ï¼ˆ`adapters/base.py`ï¼‰

```python
from abc import ABC, abstractmethod
import pandas as pd
from typing import Dict, Any

class BaseModelAdapter(ABC):
    @abstractmethod
    def train(self, X: pd.DataFrame, y: pd.Series, params: Dict[str, Any]):
        pass

    @abstractmethod
    def predict(self, X: pd.DataFrame) -> Any:
        pass

    @abstractmethod
    def save(self, path: str):
        pass

    @abstractmethod
    def load(self, path: str):
        pass
```

------

## ğŸ“ˆ 3. ç¤ºä¾‹æ¨¡å‹é€‚é…å™¨ï¼šçº¿æ€§å›å½’ï¼ˆ`adapters/linear_regression.py`ï¼‰

```python
from sklearn.linear_model import LinearRegression
from .base import BaseModelAdapter
import joblib

class LinearRegressionAdapter(BaseModelAdapter):
    def __init__(self):
        self.model = None

    def train(self, X, y, params):
        self.model = LinearRegression(**params)
        self.model.fit(X, y)

    def predict(self, X):
        return self.model.predict(X)

    def save(self, path):
        joblib.dump(self.model, path)

    def load(self, path):
        self.model = joblib.load(path)
```

å…¶ä»–æ¨¡å‹å¦‚ XGBoost å’Œ LLM åŒç†ã€‚

------

## ğŸ§¼ 4. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†æ¨¡å—ï¼ˆ`core/cleaner.py`, `core/preprocessor.py`ï¼‰

```python
# cleaner.py
import pandas as pd
import io
from typing import List

class DataCleaner:
    @staticmethod
    def clean(csv_contents: List[str], factor_names: List[str]) -> pd.DataFrame:
        dfs = [pd.read_csv(io.StringIO(c)) for c in csv_contents]
        df = pd.concat(dfs).drop_duplicates()
        return df[factor_names]
# preprocessor.py
class DataPreprocessor:
    @staticmethod
    def normalize(df: pd.DataFrame, factors: List[Dict]):
        for factor in factors:
            name = factor["name"]
            if factor.get("normalize") and pd.api.types.is_numeric_dtype(df[name]):
                df[name] = (df[name] - df[name].mean()) / df[name].std()
        return df
```

------

## ğŸ§  5. æ¨¡å‹è°ƒåº¦å™¨ï¼šè®­ç»ƒæ‰§è¡Œå™¨ï¼ˆ`core/scheduler.py`ï¼‰

```python
from config.registry import ADAPTER_REGISTRY
from .cleaner import DataCleaner
from .preprocessor import DataPreprocessor
import pandas as pd
import os

class TrainingScheduler:
    def train(self, model_type: str, factors: List[Dict], model_params: Dict, csv_contents: List[str]) -> str:
        # 1. æ¸…æ´—æ•°æ®
        field_names = [f["name"] for f in factors]
        df = DataCleaner.clean(csv_contents, field_names)

        # 2. é¢„å¤„ç†
        df = DataPreprocessor.normalize(df, factors)

        # 3. ç”Ÿæˆ X/y
        target = field_names[-1]
        X = df.drop(columns=[target])
        y = df[target]

        # 4. è·å–é€‚é…å™¨
        adapter_cls = ADAPTER_REGISTRY[model_type]
        model = adapter_cls()
        model.train(X, y, model_params)

        # 5. ä¿å­˜æ¨¡å‹
        path = f"models/{model_type}_{hash(tuple(field_names))}.pkl"
        os.makedirs("models", exist_ok=True)
        model.save(path)

        return path
```

------

## â˜ï¸ 6. å‘å¸ƒæ¨¡å—ï¼ˆ`core/registry.py`ï¼‰

```python
import boto3

class ModelPublisher:
    def upload_to_s3(self, local_path: str, model_key: str, bucket: str):
        s3 = boto3.client("s3")
        s3.upload_file(local_path, bucket, model_key)
```

------

## ğŸ” 7. æ¨ç†æ¨¡å—ï¼ˆ`core/predictor.py`ï¼‰

```python
from config.registry import ADAPTER_REGISTRY

class Predictor:
    def predict(self, model_type: str, model_path: str, input_df):
        adapter_cls = ADAPTER_REGISTRY[model_type]
        model = adapter_cls()
        model.load(model_path)
        return model.predict(input_df)
```

------

## ğŸ§© 8. æ¨¡å‹æ³¨å†Œè¡¨ï¼ˆ`config/registry.py`ï¼‰

```python
from adapters.linear_regression import LinearRegressionAdapter
from adapters.xgboost_model import XGBoostAdapter
from adapters.llm_model import LLMAdapter

ADAPTER_REGISTRY = {
    "linear_regression": LinearRegressionAdapter,
    "xgboost": XGBoostAdapter,
    "llm": LLMAdapter
}
```

------

## ğŸš€ 9. ä¸»è°ƒç”¨æµç¨‹ï¼ˆ`main.py` ç¤ºä¾‹ï¼‰

```python
from core.scheduler import TrainingScheduler
from core.predictor import Predictor

# æ¨¡æ‹Ÿå‰ç«¯è¾“å…¥
factors = [
    {"name": "feature1", "normalize": True},
    {"name": "feature2", "normalize": False},
    {"name": "target", "normalize": False}
]
model_type = "linear_regression"
model_params = {"fit_intercept": True}
csvs = [open("data1.csv").read(), open("data2.csv").read()]

# è®­ç»ƒæµç¨‹
scheduler = TrainingScheduler()
model_path = scheduler.train(model_type, factors, model_params, csvs)

# æ¨ç†æµç¨‹
predictor = Predictor()
import pandas as pd
input_data = pd.DataFrame([{"feature1": 1.2, "feature2": 3.4}])
pred = predictor.predict(model_type, model_path, input_data)
print(pred)
```

------

## âœ… æ€»ç»“ï¼šæ ¸å¿ƒè§£è€¦ç‚¹

| æ¨¡å—                               | åŠŸèƒ½         | è§£è€¦è¯´æ˜                   |
| ---------------------------------- | ------------ | -------------------------- |
| `BaseModelAdapter`                 | æ¨¡å‹æ ‡å‡†æ¥å£ | é€‚é…ä¸åŒæ¨¡å‹è®­ç»ƒä¸æ¨ç†é€»è¾‘ |
| `TrainingScheduler`                | è°ƒåº¦è®­ç»ƒæµç¨‹ | ç‹¬ç«‹äºå…·ä½“æ¨¡å‹ä¸æ•°æ®æ ¼å¼   |
| `DataCleaner` + `DataPreprocessor` | æ•°æ®å¤„ç†æ¨¡å— | ä¸ä¾èµ–æ¨¡å‹é€»è¾‘ï¼ŒæŒ‰å› å­é©±åŠ¨ |
| `ADAPTER_REGISTRY`                 | é€‚é…å™¨æ³¨å†Œè¡¨ | æ¨¡å‹æ‰©å±•é›¶ä¾µå…¥             |
| `Predictor`                        | æ¨ç†ç»Ÿä¸€å…¥å£ | å°è£…æ¨¡å‹åŠ è½½å’Œæ¨ç†é€»è¾‘     |

------

å¦‚ä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥è¡¥å……ï¼š

- FastAPI æ¥å£å°è£…ï¼›
- S3 æ¥å£æœ¬åœ° mock æµ‹è¯•ï¼›
- å¯¹æ¥ä»»åŠ¡é˜Ÿåˆ—ï¼ˆå¦‚ Celeryï¼‰æ‰§è¡Œå¼‚æ­¥è®­ç»ƒã€‚

æ˜¯å¦éœ€è¦æˆ‘ç»§ç»­å°è£…è¿™äº›æ¨¡å—ï¼Ÿ

ä»¥ä¸‹æ˜¯å®Œæ•´çš„å¤šæ•°æ®é›†æ¸…æ´—æ¨¡å— `MultiDatasetProcessor` çš„å®ç°ã€è¯´æ˜å’Œæµ‹è¯•ç¤ºä¾‹ã€‚

------

## âœ… æ¨¡å—åŠŸèƒ½æ¦‚è§ˆ

`MultiDatasetProcessor` ä¸»è¦å®ç°ä»¥ä¸‹æ­¥éª¤ï¼š

1. **åŠ è½½å¤šä¸ªæ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒ CSV å’Œ Excelï¼‰**
2. **åˆå¹¶æ•°æ®é›†å¹¶å»é™¤é‡å¤å€¼**
3. **æ ¹æ®â€œå› å­é…ç½®â€è‡ªåŠ¨æ¸…æ´—å’Œå­—æ®µæ˜ å°„**
4. **æ¨æ–­å­—æ®µç±»å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«æ•°å€¼/å­—ç¬¦å‹ï¼‰**
5. **æ”¯æŒæ•°å€¼å½’ä¸€åŒ–**
6. **è®­ç»ƒé›†/æµ‹è¯•é›†æŒ‰æ¯”ä¾‹åˆ’åˆ†**

------

## âœ… ä½¿ç”¨æ–¹å¼

### ç¤ºä¾‹æ•°æ®ï¼š`dataset1.csv`, `dataset2.xlsx`

#### ç¤ºä¾‹å†…å®¹ï¼ˆå­—æ®µä¸€è‡´ï¼‰ï¼š

| å¹´é¾„ | æ€§åˆ« | å·¥èµ„  |
| ---- | ---- | ----- |
| 25   | ç”·   | 7000  |
| 32   | å¥³   | 8000  |
| 45   | ç”·   | 10000 |

### ç¤ºä¾‹å› å­é…ç½®

```python
factor_config = {
    "features": [
        {"source": "å¹´é¾„", "alias": "age", "min": 18, "max": 65},
        {"source": "æ€§åˆ«", "alias": "gender", "enum": ["ç”·", "å¥³"]},
        {"source": "å·¥èµ„", "alias": "salary"}
    ],
    "normalize": True  # å½’ä¸€åŒ–æ‰€æœ‰æ•°å€¼å‹å­—æ®µ
}
```

------

## âœ… æµ‹è¯•ä»£ç ç¤ºä¾‹

```python
from processor import MultiDatasetProcessor

filepaths = ["dataset1.csv", "dataset2.xlsx"]

# å®ä¾‹åŒ–å¤„ç†å™¨
processor = MultiDatasetProcessor(filepaths=filepaths, factor_config=factor_config)

# æ‰§è¡Œæ¸…æ´—
cleaned_df = processor.clean()
print("æ¸…æ´—åçš„æ•°æ®ï¼š")
print(cleaned_df.head())

# æ•°æ®æ‹†åˆ†
X_train, X_test, y_train, y_test = processor.split(target_column="salary")

print("\nè®­ç»ƒç‰¹å¾é›†:")
print(X_train.head())

print("\næµ‹è¯•ç›®æ ‡é›†:")
print(y_test.head())
```

------

## âœ… æ¨èçš„å°è£…æ–¹å¼ï¼ˆå¯é€‰ï¼‰

ä½ å¯ä»¥å°†è¿™ä¸ªæ¨¡å—æ‰“åŒ…ä¸ºï¼š

- Django app çš„å·¥å…·æ¨¡å—ï¼ˆæ”¾å…¥ `utils/` æˆ– `services/` ä¸­ï¼‰
- ç‹¬ç«‹ pip åŒ…ç”¨äºå¤šé¡¹ç›®å¤ç”¨
- æ­é… Celery åå°ä»»åŠ¡å¼‚æ­¥æ‰§è¡Œ
- æä¾› API æ¥å£ï¼ˆå¦‚ä¸Šä¼ æ•°æ®åæ¸…æ´—å’Œè¿”å›ç‰ˆæœ¬ IDï¼‰

------

å¦‚æœä½ éœ€è¦æˆ‘å¸®ä½ è¿›ä¸€æ­¥å°è£…ä¸º API æˆ–ä»»åŠ¡é˜Ÿåˆ—ä»»åŠ¡ï¼Œæˆ–è€…å®ç°æ—¥å¿—è®°å½•ç­‰åŠŸèƒ½ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ è¡¥å……åç«¯ç»“æ„ä¸æ¥å£è®¾è®¡ã€‚

éå¸¸å¥½ï¼Œè¿™ç§ç»„åˆæ–¹å¼åœ¨ AI å¹³å°ä¸­éå¸¸å¸¸è§ï¼š

- ç”¨ `openpyxl` è¯»å– **Excel æ–‡ä»¶**ï¼ˆå°¤å…¶æ”¯æŒ `.xlsx`ï¼Œé¿å… `pandas.read_excel` åœ¨éƒ¨åˆ†æ ¼å¼æˆ–åˆå¹¶å•å…ƒæ ¼ä¸‹è¡¨ç°ä¸ä½³ï¼‰
- ç„¶åä½¿ç”¨ `pandas` è¿›è¡Œ **é«˜æ•ˆçš„æ•°æ®æ¸…æ´—ä¸åˆ†æ**

------

## âœ… ä½¿ç”¨ `openpyxl` + `pandas` è¯»å– Excel å¹¶æ¸…æ´—æ•°æ®çš„æ¨èæµç¨‹

### ğŸ“¦ å®‰è£…ä¾èµ–ï¼ˆå¦‚æœªå®‰è£…ï¼‰

```bash
pip install openpyxl pandas
```

------

## ğŸ§  ç¤ºä¾‹ï¼šè¯»å– Excelã€æ¸…æ´—å¹¶å½’ä¸€åŒ–å­—æ®µ

```python
import openpyxl
import pandas as pd
from sklearn.preprocessing import StandardScaler


def read_excel_with_openpyxl(path: str, sheet_name: str = None) -> pd.DataFrame:
    """
    ä½¿ç”¨ openpyxl è¯»å– Excel çš„æŒ‡å®š sheetï¼Œç„¶åè½¬æ¢ä¸º pandas DataFrame
    :param path: Excel æ–‡ä»¶è·¯å¾„
    :param sheet_name: è¡¨åï¼Œé»˜è®¤è¯»å–ç¬¬ä¸€ä¸ª
    :return: DataFrame
    """
    wb = openpyxl.load_workbook(path, data_only=True)
    sheet = wb[sheet_name] if sheet_name else wb.active

    # è¯»å–è¡¨å¤´
    headers = [cell.value for cell in next(sheet.iter_rows(min_row=1, max_row=1))]

    # è¯»å–æ•°æ®
    data = []
    for row in sheet.iter_rows(min_row=2, values_only=True):
        data.append(row)

    df = pd.DataFrame(data, columns=headers)
    return df


def clean_and_normalize(df: pd.DataFrame, factor_config: dict) -> pd.DataFrame:
    """
    æ ¹æ®å› å­é…ç½®æ¸…æ´—å­—æ®µå¹¶å½’ä¸€åŒ–ï¼ˆä»…æ•°å€¼å‹ï¼‰
    :param df: åŸå§‹ DataFrame
    :param factor_config: å› å­é…ç½®
    :return: æ¸…æ´—åçš„ DataFrame
    """
    result = {}
    scaler = StandardScaler()

    for feature in factor_config["features"]:
        source = feature["source"]
        alias = feature["alias"]
        col = df[source].copy()

        # å­—æ®µç±»å‹æ¨æ–­
        if pd.api.types.is_numeric_dtype(col):
            col = col.astype(float)

            # å¤„ç† min / max / enum è§„åˆ™
            if "min" in feature:
                col = col[col >= feature["min"]]
            if "max" in feature:
                col = col[col <= feature["max"]]
            if "enum" in feature:
                col = col[col.isin(feature["enum"])]
        else:
            if "enum" in feature:
                col = col[col.isin(feature["enum"])]

        result[alias] = col

    cleaned_df = pd.DataFrame(result).dropna()

    # å¦‚æœå¼€å¯å½’ä¸€åŒ–ï¼Œåªå¯¹æ•°å€¼å‹å­—æ®µè¿›è¡Œ
    if factor_config.get("normalize"):
        numeric_cols = [f["alias"] for f in factor_config["features"]
                        if pd.api.types.is_numeric_dtype(cleaned_df[f["alias"]])]
        cleaned_df[numeric_cols] = scaler.fit_transform(cleaned_df[numeric_cols])

    return cleaned_df
```

------

### âœ… ç¤ºä¾‹å› å­é…ç½® & ä½¿ç”¨

```python
factor_config = {
    "features": [
        {"source": "å¹´é¾„", "alias": "age", "min": 18, "max": 100},
        {"source": "æ€§åˆ«", "alias": "gender", "enum": ["ç”·", "å¥³"]}
    ],
    "normalize": True
}

df_raw = read_excel_with_openpyxl("æ•°æ®æ ·æœ¬.xlsx")
df_cleaned = clean_and_normalize(df_raw, factor_config)
```

------

## ğŸ” æ•°æ®åç»­å¤„ç†ï¼ˆå¯ç»§ç»­è¿›è¡Œè®­ç»ƒ/åˆ†å‰²ï¼‰

```python
from sklearn.model_selection import train_test_split

X = df_cleaned.drop(columns=["target_column"])
y = df_cleaned["target_column"]
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)
```

------

## âœ… ä¼˜åŠ¿æ€»ç»“

| æ­¥éª¤             | å·¥å…·                                   | ä¼˜ç‚¹                           |
| ---------------- | -------------------------------------- | ------------------------------ |
| è¯»å– Excel       | `openpyxl`                             | æ›´å¥½æ§åˆ¶å•å…ƒæ ¼ã€å…¬å¼ã€åˆå¹¶æ ¼å¼ |
| è½¬æ¢ä¸º DataFrame | `pandas`                               | é«˜æ•ˆå¤„ç†ã€æ¸…æ´—ã€åˆ†æ           |
| ç±»å‹å¤„ç†         | `pandas` + ç±»å‹æ¨æ–­                    | è‡ªåŠ¨é€‚é…å­—ç¬¦/æ•°å€¼/ç©ºå€¼         |
| å½’ä¸€åŒ–           | `sklearn.preprocessing.StandardScaler` | ç»Ÿä¸€å½’ä¸€åŒ–æµç¨‹                 |

------

å¦‚æœä½ å¸Œæœ›æŠŠè¿™éƒ¨åˆ†åšæˆä¸€ä¸ª**ç»Ÿä¸€å°è£…æ¨¡å—**ã€æˆ–æä¾›**æ”¯æŒ API è°ƒç”¨æˆ–ä»»åŠ¡é˜Ÿåˆ—é›†æˆ**çš„å½¢å¼ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ æŠ½è±¡æˆæœåŠ¡æˆ–ç‹¬ç«‹åŒ…ã€‚æ˜¯å¦éœ€è¦ï¼Ÿ

ä¸‹é¢æ˜¯ä¸€ä¸ªè®¾è®¡è‰¯å¥½ã€æ³¨é‡Šæ¸…æ™°ã€**æ”¯æŒå­—æ®µç±»å‹è‡ªåŠ¨æ¨æ–­**ã€å¯æ‰“åŒ…æˆç‹¬ç«‹æœåŠ¡çš„ **æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†æ¨¡å—**ï¼Œå¯åœ¨ä½ çš„ AI å¹³å°ä¸­å¤ç”¨ã€‚

------

## âœ… åŠŸèƒ½æ¦‚è§ˆ

- æ¥æ”¶å¤šä¸ªæ•°æ®é›†ï¼ˆCSV/`DataFrame`ï¼‰ï¼Œåˆå¹¶å»é‡ï¼›
- ä½¿ç”¨å› å­é…ç½®è¿›è¡Œå­—æ®µæ˜ å°„ã€æ¸…æ´—ï¼ˆåŒ…æ‹¬æ•°å€¼èŒƒå›´ã€æšä¸¾ç­‰ï¼‰ï¼›
- è‡ªåŠ¨æ¨æ–­å­—æ®µç±»å‹ï¼ˆå­—ç¬¦å‹ / æ•°å€¼å‹ï¼‰ï¼›
- å¯é€‰å½’ä¸€åŒ–ï¼›
- è®­ç»ƒé›† / æµ‹è¯•é›†åˆ’åˆ†ï¼›
- å¯æ‰“åŒ…ä¸ºç‹¬ç«‹æ¨¡å—ï¼Œä¾¿äºå¤ç”¨ã€‚

------

## ğŸ—‚ï¸ ç›®å½•ç»“æ„å»ºè®®ï¼ˆå¯æ”¾å…¥å¹³å°å·¥å…·åŒ…ï¼‰

```
your_project/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_cleaner.py  # â† æœ¬æ¨¡å—
```

------

## ğŸ§  `data_cleaner.py` ä»£ç å®ç°ï¼ˆå«è¯¦ç»†æ³¨é‡Šï¼‰

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from pandas.api.types import is_numeric_dtype, is_string_dtype
from typing import List, Dict, Tuple, Union, Optional


class DataCleaner:
    """
    æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†æ¨¡å—ï¼Œå¯è‡ªåŠ¨æ¨æ–­å­—æ®µç±»å‹ï¼Œå¹¶æ‰§è¡Œæ¸…æ´—ã€å½’ä¸€åŒ–å’Œæ•°æ®åˆ†å‰²ã€‚
    """

    def __init__(self, factor_config: Dict, train_ratio: float = 0.8):
        """
        :param factor_config: å› å­é…ç½®ï¼Œå®šä¹‰ç‰¹å¾ã€ç›®æ ‡å­—æ®µã€å½’ä¸€åŒ–è§„åˆ™ç­‰
        :param train_ratio: è®­ç»ƒé›†å æ¯” (é»˜è®¤ 80%)
        """
        self.factor_config = factor_config
        self.train_ratio = train_ratio
        self.scaler = StandardScaler() if factor_config.get("normalize") else None

    def _load_csvs(self, csv_contents: List[Union[str, pd.DataFrame]]) -> pd.DataFrame:
        """
        è¯»å– CSV å­—ç¬¦ä¸²æˆ– DataFrame åˆ—è¡¨ï¼Œåˆå¹¶å¹¶å»é‡
        """
        dfs = []
        for content in csv_contents:
            if isinstance(content, str):
                df = pd.read_csv(pd.compat.StringIO(content))
            elif isinstance(content, pd.DataFrame):
                df = content.copy()
            else:
                raise ValueError("CSV å†…å®¹å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ– DataFrame")
            dfs.append(df)

        merged = pd.concat(dfs, ignore_index=True).drop_duplicates()
        return merged

    def _infer_type(self, series: pd.Series) -> str:
        """
        æ¨æ–­å­—æ®µç±»å‹ï¼šè¿”å› 'numeric' æˆ– 'categorical'
        """
        if is_numeric_dtype(series):
            return "numeric"
        elif is_string_dtype(series):
            return "categorical"
        else:
            # å¯æ‹“å±•æ”¯æŒ datetime ç­‰
            return "categorical"

    def _clean_feature(self, df: pd.DataFrame, feature: Dict) -> pd.Series:
        """
        æ˜ å°„ã€æ¸…æ´—å•ä¸ªç‰¹å¾å­—æ®µï¼Œæ ¹æ®ç±»å‹æ¨æ–­æˆ–é…ç½®æ‰§è¡Œå€¼æ ¡éªŒ
        """
        source_col = feature["source"]
        alias_col = feature["alias"]
        col = df[source_col].copy()

        # æ¨æ–­å­—æ®µç±»å‹æˆ–è¯»å–é…ç½®
        dtype = feature.get("type") or self._infer_type(col)

        # æ•°å€¼å‹æ ¡éªŒ
        if dtype == "numeric":
            if "min" in feature:
                col = col[col >= feature["min"]]
            if "max" in feature:
                col = col[col <= feature["max"]]
            if "enum" in feature:
                col = col[col.isin(feature["enum"])]

        # æšä¸¾å‹æ ¡éªŒ
        elif dtype == "categorical":
            if "enum" in feature:
                col = col[col.isin(feature["enum"])]

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {dtype}")

        return col.rename(alias_col)

    def _clean_target(self, df: pd.DataFrame) -> pd.Series:
        """
        æ¸…æ´—ç›®æ ‡å­—æ®µï¼ˆä¸€èˆ¬æ— éœ€æšä¸¾é™åˆ¶ï¼‰
        """
        target = self.factor_config["target"]
        target_col = df[target["source"]].copy()
        return target_col.rename(target["alias"])

    def clean(self, csv_contents: List[Union[str, pd.DataFrame]]) -> Tuple[pd.DataFrame, pd.Series]:
        """
        åˆå¹¶ã€æ˜ å°„å¹¶æ¸…æ´—ç‰¹å¾ + ç›®æ ‡åˆ—ï¼Œè¿”å› X å’Œ y
        """
        df = self._load_csvs(csv_contents)

        # æ¸…æ´—æ‰€æœ‰ç‰¹å¾åˆ—
        feature_series = []
        for feature in self.factor_config["features"]:
            if feature["source"] not in df.columns:
                raise KeyError(f"ç¼ºå°‘å­—æ®µ: {feature['source']}")
            cleaned = self._clean_feature(df, feature)
            feature_series.append(cleaned)

        # åˆå¹¶ä¸ºç‰¹å¾ DataFrame
        X = pd.concat(feature_series, axis=1)

        # æ¸…æ´—ç›®æ ‡å­—æ®µ
        y = self._clean_target(df)

        # åˆå¹¶åç»Ÿä¸€å»é™¤ NAï¼ˆé˜²æ­¢å¯¹é½å¤±è´¥ï¼‰
        data = pd.concat([X, y], axis=1).dropna()
        X_cleaned = data.drop(columns=[y.name])
        y_cleaned = data[y.name]

        # æ‰§è¡Œå½’ä¸€åŒ–ï¼ˆä»…å¯¹æ•°å€¼å‹åˆ—ï¼‰
        if self.scaler:
            numeric_cols = [f["alias"] for f in self.factor_config["features"]
                            if f.get("type") == "numeric" or self._infer_type(df[f["source"]]) == "numeric"]
            X_cleaned[numeric_cols] = self.scaler.fit_transform(X_cleaned[numeric_cols])

        return X_cleaned, y_cleaned

    def split(self, X: pd.DataFrame, y: pd.Series) -> Tuple:
        """
        æ‹†åˆ†è®­ç»ƒé›† / æµ‹è¯•é›†
        """
        return train_test_split(X, y, train_size=self.train_ratio, shuffle=True, random_state=42)

    def process(self, csv_contents: List[Union[str, pd.DataFrame]]) -> Tuple:
        """
        ä¸€ä½“åŒ–å¤„ç†ï¼šæ¸…æ´— + æ‹†åˆ†
        """
        X, y = self.clean(csv_contents)
        return self.split(X, y)
```

------

## âœ… ä½¿ç”¨æ–¹å¼ï¼ˆå…¶ä»–æ¨¡å—å¯ç›´æ¥å¼•ç”¨ï¼‰

```python
from utils.data_cleaner import DataCleaner

# å‡è®¾æ¥è‡ªå‰ç«¯çš„å› å­é…ç½®ï¼š
factor_config = {
    "features": [
        {"alias": "Age", "source": "å¹´é¾„", "min": 18, "max": 100},
        {"alias": "Gender", "source": "æ€§åˆ«", "enum": ["ç”·", "å¥³"]}
    ],
    "target": {"alias": "æ”¶å…¥", "source": "å¹´æ”¶å…¥"},
    "normalize": True
}

cleaner = DataCleaner(factor_config, train_ratio=0.8)
X_train, X_test, y_train, y_test = cleaner.process([df1, df2])
```

------

## â˜‘ï¸ æ¨¡å—å¤ç”¨å»ºè®®

ä½ å¯ä»¥å°† `DataCleaner` æ”¾å…¥å¹³å°çš„ï¼š

- âœ… æ¨¡å‹è®­ç»ƒå…¥å£å‰çš„æ•°æ®æ¸…æ´—é˜¶æ®µï¼›
- âœ… æ•°æ®åˆ†ææ¨¡å—å‰çš„ç‰¹å¾ç­›é€‰é˜¶æ®µï¼›
- âœ… è‡ªåŠ¨å»ºæ¨¡ã€è°ƒå‚ä»»åŠ¡çš„ç»Ÿä¸€é¢„å¤„ç†æ¥å£ï¼›
- âœ… æ¥å…¥ä»»åŠ¡é˜Ÿåˆ—ä¸­ï¼Œå®ç°å¼‚æ­¥æ•°æ®é¢„å¤„ç†ã€‚

------

å¦‚éœ€ï¼š

- **å­—æ®µåæ˜ å°„æŒä¹…åŒ–**ï¼ˆä¿å­˜å†å²æ˜ å°„æ–¹æ¡ˆï¼‰
- **å­—æ®µç±»å‹æ™ºèƒ½æ ‡æ³¨ç³»ç»Ÿ**ï¼ˆç”±æ ·æœ¬è‡ªåŠ¨è¯†åˆ«é…ç½®ï¼‰
- **å°†æ¨¡å—è½¬ä¸º API å¾®æœåŠ¡**

æˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ æ„å»ºã€‚æ˜¯å¦ç°åœ¨å°±æ‰“åŒ…æˆä¸€ä¸ª FastAPI æœåŠ¡æˆ–ä»»åŠ¡ä»»åŠ¡æ¨¡å—ï¼Ÿ

å¥½çš„ï¼Œæˆ‘ä»¬å°†å‡çº§æ•°æ®æ¸…æ´—æ¨¡å—ä»¥æ”¯æŒä»¥ä¸‹èƒ½åŠ›ï¼š

------

## âœ… åŠŸèƒ½éœ€æ±‚ï¼ˆæ‰©å±•ç‰ˆï¼‰

1. **å­—æ®µæ˜ å°„æ”¯æŒ**ï¼šå­—æ®µåå¯èƒ½ä¸æ•°æ®è¡¨ä¸­çš„ä¸ä¸€è‡´ï¼Œæ”¯æŒç»Ÿä¸€å­—æ®µæ˜ å°„ã€‚
2. **å­—æ®µç±»å‹åˆ¤æ–­ä¸æ¸…æ´—è§„åˆ™**ï¼š
   - **æ•°å€¼å‹å­—æ®µ**ï¼šæ”¯æŒ `min` / `max` é™åˆ¶ã€æšä¸¾å€¼é™åˆ¶ã€‚
   - **å­—ç¬¦å‹å­—æ®µ**ï¼šæ”¯æŒæšä¸¾å€¼ï¼ˆç™½åå•ï¼‰æ ¡éªŒã€‚
3. **å­—æ®µç¼ºå¤±ä¸å¼‚å¸¸å€¼å¤„ç†**ï¼šéæ³•å€¼ã€ç¼ºå¤±å€¼ç›´æ¥è¿‡æ»¤æˆ–æ—¥å¿—è®°å½•ã€‚
4. **ç»Ÿä¸€å­—æ®µåå¤„ç†åå†åšå½’ä¸€åŒ–ã€åˆ‡åˆ†ç­‰æ“ä½œã€‚**

------

## ğŸ§© å› å­é…ç½®ç»“æ„ï¼ˆç¤ºä¾‹ï¼‰

```json
{
  "features": [
    {
      "alias": "Age",          // å‰ç«¯å­—æ®µåï¼ˆç»Ÿä¸€åï¼‰
      "source": "age_years",   // åŸæ•°æ®å­—æ®µå
      "type": "numeric",
      "min": 18,
      "max": 100
    },
    {
      "alias": "Gender",
      "source": "sex",
      "type": "categorical",
      "enum": ["Male", "Female"]
    }
  ],
  "target": {
    "alias": "bmi",
    "source": "body_mass_index"
  },
  "normalize": true
}
```

------

## ğŸ§© æ¨¡å—è®¾è®¡ï¼ˆç®€åŒ–æ ¸å¿ƒé€»è¾‘ï¼‰

æ–‡ä»¶è·¯å¾„ï¼š`core/data/processor.py`

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple, Union


class DataProcessorV2:
    def __init__(self, factor_config: Dict, train_ratio: float = 0.8):
        self.feature_config = factor_config["features"]
        self.target_config = factor_config["target"]
        self.normalize = factor_config.get("normalize", False)
        self.train_ratio = train_ratio

    def load_and_merge(self, csv_contents: List[Union[str, pd.DataFrame]]) -> pd.DataFrame:
        dfs = []
        for item in csv_contents:
            if isinstance(item, str):
                df = pd.read_csv(pd.compat.StringIO(item))
            elif isinstance(item, pd.DataFrame):
                df = item
            else:
                raise ValueError("Unsupported csv content type")
            dfs.append(df)
        merged = pd.concat(dfs, ignore_index=True)
        return merged.drop_duplicates()

    def map_and_clean(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        cleaned_df = pd.DataFrame()

        for feature in self.feature_config:
            src = feature["source"]
            dst = feature["alias"]
            dtype = feature["type"]

            if src not in df.columns:
                raise ValueError(f"Missing feature column: {src}")

            col = df[src].copy()

            if dtype == "numeric":
                if "min" in feature:
                    col = col[col >= feature["min"]]
                if "max" in feature:
                    col = col[col <= feature["max"]]
                if "enum" in feature:
                    col = col[col.isin(feature["enum"])]
            elif dtype == "categorical":
                if "enum" in feature:
                    col = col[col.isin(feature["enum"])]
            else:
                raise ValueError(f"Unsupported type {dtype} for feature {dst}")

            cleaned_df[dst] = col

        # å¤„ç† target
        target_src = self.target_config["source"]
        target_dst = self.target_config["alias"]

        if target_src not in df.columns:
            raise ValueError("Missing target column")

        target_col = df[target_src]
        cleaned_df[target_dst] = target_col

        # å»é™¤å«æœ‰ NA çš„è¡Œ
        cleaned_df.dropna(inplace=True)

        # æå– X å’Œ y
        X = cleaned_df[[f["alias"] for f in self.feature_config]]
        y = cleaned_df[target_dst]

        # å½’ä¸€åŒ–
        if self.normalize:
            numeric_features = [f["alias"] for f in self.feature_config if f["type"] == "numeric"]
            scaler = StandardScaler()
            X[numeric_features] = scaler.fit_transform(X[numeric_features])

        return X, y

    def split_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple:
        return train_test_split(X, y, train_size=self.train_ratio, shuffle=True, random_state=42)

    def process(self, csv_contents: List[Union[str, pd.DataFrame]]) -> Tuple:
        merged_df = self.load_and_merge(csv_contents)
        X, y = self.map_and_clean(merged_df)
        return self.split_data(X, y)
```

------

## âœ… ä½¿ç”¨ç¤ºä¾‹

```python
from core.data.processor import DataProcessorV2

factor_config = {
    "features": [
        {"alias": "Age", "source": "age", "type": "numeric", "min": 18, "max": 80},
        {"alias": "Sex", "source": "gender", "type": "categorical", "enum": ["Male", "Female"]}
    ],
    "target": {"alias": "BMI", "source": "bmi"},
    "normalize": True
}

processor = DataProcessorV2(factor_config=factor_config, train_ratio=0.75)
X_train, X_test, y_train, y_test = processor.process([df1, df2])
```

------

## âœ… æ‰©å±•å»ºè®®ï¼ˆåç»­å¯åŠ ï¼‰

- æ”¯æŒå­—æ®µç±»å‹è‡ªåŠ¨æ¨æ–­ï¼ˆ`pandas.api.types`ï¼‰
- ç¼ºå¤±å€¼å¡«å……ç­–ç•¥ï¼ˆå‡å€¼ã€ä¼—æ•°ã€è‡ªå®šä¹‰ï¼‰
- æ—¥å¿—è®°å½•æ¯æ­¥è¿‡æ»¤/ä¸¢å¼ƒçš„æ•°é‡
- æŠ¥é”™é›†ä¸­æ”¶é›†ï¼Œåé¦ˆè‡³ä»»åŠ¡ç³»ç»Ÿ

------

éœ€è¦æˆ‘å¸®ä½ è¡¥å……åˆ°ä»»åŠ¡é˜Ÿåˆ—æµç¨‹æˆ–åŠ å…¥æ—¥å¿—è®°å½•æ¨¡å—å—ï¼Ÿæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å†™æˆç‹¬ç«‹æœåŠ¡æˆ–åŒ…ï¼Œä¾›å¹³å°å¤šä¸ªæ¨¡å—å¤ç”¨ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„ä»£ç è¯´æ˜ã€æ³¨é‡ŠåŠç¤ºä¾‹æµ‹è¯•æ–¹æ³•ï¼Œå°è£…ä¸ºä¸€ä¸ªé€šç”¨çš„æ•°æ®æ¸…æ´—æ¨¡å—ï¼Œæ”¯æŒå¹³å°å¤ç”¨ã€‚

------

## âœ… æ¨¡å—åŠŸèƒ½æ¦‚è¿°

è¿™ä¸ªæ¨¡å— `MultiDatasetProcessor` ç”¨äºå¤„ç†å¤šä¸ª Excel æˆ– CSV æ•°æ®é›†ï¼Œå…·æœ‰ä»¥ä¸‹åŠŸèƒ½ï¼š

1. **è¯»å–å¤šæ–‡ä»¶ï¼ˆæ”¯æŒè·¯å¾„æˆ–å†…å­˜æ–‡ä»¶å¯¹è±¡ï¼‰**
2. **å¯¹åˆ—åæ’åºå’Œæ ‡å‡†åŒ–ï¼Œç¡®ä¿ç»“æ„ä¸€è‡´**
3. **åˆå¹¶å¤šä¸ªæ•°æ®é›†å¹¶å»é‡**
4. **æ ¹æ®å› å­é…ç½®æ‰§è¡Œå­—æ®µæ˜ å°„ã€æœ€å¤§å€¼/æœ€å°å€¼/æšä¸¾è¿‡æ»¤**
5. **å½’ä¸€åŒ–æ•°å€¼å‹å­—æ®µï¼ˆå¯é€‰ï¼‰**
6. **æ ¹æ®ç›®æ ‡å­—æ®µåˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•æ•°æ®**

------

## âœ… ä½¿ç”¨æ–¹å¼ç¤ºä¾‹ï¼ˆæµ‹è¯•ä»£ç ï¼‰

```python
from io import BytesIO

# æ¨¡æ‹Ÿå› å­é…ç½®
factor_config = {
    "features": [
        {
            "source": "Age",        # åŸå­—æ®µå
            "alias": "age",         # ç›®æ ‡å­—æ®µå
            "min": 18,
            "max": 60
        },
        {
            "source": "Gender",
            "alias": "gender",
            "enum": ["Male", "Female"]
        },
        {
            "source": "Income",
            "alias": "income",
            "min": 3000
        }
    ],
    "normalize": True  # æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–
}

# æ¨¡æ‹Ÿå¤šä¸ªæ•°æ®æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡ï¼ˆå¯æ›¿æ¢ä¸ºå®é™…ä¸Šä¼ æ–‡ä»¶å¯¹è±¡ï¼‰
files = [
    "dataset1.csv",
    "dataset2.xlsx",
]

# åˆå§‹åŒ–å¤„ç†å™¨
processor = MultiDatasetProcessor(files=files, factor_config=factor_config)

# æ¸…æ´—æ•°æ®
cleaned_df = processor.clean()

# æ‰“å°æ¸…æ´—ç»“æœ
print("Cleaned Data:\n", cleaned_df.head())

# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = processor.split(target_column="income")

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)
```

------

## âœ… æ¨¡å—å¤ç”¨å»ºè®®

ä½ å¯ä»¥å°†è¯¥æ¨¡å—ä¿å­˜ä¸º Python åŒ…æˆ–å¾®æœåŠ¡ï¼ˆå¦‚ Flask FastAPIï¼‰ï¼š

### å°è£…ä¸ºåŒ…

åˆ›å»ºç›®å½•ç»“æ„ï¼š

```
my_data_cleaner/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ processor.py
```

`processor.py` ä¸­å­˜æ”¾ä¸Šé¢çš„ç±»ã€‚

### æ¥å…¥ä»»åŠ¡é˜Ÿåˆ—ï¼ˆCelery ç¤ºä¾‹ï¼‰

```python
# tasks.py
from celery import shared_task
from my_data_cleaner.processor import MultiDatasetProcessor

@shared_task
def run_data_cleaning(files, factor_config):
    processor = MultiDatasetProcessor(files, factor_config)
    cleaned = processor.clean()
    return cleaned.to_json()
```

------

## âœ… å°ç»“

| åŠŸèƒ½é¡¹             | æ˜¯å¦æ”¯æŒ                     |
| ------------------ | ---------------------------- |
| å¤šæ•°æ®é›†åˆ—åä¸ä¸€è‡´ | âœ… åˆ—æ ‡å‡†åŒ–å¹¶æŒ‰åç§°æ’åº       |
| æ•°æ®æ¸…æ´—           | âœ… æ ¹æ®å› å­é…ç½®ï¼ˆæ•°å€¼/å­—ç¬¦ï¼‰  |
| å½’ä¸€åŒ–             | âœ… é’ˆå¯¹æ•°å€¼å­—æ®µ               |
| ç±»å‹æ¨æ–­           | âœ… è‡ªåŠ¨è¯†åˆ«æ•°å€¼æˆ–å­—ç¬¦å‹       |
| æ–‡ä»¶æ¥æºæ”¯æŒ       | âœ… CSV, Excel, è·¯å¾„/å†…å­˜å¯¹è±¡  |
| æ¨¡å—å¤ç”¨           | âœ… å¯ä½œä¸ºåŒ…/æœåŠ¡/ä»»åŠ¡é˜Ÿåˆ—è°ƒç”¨ |

å¦‚æœä½ éœ€è¦æˆ‘å°†æ­¤æ¨¡å—æ•´ç†ä¸º Python åŒ…ã€FastAPI æœåŠ¡æˆ– Celery å·¥å…·ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®åŠ©ä½ å°è£…éƒ¨ç½²ã€‚

ä»¥ä¸‹æ˜¯ä¿®æ”¹åçš„å®Œæ•´ Python æ¨¡å—ï¼Œå·²å°†æ–‡ä»¶è¯»å–é€»è¾‘æ›´æ–°ä¸ºä½¿ç”¨ `openpyxl` æ¥å¤„ç† Excel æ–‡ä»¶ï¼Œå¹¶è¾…ä»¥ `pandas` è¿›è¡Œæ•°æ®æ“ä½œï¼Œæ”¯æŒ CSVã€Excel æ–‡ä»¶è·¯å¾„æˆ– `BytesIO` æ–‡ä»¶å¯¹è±¡çš„å¤šæ•°æ®é›†è¯»å–ã€æ¸…æ´—ä¸åˆ‡åˆ†ã€‚

------

## âœ… æ¨¡å—è¯´æ˜ï¼š`MultiDatasetProcessor`

### ğŸ“Œ åŠŸèƒ½æ¦‚è§ˆ

- æ”¯æŒå¤šä¸ª CSV / Excel æ–‡ä»¶
- ä½¿ç”¨ `openpyxl` è¯»å– `.xlsx` æ–‡ä»¶ï¼Œç¡®ä¿å…¼å®¹å¤šæ ¼å¼
- å¯¹å­—æ®µè¿›è¡Œç»Ÿä¸€å¯¹é½ã€æ¸…æ´—ã€å­—æ®µæ˜ å°„
- æ”¯æŒæ•°å€¼èŒƒå›´/æšä¸¾/å½’ä¸€åŒ–ç­‰å› å­è§„åˆ™
- æ”¯æŒè®­ç»ƒ/æµ‹è¯•é›†åˆ‡åˆ†

------

## âœ… ä½¿ç”¨ç¤ºä¾‹

```python
from io import BytesIO

factor_config = {
    "features": [
        {
            "source": "å¹´é¾„",
            "alias": "age",
            "min": 18,
            "max": 60
        },
        {
            "source": "æ€§åˆ«",
            "alias": "gender",
            "enum": ["ç”·", "å¥³"]
        },
        {
            "source": "æ”¶å…¥",
            "alias": "income",
            "min": 3000
        }
    ],
    "normalize": True
}

files = ["dataset1.csv", "dataset2.xlsx"]

processor = MultiDatasetProcessor(files=files, factor_config=factor_config)

cleaned = processor.clean()
print(cleaned.head())

X_train, X_test, y_train, y_test = processor.split(target_column="income")
print("Train:", X_train.shape, y_train.shape)
print("Test:", X_test.shape, y_test.shape)
```

------

## âœ… æ ¸å¿ƒä»£ç åŠŸèƒ½è§£é‡Šï¼ˆç®€è¦ï¼‰

| å‡½æ•°                        | è¯´æ˜                                                         |
| --------------------------- | ------------------------------------------------------------ |
| `_read_excel_with_openpyxl` | ä½¿ç”¨ openpyxl è¯»å– Excelï¼Œè·å–è¡¨å¤´ä¸æ•°æ®ï¼Œè½¬ä¸º pandas.DataFrame |
| `_load_single_file`         | è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼Œå†³å®šä½¿ç”¨ pandas è¿˜æ˜¯ openpyxl åŠ è½½             |
| `_align_columns`            | æ ‡å‡†åŒ–åˆ—åå¹¶æŒ‰å­—æ¯æ’åºï¼Œç”¨äºåˆ—å¯¹é½                           |
| `_load_and_merge`           | é€ä¸ªè¯»å–æ•°æ®é›†å¹¶åˆå¹¶å»é‡                                     |
| `clean`                     | æŒ‰ç…§å› å­é…ç½®è¿›è¡Œå­—æ®µæ˜ å°„ã€ç­›é€‰ã€å½’ä¸€åŒ–ç­‰å¤„ç†                 |
| `split`                     | æŒ‰ç…§è®¾å®šæ¯”ä¾‹å¯¹æ¸…æ´—åçš„æ•°æ®è¿›è¡Œè®­ç»ƒ/æµ‹è¯•æ‹†åˆ†                  |

------

å¦‚ä½ å¸Œæœ›å°†è¯¥æ¨¡å—å°è£…ä¸º API æœåŠ¡æˆ– Celery ä»»åŠ¡é˜Ÿåˆ—æ”¯æŒç»„ä»¶ï¼Œä¹Ÿå¯ä»¥ç»§ç»­è¿›è¡Œé›†æˆè®¾è®¡ã€‚å¦‚æœä½ æƒ³è¦æˆ‘ç”Ÿæˆå®Œæ•´ FastAPI æˆ– Flask æœåŠ¡ç»“æ„ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚

ä¸‹é¢æ˜¯æ›´æ–°åçš„ `MultiDatasetProcessor`ï¼Œå®ç°äº† **æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶åªç”¨ `openpyxl`ï¼ˆExcelï¼‰å’Œ Python æ ‡å‡†åº“çš„ `csv`ï¼ˆCSVï¼‰** æ¥è¯»å–ï¼Œå†äº¤ç”± `pandas` ç»Ÿä¸€å°è£…æˆ `DataFrame` è¿›è¡Œåç»­å¤„ç†ã€‚è¿™æ ·å°±ç»•è¿‡äº†æœ‰æ—¶ `pandas.read_csv` å¯èƒ½å‡ºç°çš„é—®é¢˜ã€‚

------

```python
import os
import csv
from io import TextIOWrapper, BytesIO
from typing import List, Union

import pandas as pd
from openpyxl import load_workbook


class MultiDatasetProcessor:
    """
    å¤šæ•°æ®é›†å¤„ç†å™¨ï¼Œåªä½¿ç”¨ openpyxl è¯»å– Excelï¼Œä½¿ç”¨ csv åº“è¯»å– CSVï¼Œ
    å¹¶åŸºäºå› å­é…ç½®æ‰§è¡Œæ¸…æ´—ã€å½’ä¸€åŒ–å’Œè®­ç»ƒ/æµ‹è¯•æ‹†åˆ†ã€‚
    """

    def __init__(
        self,
        files: List[Union[str, BytesIO]],
        factor_config: dict,
        sheet_name: str = None
    ):
        """
        :param files: æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡åˆ—è¡¨ï¼ˆ.csv æˆ– .xlsx/.xlsï¼‰
        :param factor_config: å› å­é…ç½®ï¼Œè§ç¤ºä¾‹
        :param sheet_name: Excel çš„ sheet åç§°ï¼ˆå¯é€‰ï¼‰
        """
        self.files = files
        self.factor_config = factor_config
        self.sheet_name = sheet_name
        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
        self.df = self._load_and_merge()
        self.cleaned_df = None

    def _read_excel_with_openpyxl(self, file: Union[str, BytesIO]) -> pd.DataFrame:
        """
        ä½¿ç”¨ openpyxl è¯»å– Excel æ–‡ä»¶ï¼Œå°†å…¶è½¬æˆ pandas.DataFrameã€‚
        """
        # åŠ è½½å·¥ä½œç°¿
        wb = load_workbook(filename=file, data_only=True)
        # é€‰æ‹©æŒ‡å®š sheet æˆ–ç¬¬ä¸€ä¸ª sheet
        ws = wb[self.sheet_name or wb.sheetnames[0]]
        data = list(ws.values)
        # ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
        headers = [str(h).strip() if h is not None else "" for h in data[0]]
        # åç»­è¡Œä½œä¸ºæ•°æ®
        values = data[1:]
        return pd.DataFrame(values, columns=headers)

    def _read_csv_with_stdlib(self, file: Union[str, BytesIO]) -> pd.DataFrame:
        """
        ä½¿ç”¨ Python æ ‡å‡†åº“ csv è¯»å– CSV æ–‡ä»¶ï¼Œå°†å…¶è½¬æˆ pandas.DataFrameã€‚
        """
        # æ‰“å¼€æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡
        if isinstance(file, str):
            f = open(file, newline='', encoding='utf-8')
        else:
            # BytesIO -> æ–‡æœ¬æµ
            f = TextIOWrapper(file, encoding='utf-8')
        reader = csv.reader(f)
        rows = list(reader)
        f.close()

        if not rows:
            return pd.DataFrame()

        # ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
        headers = [h.strip() for h in rows[0]]
        # å‰©ä½™è¡Œæ˜¯æ•°æ®
        values = rows[1:]
        return pd.DataFrame(values, columns=headers)

    def _load_single_file(self, file: Union[str, BytesIO]) -> pd.DataFrame:
        """
        åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶è°ƒç”¨ç›¸åº”çš„è¯»å–å‡½æ•°ã€‚
        """
        # è¯†åˆ«æ‰©å±•å
        if isinstance(file, str):
            ext = os.path.splitext(file)[1].lower()
        else:
            # å¦‚æœæ˜¯ BytesIO å¯¹è±¡ï¼Œé»˜è®¤å½“æˆ Excel
            ext = '.xlsx'

        if ext == '.csv':
            return self._read_csv_with_stdlib(file)
        elif ext in ('.xls', '.xlsx'):
            return self._read_excel_with_openpyxl(file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")

    def _align_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–åˆ—åï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰å¹¶æŒ‰å­—æ¯æ’åºï¼Œä½¿å¤šä¸ª DataFrame åˆ—é¡ºåºä¸€è‡´ã€‚
        """
        df.columns = [col.strip() for col in df.columns]
        return df.reindex(sorted(df.columns), axis=1)

    def _load_and_merge(self) -> pd.DataFrame:
        """
        é€ä¸ªåŠ è½½æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ—å¯¹é½ååˆå¹¶ï¼Œå¹¶å»é‡ã€‚
        """
        aligned = []
        for file in self.files:
            df = self._load_single_file(file)
            df = self._align_columns(df)
            aligned.append(df)

        combined = pd.concat(aligned, ignore_index=True)
        return combined.drop_duplicates()

    def _infer_column_type(self, col: pd.Series) -> str:
        """
        è‡ªåŠ¨åˆ¤æ–­å­—æ®µç±»å‹ï¼šæ•°å€¼å‹æˆ–å­—ç¬¦ä¸²å‹ã€‚
        """
        if pd.api.types.is_numeric_dtype(col):
            return "numeric"
        return "string"

    def clean(self) -> pd.DataFrame:
        """
        æŒ‰å› å­é…ç½®æ¸…æ´—æ•°æ®ï¼š
        1) å­—æ®µæ˜ å°„ï¼ˆsource -> aliasï¼‰
        2) æ•°å€¼å‹å­—æ®µæ ¹æ® min/max/enum è¿‡æ»¤
        3) å­—ç¬¦å‹å­—æ®µæ ¹æ® enum è¿‡æ»¤
        4) åˆ é™¤ä»»ä½• alias åˆ—ä¸­å« NA çš„è¡Œ
        5) å¯¹æ•°å€¼å‹å­—æ®µæ‰§è¡Œæ ‡å‡†åŒ–ï¼ˆå¦‚ normalize=Trueï¼‰
        """
        result = {}
        for feature in self.factor_config["features"]:
            src = feature["source"]
            alias = feature["alias"]
            if src not in self.df.columns:
                continue

            col = self.df[src].copy()
            col_type = self._infer_column_type(col)

            # æ•°å€¼å‹æ¸…æ´—
            if col_type == "numeric":
                col = pd.to_numeric(col, errors="coerce")
                if "min" in feature:
                    col = col.where(col >= feature["min"])
                if "max" in feature:
                    col = col.where(col <= feature["max"])
                if "enum" in feature:
                    col = col.where(col.isin(feature["enum"]))
            # å­—ç¬¦å‹æ¸…æ´—
            else:
                col = col.astype(str).str.strip()
                if "enum" in feature:
                    col = col.where(col.isin(feature["enum"]))

            result[alias] = col

        # æ„é€ æ¸…æ´—åçš„ DataFrameï¼Œåˆ é™¤ä»»ä½•å« NA çš„è¡Œ
        cleaned = pd.DataFrame(result).dropna()

        # å½’ä¸€åŒ–æ•°å€¼å‹å­—æ®µ
        if self.factor_config.get("normalize"):
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            num_cols = [
                f["alias"] for f in self.factor_config["features"]
                if f["alias"] in cleaned.columns and pd.api.types.is_numeric_dtype(cleaned[f["alias"]])
            ]
            if num_cols:
                cleaned[num_cols] = scaler.fit_transform(cleaned[num_cols])

        self.cleaned_df = cleaned
        return cleaned

    def split(self, target_column: str, train_ratio: float = 0.8):
        """
        å°†æ¸…æ´—åçš„æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
        :param target_column: ç›®æ ‡å­—æ®µï¼ˆalias åç§°ï¼‰
        :param train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        :return: X_train, X_test, y_train, y_test
        """
        from sklearn.model_selection import train_test_split

        if self.cleaned_df is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ `.clean()` æ–¹æ³•")

        X = self.cleaned_df.drop(columns=[target_column])
        y = self.cleaned_df[target_column]
        return train_test_split(X, y, train_size=train_ratio, random_state=42)
```

### æ ¸å¿ƒè¡Œ-by-è¡Œè§£é‡Š

1. **`load_workbook`**ã€**`csv.reader`**
   - ç”¨ `openpyxl.load_workbook` è¯»å– Excelï¼Œè·å–æ‰€æœ‰å•å…ƒæ ¼å€¼ã€‚
   - ç”¨ Python æ ‡å‡†åº“ `csv.reader` è¯»å– CSVï¼Œä¸ä¾èµ– `pandas.read_csv`ã€‚
2. **`_align_columns`**
   - `df.columns = [col.strip() â€¦]`ï¼šå»é™¤åˆ—åå‰åç©ºæ ¼ã€‚
   - `df.reindex(sorted(df.columns), axis=1)`ï¼šæŒ‰å­—æ¯é¡ºåºæ’åˆ—åˆ—ï¼Œä¿è¯ä¸åŒæ–‡ä»¶åˆ—é¡ºåºä¸€è‡´ã€‚
3. **åˆå¹¶ä¸å»é‡**
   - `pd.concat(aligned, ignore_index=True)`ï¼šåˆå¹¶æ‰€æœ‰ DataFrameã€‚
   - `.drop_duplicates()`ï¼šå»é™¤é‡å¤è¡Œã€‚
4. **å­—æ®µæ¸…æ´—**
   - `pd.to_numeric(..., errors="coerce")`ï¼šå°†åˆ—è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬æ¢çš„å˜ä¸º NaNã€‚
   - `col.where(cond)`ï¼šåªæœ‰æ»¡è¶³æ¡ä»¶çš„è¡Œä¿ç•™ï¼Œå¦åˆ™ç½®ä¸º NaNã€‚
   - å¯¹å­—ç¬¦ä¸²åˆ—åš `.str.strip()` å¹¶ç”¨ `.isin()` è¿‡æ»¤æšä¸¾å€¼ã€‚
5. **åˆ é™¤ç¼ºå¤±**
   - `pd.DataFrame(result).dropna()`ï¼šå¯¹æ‰€æœ‰ alias åˆ—ï¼Œä»»ä½•åŒ…å« NaN çš„è¡Œéƒ½ä¼šè¢«åˆ é™¤ï¼Œç¡®ä¿æ¯æ¡æ•°æ®å®Œæ•´ã€‚
6. **å½’ä¸€åŒ–**
   - ä½¿ç”¨ `StandardScaler` å¯¹æ‰€æœ‰æ•°å€¼å‹å­—æ®µåš `fit_transform`ã€‚
7. **æ‹†åˆ†æ•°æ®**
   - `train_test_split`ï¼šæŒ‰ `train_ratio` æ¯”ä¾‹éšæœºåˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•é›†ã€‚

------

## æµ‹è¯•ç¤ºä¾‹

```python
from io import BytesIO

# ç¤ºä¾‹å› å­é…ç½®
factor_config = {
    "features": [
        {"source": "Age", "alias": "age", "min": 18, "max": 60},
        {"source": "Gender", "alias": "gender", "enum": ["Male", "Female"]},
        {"source": "Income", "alias": "income", "min": 3000}
    ],
    "normalize": True
}

# å‡è®¾ dataset1.xlsx, dataset2.xlsx æ”¾åœ¨å½“å‰ç›®å½•
files = ["dataset1.xlsx", "dataset2.xlsx"]

processor = MultiDatasetProcessor(files, factor_config)
# æ¸…æ´—æ•°æ®
cleaned_df = processor.clean()
print("Cleaned DataFrame:\n", cleaned_df.head())

# æ‹†åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = processor.split(target_column="income")
print(f"Train X shape: {X_train.shape}, y shape: {y_train.shape}")
print(f"Test X shape: {X_test.shape}, y shape: {y_test.shape}")
```

è¿™æ ·ï¼Œæ‰€æœ‰çš„æ–‡ä»¶è¯»å–éƒ½ä»…ä¾èµ– `openpyxl`ï¼ˆExcelï¼‰å’Œæ ‡å‡†åº“ `csv`ï¼ˆCSVï¼‰ï¼Œé¿å…äº† `pandas.read_csv` çš„å¶å‘é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™äº†å¼ºå¤§çš„ DataFrame æ“ä½œèƒ½åŠ›ã€‚

å¯¹ï¼Œç”¨ openpyxl åªæ”¯æŒ Excel æ ¼å¼ï¼ˆ.xls/.xlsxï¼‰ï¼Œå®ƒ**æ— æ³•**è¯»å– CSVã€‚è¦å¤„ç† CSVï¼Œä½ éœ€è¦ç”¨ Python çš„ csv æ¨¡å—æˆ– `pandas.read_csv`ï¼ˆå¦‚æœå®ƒç¨³å®šï¼‰æ¥è¯»å–ã€‚ä¸‹é¢ç»™ä½ ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•ç»Ÿä¸€ã€Œopenpyxl + csv æ¨¡å—ã€æ¥è¯»å– Excel å’Œ CSV æ–‡ä»¶ï¼Œå¹¶æ‹¼æˆ DataFrameã€‚

```python
import os
import csv
from io import TextIOWrapper, BytesIO
import pandas as pd
from openpyxl import load_workbook

def read_file_to_df(file: Union[str, BytesIO], sheet_name: str = None) -> pd.DataFrame:
    """
    æ ¹æ®æ‰©å±•åå†³å®šè¯»å–æ–¹å¼ï¼š
    - Excel (.xls/.xlsx) ç”¨ openpyxl
    - CSV (.csv) ç”¨ Python csv æ¨¡å—
    è¿”å› pandas.DataFrame
    """
    # åˆ¤æ–­æ‰©å±•å
    if isinstance(file, str):
        ext = os.path.splitext(file)[1].lower()
    else:
        # æ–‡ä»¶å¯¹è±¡ï¼Œå‡è®¾æ˜¯ CSV or Excelï¼Œæ ¹æ®åœºæ™¯å†³å®š
        # è¿™é‡Œå¦‚æœæ˜¯ BytesIO ä¸”æƒ³è¯» CSVï¼Œå¯ä¼ å…¥ä¸€ä¸ªæ ‡å¿—
        raise ValueError("è¯·ä¼ å…¥æ–‡ä»¶è·¯å¾„ä»¥è¯†åˆ«ç±»å‹")

    if ext in (".xls", ".xlsx"):
        # --- EXCEL: openpyxl è¯»å– ---
        wb = load_workbook(filename=file, data_only=True)
        ws = wb[sheet_name or wb.sheetnames[0]]
        rows = list(ws.values)
        headers = [str(h).strip() if h is not None else "" for h in rows[0]]
        values = rows[1:]
        return pd.DataFrame(values, columns=headers)

    elif ext == ".csv":
        # --- CSV: csv æ¨¡å— è¯»å– ---
        with open(file, newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            rows = list(reader)
        if not rows:
            return pd.DataFrame()
        headers = [h.strip() for h in rows[0]]
        values = rows[1:]
        return pd.DataFrame(values, columns=headers)

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")

# ä½¿ç”¨ç¤ºä¾‹
df_excel = read_file_to_df("data.xlsx")
df_csv   = read_file_to_df("data.csv")
```

**è¦ç‚¹æ€»ç»“**

1. **openpyxl** åªç®¡ Excelï¼ˆ.xlsx/.xlsï¼‰ï¼›
2. **CSV** å¿…é¡»ç”¨ Python å†…ç½®çš„ **csv** æ¨¡å—æˆ–å…¶ä»–åº“æ¥è¯»ï¼›
3. è¯»å®Œåéƒ½è½¬æ¢æˆ pandas.DataFrameï¼Œå†åšåç»­æ¸…æ´—ã€å½’ä¸€åŒ–ã€åˆ‡åˆ†ç­‰æ“ä½œã€‚

è¿™æ ·å°±èƒ½é¿å¼€ `pandas.read_csv` åœ¨æŸäº›å¤æ‚ CSV ä¸Šçš„å…¼å®¹é—®é¢˜ï¼ŒåŒæ—¶æ•´åˆ `openpyxl` çš„ Excel è¯»å–ä¼˜ç‚¹ã€‚

ä¸‹é¢æ˜¯å°†ä½ æå‡ºçš„**å¤šå­—æ®µæ•°æ®æ¸…æ´—è§„åˆ™ï¼ˆæœ€å¤§/æœ€å°å€¼ + æ­¥é•¿ + å®¹å·® + å€¼æ›¿æ¢ï¼‰**é€»è¾‘ï¼Œ**é€‚é…é›†æˆåˆ°ä¹‹å‰è®¨è®ºè¿‡çš„ç»Ÿä¸€æ•°æ®æ¸…æ´—æ¨¡å—ä¸­çš„å®ç°**ã€‚æ­¤ç‰ˆæœ¬åŸºäºä½ ä¹‹å‰çš„éœ€æ±‚æ•´åˆï¼š

- æ”¯æŒå¤šä¸ªæ•°æ®é›†æ–‡ä»¶ï¼ˆCSV æˆ– XLSXï¼‰
- ä½¿ç”¨ `openpyxl` è¯»å– XLSXï¼Œ`pandas` è¯»å– CSVï¼ˆå¯æ›¿æ¢ï¼‰
- æ‰€æœ‰å­—æ®µåˆ—åä¸€è‡´ä½†é¡ºåºå¯èƒ½ä¸åŒï¼Œæ”¯æŒå­—æ®µå¯¹é½
- æ¸…æ´—å› å­å­—æ®µä¸ºç©ºçš„è®°å½•
- æ¯ä¸ªå­—æ®µå¯é…ç½®æ¸…æ´—è§„åˆ™ï¼ŒåŒ…æ‹¬æ•°å€¼é™å®šã€å€¼å¯¹é½ä¸å‰”é™¤

------

### âœ… æ€»ä½“ç»“æ„

```text
data_cleaning/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ cleaner.py        # ä¸»æ¸…æ´—é€»è¾‘
â”œâ”€â”€ file_reader.py    # XLSX/CSV æ–‡ä»¶è¯»å–
â”œâ”€â”€ utils.py          # å·¥å…·å‡½æ•°
```

------

## âœ… 1. `file_reader.py`ï¼šè¯»å–å¹¶å¯¹é½å­—æ®µ

```python
import pandas as pd
from openpyxl import load_workbook
from pathlib import Path
from typing import List, Union
import io


def read_excel_file(file: Union[str, Path, io.BytesIO]) -> pd.DataFrame:
    """ä½¿ç”¨ openpyxl è¯»å– xlsx æ–‡ä»¶ä¸º DataFrame"""
    wb = load_workbook(file)
    ws = wb.active
    data = list(ws.iter_rows(values_only=True))
    header, *rows = data
    return pd.DataFrame(rows, columns=header)


def read_dataset_files(file_list: List[Union[str, Path, io.BytesIO]]) -> pd.DataFrame:
    """
    è¯»å–å¹¶åˆå¹¶å¤šä¸ªæ•°æ®é›†æ–‡ä»¶ï¼ˆåªæ”¯æŒç›¸åŒç»“æ„ä½†åˆ—é¡ºåºå¯ä¸åŒï¼‰
    """
    dfs = []
    for file in file_list:
        if isinstance(file, (str, Path)) and str(file).lower().endswith(".xlsx"):
            df = read_excel_file(file)
        elif isinstance(file, (str, Path)) and str(file).lower().endswith(".csv"):
            df = pd.read_csv(file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æˆ–ç±»å‹: {file}")
        dfs.append(df)

    # å¯¹å­—æ®µåˆ—ç»Ÿä¸€ï¼ˆæŒ‰ç…§ç¬¬ä¸€ä¸ªä¸ºåŸºå‡†ï¼‰
    base_columns = dfs[0].columns
    for df in dfs[1:]:
        df.columns = base_columns  # ä¿è¯ä¸€è‡´æ€§

    combined_df = pd.concat(dfs, ignore_index=True).drop_duplicates()
    return combined_df
```

------

## âœ… 2. `utils.py`ï¼šç­›é€‰å€¼ç”Ÿæˆå’Œå­—æ®µæ¸…æ´—

```python
import numpy as np
import pandas as pd


def generate_filter_values(min_val: float, max_val: float, step: float) -> np.ndarray:
    """æ ¹æ® min/max/step ç”Ÿæˆç­›é€‰å€¼æ•°ç»„"""
    return np.arange(min_val, max_val + step, step)


def apply_filter_to_series(series: pd.Series, filters: np.ndarray, tolerance: float) -> pd.Series:
    """å°†å­—æ®µå€¼æ›¿æ¢ä¸ºæœ€æ¥è¿‘çš„ç­›é€‰å€¼ï¼Œè¶…å‡ºå®¹å·®åˆ™æ›¿æ¢ä¸º NaN"""
    cleaned = []
    for val in series:
        if pd.isnull(val):
            cleaned.append(np.nan)
            continue
        matched = False
        for f in filters:
            if abs(val - f) <= tolerance:
                cleaned.append(f)
                matched = True
                break
        if not matched:
            cleaned.append(np.nan)
    return pd.Series(cleaned, index=series.index)
```

------

## âœ… 3. `cleaner.py`ï¼šæ¸…æ´—ä¸»é€»è¾‘å…¥å£

```python
import pandas as pd
from typing import List, Union, Dict
from .file_reader import read_dataset_files
from .utils import generate_filter_values, apply_filter_to_series


def clean_datasets(
    files: List[Union[str, bytes]],
    factor_config: Dict[str, dict],
    required_factors: List[str]
) -> pd.DataFrame:
    """
    ç»Ÿä¸€æ•°æ®æ¸…æ´—ä¸»å‡½æ•°

    å‚æ•°ï¼š
    - files: æ–‡ä»¶è·¯å¾„æˆ– file-like å¯¹è±¡åˆ—è¡¨ï¼ˆcsv/xlsxï¼‰
    - factor_config: å„å› å­å­—æ®µçš„æ¸…æ´—é…ç½®ï¼Œå¦‚ min/max/step/tolerance
    - required_factors: å¿…é¡»å­˜åœ¨çš„å› å­å­—æ®µï¼ˆéç©ºï¼‰

    è¿”å›ï¼š
    - æ¸…æ´—åçš„ DataFrame
    """
    df = read_dataset_files(files)

    # æ¸…é™¤å› å­ç¼ºå¤±çš„è®°å½•
    df.dropna(subset=required_factors, inplace=True)

    # åº”ç”¨æ¯ä¸ªå­—æ®µçš„æ¸…æ´—è§„åˆ™
    for factor, config in factor_config.items():
        if factor not in df.columns:
            raise ValueError(f"å­—æ®µ '{factor}' ä¸å­˜åœ¨")

        # èŒƒå›´è¿‡æ»¤
        df = df[(df[factor] >= config["min"]) & (df[factor] <= config["max"])]

        # å€¼æ›¿æ¢
        filters = generate_filter_values(config["min"], config["max"], config["step"])
        df[factor] = apply_filter_to_series(df[factor], filters, config["tolerance"])

    # å‰”é™¤æ— æ³•åŒ¹é…çš„ NaN
    df.dropna(subset=factor_config.keys(), inplace=True)
    return df.reset_index(drop=True)
```

------

## âœ… ç¤ºä¾‹ç”¨æ³•ï¼ˆæµ‹è¯•ç”¨ï¼‰

```python
if __name__ == "__main__":
    from pathlib import Path

    cleaning_rules = {
        "temperature": {"min": 6, "max": 10, "step": 1, "tolerance": 0.2},
        "humidity": {"min": 20, "max": 30, "step": 2, "tolerance": 0.3}
    }

    required_fields = ["temperature", "humidity"]

    files = [
        Path("data/file1.xlsx"),
        Path("data/file2.xlsx")
    ]

    cleaned = clean_datasets(files, cleaning_rules, required_fields)
    print(cleaned)
```

------

## âœ… æ€»ç»“è¯´æ˜

| æ¨¡å—             | åŠŸèƒ½                                     | å¯å¤ç”¨æ€§ |
| ---------------- | ---------------------------------------- | -------- |
| `file_reader.py` | æ”¯æŒä¸åŒæ ¼å¼è¯»å–åˆå¹¶                     | âœ…        |
| `utils.py`       | é€šç”¨å·¥å…·å‡½æ•°ï¼ˆé—´éš”å€¼ç”Ÿæˆä¸ç­›é€‰ï¼‰         | âœ…        |
| `cleaner.py`     | æ¸…æ´—æ€»æ§å‡½æ•°ï¼Œæ”¯æŒå¤šä¸ªå­—æ®µã€å­—æ®µç¼ºå¤±åˆ¤æ–­ | âœ…        |

------

æ˜¯å¦éœ€è¦æˆ‘å°†å®ƒå°è£…ä¸º pip å®‰è£…åŒ…æ ¼å¼ï¼Œæˆ–æ”¯æŒ Celery ä»»åŠ¡å°è£…ç”¨äºå¼‚æ­¥å¤„ç†æ¸…æ´—ï¼Ÿ
