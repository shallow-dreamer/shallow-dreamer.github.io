---
layout:     post
title:      "æ­£æ€åˆ†å¸ƒæµ‹è¯•"
subtitle:   ""
date:       2025-09-07
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - å­¦ä¹ æ¨¡å‹
---

# å¦‚ä½•éªŒè¯ä¸€ç»„æ•°æ®æ˜¯å¦ç¬¦åˆâ€œÏƒï¼ˆsigmaï¼‰æ­£æ€åˆ†å¸ƒâ€è§„åˆ™ï¼ˆå®ç”¨æŒ‡å—ï¼‰

ä¸‹é¢ç»™å‡ºä¸€å¥—æ¸…æ™°çš„ã€å¯å¤ç”¨çš„æ­¥éª¤â€”â€”æ—¢æœ‰ç›´è§‚å¯è§†åŒ–ï¼Œä¹Ÿæœ‰å®šé‡æ£€éªŒï¼Œå¹¶é™„å¸¦ Python å®ç”¨å‡½æ•°ã€‚ä¸»è¦æ€è·¯æ˜¯åŒæ—¶æ£€æŸ¥**ç»éªŒè§„åˆ™ï¼ˆ68â€“95â€“99.7%ï¼‰\**ä¸\**ç»Ÿè®¡æ£€éªŒ/å›¾å½¢è¯Šæ–­**ï¼Œå› ä¸ºå•ä¸€æ–¹æ³•å®¹æ˜“è¯¯å¯¼ã€‚

------

## 1) å¿«é€Ÿæ•°å€¼æ£€æŸ¥ï¼ˆå‡å€¼ / æ ‡å‡†å·® / z-score /ç»éªŒæ¯”ä¾‹ï¼‰

è®¡ç®—æ ·æœ¬å‡å€¼ `Î¼` å’Œæ ‡å‡†å·® `Ïƒ`ï¼Œç„¶åç»Ÿè®¡æ•°æ®è½åœ¨åŒºé—´ `Î¼Â±1Ïƒ`ã€`Î¼Â±2Ïƒ`ã€`Î¼Â±3Ïƒ` çš„æ¯”ä¾‹ï¼Œå’Œç†è®ºæ¯”ä¾‹å¯¹æ¯”ï¼š

- ç†è®ºï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼š
  - Â±1Ïƒ â‰ˆ 68.27%
  - Â±2Ïƒ â‰ˆ 95.45%
  - Â±3Ïƒ â‰ˆ 99.73%

**åˆ¤æ–­å‚è€ƒï¼ˆç»éªŒï¼‰**ï¼šè‹¥å®æµ‹æ¯”ä¾‹æ¥è¿‘ç†è®ºå€¼ï¼ˆä¾‹å¦‚åœ¨ Â±3% æˆ– Â±5% çš„å®¹å·®å†…ï¼‰ï¼Œè¯´æ˜ä¸æ­£æ€ç›¸ç¬¦ï¼›è‹¥å·®å¼‚å¾ˆå¤§ï¼Œåˆ™å¯èƒ½ä¸æ˜¯æ­£æ€ã€‚

------

## 2) å¯è§†åŒ–è¯Šæ–­ï¼ˆå¿…åšï¼‰

- **ç›´æ–¹å›¾ + æ‹Ÿåˆæ­£æ€æ›²çº¿**ï¼šè§‚å¯Ÿå½¢çŠ¶ï¼ˆå¯¹ç§°æ€§ã€å°¾éƒ¨ï¼‰ã€‚
- **Qâ€“Q å›¾ï¼ˆQuantile-Quantile plotï¼‰**ï¼šè‹¥ç‚¹æ¥è¿‘å¯¹è§’çº¿ï¼Œè¯´æ˜æ¥è¿‘æ­£æ€ï¼›ç³»ç»Ÿæ€§åç¦»è¡¨æ˜åæ–œæˆ–åš/è–„å°¾ã€‚
- **ç®±çº¿å›¾** å¯ä»¥å¸®åŠ©æŸ¥çœ‹å¼‚å¸¸å€¼ã€‚

å¯è§†åŒ–é€šå¸¸æ¯”å•ä¸ª p å€¼æ›´æœ‰ä¿¡æ¯é‡ã€‚

------

## 3) ç»Ÿè®¡æ£€éªŒï¼ˆå®šé‡ï¼‰

- **Shapiroâ€“Wilk æ£€éªŒ**ï¼ˆ`scipy.stats.shapiro`ï¼‰ï¼šå¯¹å°/ä¸­æ ·æœ¬ï¼ˆä¾‹å¦‚ n < 5000ï¼‰å¾ˆå¸¸ç”¨ã€‚`p > 0.05` é€šå¸¸è¡¨ç¤ºä¸èƒ½æ‹’ç»æ­£æ€æ€§ï¼ˆå³â€œçœ‹èµ·æ¥åƒæ­£æ€â€ï¼‰ã€‚
- **Andersonâ€“Darling æ£€éªŒ**ï¼ˆ`scipy.stats.anderson`ï¼‰ï¼šç»™å‡ºæ›´æ•æ„Ÿçš„åå·®æ£€æµ‹ï¼Œè¿”å›ç»Ÿè®¡é‡å¹¶æœ‰æ˜¾è‘—æ€§è¡¨æ ¼ã€‚
- **Kolmogorovâ€“Smirnov æ£€éªŒï¼ˆå¯¹æ¯”æ­£æ€ï¼‰**ï¼šåœ¨ç”¨äºæ¯”è¾ƒæ€»ä½“å’ŒæŒ‡å®šåˆ†å¸ƒæ—¶è¦è°¨æ…ï¼ˆå‚æ•°éœ€ç”¨æ ·æœ¬ä¼°è®¡æ—¶ä¸´ç•Œå€¼æ”¹å˜ï¼‰ã€‚
- æ³¨æ„ï¼šå½“æ ·æœ¬é‡å¾ˆå¤§æ—¶ï¼ˆä¾‹å¦‚ n å¾ˆå¤§ï¼‰ï¼Œå¾®å°åå·®ä¼šå¯¼è‡´æ£€éªŒæ˜¾è‘—ï¼ˆæ‹’ç»æ­£æ€æ€§ï¼‰ï¼Œå› æ­¤éœ€ç»“åˆå¯è§†åŒ–ä¸å®é™…åº”ç”¨åœºæ™¯åˆ¤æ–­ã€‚

------

## 4) å…¶ä»–æŒ‡æ ‡

- **ååº¦ï¼ˆskewnessï¼‰** å’Œ **å³°åº¦ï¼ˆkurtosisï¼‰**ï¼šæ­£æ€åˆ†å¸ƒçš„ååº¦â‰ˆ0ï¼Œå³°åº¦â‰ˆ3ï¼ˆå¸¸ç”¨ excess kurtosis = kurtosis-3ï¼‰ã€‚æ˜¾è‘—åç¦»è¡¨æ˜ä¸æ˜¯æ­£æ€ã€‚
- **ç¦»ç¾¤ç‚¹æ£€æµ‹**ï¼šè‹¥å°‘æ•°æç«¯å€¼å½±å“æ•´ä½“åˆ†å¸ƒï¼Œè€ƒè™‘å…ˆå¤„ç†æˆ–æŠ¥å‘Šã€‚

------

## 5) å®ç”¨ Python å‡½æ•°ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

def check_sigma_distribution(arr, plot=True, tolerance_pct=5.0):
    """
    è¾“å…¥: ä¸€ç»´æ•°ç»„æˆ– pandas Series
    è¾“å‡º: ä¸€ä¸ªå­—å…¸æŠ¥å‘Š + å¯é€‰å›¾å½¢
    tolerance_pct: ç”¨äºç»éªŒè§„åˆ™æ¯”è¾ƒçš„å®¹å·®ï¼ˆç™¾åˆ†æ¯”ï¼Œä¾‹å¦‚5%ï¼‰
    """
    x = np.asarray(arr).astype(float)
    x = x[~np.isnan(x)]
    n = x.size
    mu = x.mean()
    sigma = x.std(ddof=1)  # æ ·æœ¬æ ‡å‡†å·®

    # empirical proportions
    within_1 = np.mean((x >= mu - sigma) & (x <= mu + sigma)) * 100
    within_2 = np.mean((x >= mu - 2*sigma) & (x <= mu + 2*sigma)) * 100
    within_3 = np.mean((x >= mu - 3*sigma) & (x <= mu + 3*sigma)) * 100

    empirical = {'Â±1Ïƒ': within_1, 'Â±2Ïƒ': within_2, 'Â±3Ïƒ': within_3}
    theoretical = {'Â±1Ïƒ': 68.27, 'Â±2Ïƒ': 95.45, 'Â±3Ïƒ': 99.73}
    diffs = {k: empirical[k] - theoretical[k] for k in empirical}

    # statistical tests
    sw_stat, sw_p = (None, None)
    if n >= 3 and n <= 5000:
        sw_stat, sw_p = stats.shapiro(x)
    ad_result = stats.anderson(x, dist='norm')  # returns statistic + critical values

    skew = stats.skew(x)
    kurt = stats.kurtosis(x, fisher=False)  # Pearson's definition (â‰ˆ3 for normal)
    excess_kurtosis = kurt - 3.0

    report = {
        'n': n, 'mean': mu, 'std_sample': sigma,
        'empirical_pct': empirical, 'theoretical_pct': theoretical, 'diffs_pct': diffs,
        'skewness': skew, 'kurtosis': kurt, 'excess_kurtosis': excess_kurtosis,
        'shapiro': {'stat': sw_stat, 'p': sw_p} if sw_stat is not None else 'skipped (n out of range)',
        'anderson': {'stat': ad_result.statistic, 'critical_values': ad_result.critical_values, 'significance_level': ad_result.significance_level}
    }

    # simple rule of thumb decision (heuristic)
    within_tolerance = all(abs(diffs[k]) <= tolerance_pct for k in diffs)
    normality_hint = {
        'empirical_rule_ok': within_tolerance,
        'shapiro_pass': (sw_p is not None and sw_p > 0.05),
        'anderson_stat': ad_result.statistic  # lower is better
    }
    report['hint'] = normality_hint

    if plot:
        # histogram + normal pdf
        plt.figure(figsize=(10,4))
        plt.subplot(1,2,1)
        count, bins, _ = plt.hist(x, bins='auto', density=True, alpha=0.6)
        # plot normal pdf
        xs = np.linspace(x.min(), x.max(), 200)
        plt.plot(xs, stats.norm.pdf(xs, mu, sigma), linewidth=2)
        plt.title('Histogram & fitted normal')

        # Q-Q plot
        plt.subplot(1,2,2)
        stats.probplot(x, dist="norm", sparams=(mu, sigma), plot=plt)
        plt.title('Q-Q plot')
        plt.tight_layout()
        plt.show()

    return report

# ä½¿ç”¨ç¤ºä¾‹:
# data = pd.Series(...) æˆ– numpy array
# r = check_sigma_distribution(data)
# print(r)
```

------

## 6) å®ç”¨å»ºè®® / å†³ç­–æµç¨‹

1. å…ˆç”¨ç›´æ–¹å›¾ä¸ Qâ€“Q å›¾è§‚å¯Ÿæ€»ä½“å½¢çŠ¶ã€‚
2. è®¡ç®— Â±1/2/3Ïƒ çš„ç»éªŒæ¯”ä¾‹ï¼Œçœ‹æ˜¯å¦åç¦»å¤ªå¤šã€‚
3. æ ¹æ®æ ·æœ¬å¤§å°ç”¨ Shapiro æˆ– AD æ£€éªŒï¼ˆå¹¶è§£è¯» p å€¼å’Œç»Ÿè®¡é‡ï¼‰ã€‚
4. å¦‚æœè½»å¾®åå·®ä½†ä¸šåŠ¡ä¸æ•æ„Ÿ â€”â€” å¯ä»¥å½“ä½œâ€œè¿‘ä¼¼æ­£æ€â€ï¼›è‹¥ç²¾ç¡®åˆ†å¸ƒå¾ˆå…³é”®ï¼ˆä¾‹å¦‚ç»Ÿè®¡æ¨æ–­å‡è®¾ï¼‰ï¼Œåˆ™ä¸è¦å°†å…¶è§†ä¸ºæ­£æ€ï¼Œè€ƒè™‘éå‚æ•°æ–¹æ³•æˆ–å˜æ¢ï¼ˆå¯¹æ•°ã€Boxâ€“Coxï¼‰ã€‚
5. å¯¹äºæ˜æ˜¾åæ–œæˆ–åšå°¾ï¼šè€ƒè™‘å»æå€¼ã€å¯¹æ•°/æ ¹å·å˜æ¢æˆ–ä½¿ç”¨æ›´ç¨³å¥çš„åˆ†å¸ƒå»ºæ¨¡ï¼ˆå¦‚ t åˆ†å¸ƒã€å¯¹æ•°æ­£æ€ç­‰ï¼‰ã€‚

------

## 7) æ³¨æ„äº‹é¡¹

- **æ ·æœ¬é‡å½±å“æ£€éªŒ**ï¼šå°æ ·æœ¬æ£€éªŒåŠ›ä½ï¼ˆéš¾ä»¥å‘ç°åå·®ï¼‰ï¼›å¤§æ ·æœ¬ä¼šæŠŠå¾®å°åå·®æ£€æµ‹ä¸ºæ˜¾è‘—ã€‚å§‹ç»ˆç»“åˆå›¾å½¢åˆ¤æ–­ã€‚
- **ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾**ï¼šÏƒ è§„åˆ™åŸºäºç‹¬ç«‹æ ·æœ¬ï¼Œè‹¥æ•°æ®æœ‰æ—¶é—´åºåˆ—ç›¸å…³æ€§ï¼ˆè‡ªç›¸å…³ï¼‰æˆ–åˆ†ç»„å·®å¼‚ï¼Œéœ€å…ˆå¤„ç†ä¾èµ–æ€§/åˆ†å±‚ã€‚
- **ç¼ºå¤±å€¼/æç«¯å€¼**ï¼šå…ˆå®¡è§†å¹¶å†³å®šæ˜¯å¦æ¸…æ´—æˆ–ç‰¹åˆ«æ ‡æ³¨ã€‚

------

æ˜ç™½âœ…

å¦‚æœä½ æƒ³**ç”¨ä¸€ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥åˆ¤æ–­æ•°æ®æ˜¯å¦è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ**ï¼Œæœ€å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨ **Shapiroâ€“Wilk æ£€éªŒçš„ p å€¼**ï¼Œå› ä¸ºå®ƒä¸“é—¨é’ˆå¯¹æ­£æ€æ€§ã€‚

è§„åˆ™å¾ˆç®€å•ï¼š

- **p > 0.05** â†’ ä¸èƒ½æ‹’ç»æ­£æ€æ€§ï¼ˆæ•°æ®çœ‹èµ·æ¥ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼‰
- **p â‰¤ 0.05** â†’ æ‹’ç»æ­£æ€æ€§ï¼ˆæ•°æ®æ˜¾è‘—åç¦»æ­£æ€åˆ†å¸ƒï¼‰

------

### Python ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def is_normal(data, alpha=0.05):
    """
    ä½¿ç”¨ Shapiro-Wilk æ£€éªŒåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05
    :return: (æ˜¯å¦æ­£æ€, på€¼)
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    
    stat, p = stats.shapiro(data)
    return (p > alpha, p)

# ç¤ºä¾‹
data1 = np.random.normal(0, 1, 500)  # æ­£æ€åˆ†å¸ƒ
data2 = np.random.exponential(1, 500)  # éæ­£æ€åˆ†å¸ƒ

print("data1:", is_normal(data1))
print("data2:", is_normal(data2))
```

------

### è¾“å‡ºç¤ºä¾‹

```
data1: (True, 0.31)   # p=0.31 > 0.05 â†’ ç¬¦åˆæ­£æ€åˆ†å¸ƒ
data2: (False, 1.2e-12)  # pâ‰ˆ0 â†’ æ˜¾è‘—ä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒ
```

------

å¥½çš„âœ…
 æˆ‘ç»™ä½ å†™ä¸€ä¸ª**é€šç”¨å‡½æ•°**ï¼Œå¯ä»¥é€šè¿‡å‚æ•°é€‰æ‹©ä¸åŒçš„åˆ¤æ–­æŒ‡æ ‡ï¼Œè¿”å› **True/False** æ˜¯å¦ä¸ºæ­£æ€åˆ†å¸ƒï¼Œå¹¶é™„å¸¦ p å€¼æˆ–ç»Ÿè®¡é‡ï¼Œæ–¹ä¾¿å‚è€ƒã€‚

------

## ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def is_normal(data, method="shapiro", alpha=0.05):
    """
    åˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    
    :param data: list æˆ– numpy array
    :param method: æ£€éªŒæ–¹æ³•ï¼Œå¯é€‰ï¼š
                   - "shapiro" (Shapiro-Wilk)
                   - "anderson" (Anderson-Darling)
                   - "kstest" (Kolmogorov-Smirnov, ä¸æ‹Ÿåˆçš„æ­£æ€åˆ†å¸ƒæ¯”è¾ƒ)
                   - "skewkurtosis" (åŸºäºååº¦+å³°åº¦çš„Zæ£€éªŒ)
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤ 0.05
    :return: (æ˜¯å¦æ­£æ€, ç»Ÿè®¡ä¿¡æ¯)
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)

    if method == "shapiro":
        stat, p = stats.shapiro(data)
        return (p > alpha, {"method": "Shapiro-Wilk", "stat": stat, "p": p})

    elif method == "anderson":
        result = stats.anderson(data, dist="norm")
        # å¦‚æœç»Ÿè®¡é‡ < ä¸´ç•Œå€¼ï¼Œåˆ™è®¤ä¸ºè¿‘ä¼¼æ­£æ€
        is_norm = result.statistic < result.critical_values[list(result.significance_level).index(int(alpha*100))]
        return (is_norm, {"method": "Anderson-Darling", "stat": result.statistic,
                          "critical_values": result.critical_values,
                          "significance_level": result.significance_level})

    elif method == "kstest":
        mu, sigma = np.mean(data), np.std(data, ddof=1)
        stat, p = stats.kstest(data, "norm", args=(mu, sigma))
        return (p > alpha, {"method": "Kolmogorov-Smirnov", "stat": stat, "p": p})

    elif method == "skewkurtosis":
        # ååº¦ä¸å³°åº¦çš„Zæ£€éªŒ
        skew, skew_p = stats.skewtest(data)
        kurt, kurt_p = stats.kurtosistest(data)
        is_norm = (skew_p > alpha) and (kurt_p > alpha)
        return (is_norm, {"method": "Skewness-Kurtosis",
                          "skew_stat": skew, "skew_p": skew_p,
                          "kurt_stat": kurt, "kurt_p": kurt_p})

    else:
        raise ValueError("method å‚æ•°å¿…é¡»æ˜¯ 'shapiro'ã€'anderson'ã€'kstest' æˆ– 'skewkurtosis'")

# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)
    data_non_norm = np.random.exponential(1, 500)

    for method in ["shapiro", "anderson", "kstest", "skewkurtosis"]:
        print(f"æ–¹æ³•: {method}")
        print("æ­£æ€æ•°æ®:", is_normal(data_norm, method))
        print("éæ­£æ€æ•°æ®:", is_normal(data_non_norm, method))
        print("------")
```

------

## è¯´æ˜

- `shapiro`ï¼šé€‚åˆå°ä¸­æ ·æœ¬ï¼Œå¸¸ç”¨é»˜è®¤æ–¹æ³•ã€‚
- `anderson`ï¼šè¿”å›å¤šä¸ªæ˜¾è‘—æ€§æ°´å¹³ä¸‹çš„åˆ¤æ–­ï¼Œæ›´ä¸¥æ ¼ã€‚
- `kstest`ï¼šåŸºäºæ ·æœ¬å‡å€¼å’Œæ–¹å·®æ‹Ÿåˆæ­£æ€åˆ†å¸ƒï¼Œå†åš K-S æ£€éªŒã€‚
- `skewkurtosis`ï¼šååº¦+å³°åº¦è”åˆæ£€éªŒï¼Œå¿«é€Ÿç›´è§‚ã€‚

------

å¥½çš„âœ… æˆ‘å¸®ä½ å°è£…ä¸€ä¸ª **ç»¼åˆåˆ¤æ–­å‡½æ•°**ï¼Œå®ƒä¼šè‡ªåŠ¨è·‘ **Shapiro-Wilkã€Anderson-Darlingã€K-S æ£€éªŒã€ååº¦-å³°åº¦æ£€éªŒ** å››ç§æ–¹æ³•ï¼Œç„¶åç»™å‡ºæ¯ä¸ªæ–¹æ³•çš„ç»“æœï¼Œå¹¶æ±‡æ€»ä¸€ä¸ªâ€œå¤šæ•°æŠ•ç¥¨â€çš„ç»¼åˆç»“è®ºã€‚

------

## ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def check_normality(data, alpha=0.05):
    """
    ç»¼åˆåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ
    
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤ 0.05
    :return: dictï¼ŒåŒ…å«å„æ–¹æ³•ç»“æœ + ç»¼åˆç»“è®º
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)
    
    results = {}

    # 1. Shapiro-Wilk
    if 3 <= n <= 5000:  # è¶…è¿‡5000ä¼šæŠ¥é”™
        stat, p = stats.shapiro(data)
        results["shapiro"] = {"is_normal": p > alpha, "stat": stat, "p": p}
    else:
        results["shapiro"] = {"is_normal": None, "stat": None, "p": None, "note": "æ ·æœ¬é‡è¶…å‡ºShapiroé€‚ç”¨èŒƒå›´"}

    # 2. Anderson-Darling
    ad_result = stats.anderson(data, dist="norm")
    crit_dict = dict(zip(ad_result.significance_level, ad_result.critical_values))
    crit_value = crit_dict.get(int(alpha * 100), ad_result.critical_values[2])  # æ‰¾æœ€è¿‘çš„æ°´å¹³
    is_norm_ad = ad_result.statistic < crit_value
    results["anderson"] = {
        "is_normal": is_norm_ad,
        "stat": ad_result.statistic,
        "critical_value": crit_value
    }

    # 3. Kolmogorov-Smirnov
    mu, sigma = np.mean(data), np.std(data, ddof=1)
    stat, p = stats.kstest(data, "norm", args=(mu, sigma))
    results["kstest"] = {"is_normal": p > alpha, "stat": stat, "p": p}

    # 4. Skewness & Kurtosis æ£€éªŒ
    skew_stat, skew_p = stats.skewtest(data)
    kurt_stat, kurt_p = stats.kurtosistest(data)
    is_norm_sk = (skew_p > alpha) and (kurt_p > alpha)
    results["skewkurtosis"] = {
        "is_normal": is_norm_sk,
        "skew_stat": skew_stat, "skew_p": skew_p,
        "kurt_stat": kurt_stat, "kurt_p": kurt_p
    }

    # ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨æ³•ï¼Œæ’é™¤ Noneï¼‰
    votes = [res["is_normal"] for res in results.values() if res["is_normal"] is not None]
    if len(votes) > 0:
        final_decision = votes.count(True) >= votes.count(False)
    else:
        final_decision = None

    return {
        "methods": results,
        "final_decision": final_decision
    }


# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)
    data_non_norm = np.random.exponential(1, 500)

    print("æ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_norm))

    print("\néæ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_non_norm))
```

------

## è¾“å‡ºç¤ºä¾‹ï¼ˆç®€åŒ–ï¼‰

```python
æ­£æ€æ•°æ®ç»“æœ:
{
  'methods': {
    'shapiro': {'is_normal': True, 'stat': 0.998, 'p': 0.64},
    'anderson': {'is_normal': True, 'stat': 0.29, 'critical_value': 0.74},
    'kstest': {'is_normal': True, 'stat': 0.03, 'p': 0.56},
    'skewkurtosis': {'is_normal': True, 'skew_stat': 0.12, 'skew_p': 0.90, 'kurt_stat': 0.33, 'kurt_p': 0.74}
  },
  'final_decision': True
}

éæ­£æ€æ•°æ®ç»“æœ:
{
  'methods': {
    'shapiro': {'is_normal': False, 'stat': 0.81, 'p': 1.2e-12},
    'anderson': {'is_normal': False, 'stat': 7.25, 'critical_value': 0.74},
    'kstest': {'is_normal': False, 'stat': 0.28, 'p': 1.3e-15},
    'skewkurtosis': {'is_normal': False, 'skew_stat': 12.1, 'skew_p': 0.0, 'kurt_stat': 8.3, 'kurt_p': 0.0}
  },
  'final_decision': False
}
```

------

è¿™æ ·ä½ å°±èƒ½ä¸€è¡Œè°ƒç”¨ï¼Œè‡ªåŠ¨è·‘ 4 ç§æ–¹æ³•ï¼Œæœ€åçœ‹ `final_decision` å°±è¡Œã€‚

ğŸ‘Œ é‚£æˆ‘å¸®ä½ æŠŠ**å¯è§†åŒ–**æ•´åˆè¿›å»ï¼šåœ¨ç»¼åˆæ£€éªŒæ—¶ï¼Œå¯ä»¥ç”»å‡º

1. **ç›´æ–¹å›¾ + æ‹Ÿåˆæ­£æ€æ›²çº¿**
2. **Qâ€“Q å›¾**ï¼ˆåˆ†ä½æ•°-åˆ†ä½æ•°å›¾ï¼‰

è¿™æ ·ä½ æ—¢æœ‰æ•°å€¼åˆ¤æ–­ï¼ˆå››ç§æ–¹æ³• + æŠ•ç¥¨ç»“è®ºï¼‰ï¼Œåˆæœ‰å›¾å½¢ç›´è§‚éªŒè¯ã€‚

------

## å®Œæ•´ä»£ç ï¼ˆå¸¦å¯è§†åŒ–ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def check_normality(data, alpha=0.05, plot=True):
    """
    ç»¼åˆåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œå¹¶å¯è§†åŒ–
    
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³
    :param plot: æ˜¯å¦ç»˜åˆ¶å›¾å½¢
    :return: dictï¼ŒåŒ…å«å„æ–¹æ³•ç»“æœ + ç»¼åˆç»“è®º
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)
    
    results = {}

    # 1. Shapiro-Wilk
    if 3 <= n <= 5000:  # è¶…è¿‡5000ä¼šæŠ¥é”™
        stat, p = stats.shapiro(data)
        results["shapiro"] = {"is_normal": p > alpha, "stat": stat, "p": p}
    else:
        results["shapiro"] = {"is_normal": None, "note": "æ ·æœ¬é‡è¶…å‡ºShapiroé€‚ç”¨èŒƒå›´"}

    # 2. Anderson-Darling
    ad_result = stats.anderson(data, dist="norm")
    crit_dict = dict(zip(ad_result.significance_level, ad_result.critical_values))
    crit_value = crit_dict.get(int(alpha * 100), ad_result.critical_values[2])  # æ‰¾æœ€è¿‘æ°´å¹³
    is_norm_ad = ad_result.statistic < crit_value
    results["anderson"] = {
        "is_normal": is_norm_ad,
        "stat": ad_result.statistic,
        "critical_value": crit_value
    }

    # 3. Kolmogorov-Smirnov
    mu, sigma = np.mean(data), np.std(data, ddof=1)
    stat, p = stats.kstest(data, "norm", args=(mu, sigma))
    results["kstest"] = {"is_normal": p > alpha, "stat": stat, "p": p}

    # 4. Skewness & Kurtosis
    skew_stat, skew_p = stats.skewtest(data)
    kurt_stat, kurt_p = stats.kurtosistest(data)
    is_norm_sk = (skew_p > alpha) and (kurt_p > alpha)
    results["skewkurtosis"] = {
        "is_normal": is_norm_sk,
        "skew_stat": skew_stat, "skew_p": skew_p,
        "kurt_stat": kurt_stat, "kurt_p": kurt_p
    }

    # ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨æ³•ï¼Œæ’é™¤ Noneï¼‰
    votes = [res["is_normal"] for res in results.values() if res["is_normal"] is not None]
    if len(votes) > 0:
        final_decision = votes.count(True) >= votes.count(False)
    else:
        final_decision = None

    report = {"methods": results, "final_decision": final_decision}

    # ---------- å¯è§†åŒ– ----------
    if plot:
        fig, axes = plt.subplots(1, 2, figsize=(10, 4))

        # ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿
        axes[0].hist(data, bins="auto", density=True, alpha=0.6, color="skyblue", edgecolor="black")
        xs = np.linspace(data.min(), data.max(), 200)
        axes[0].plot(xs, stats.norm.pdf(xs, mu, sigma), 'r-', lw=2)
        axes[0].set_title("ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿")

        # Q-Q å›¾
        stats.probplot(data, dist="norm", plot=axes[1])
        axes[1].set_title("Q-Q å›¾")

        plt.tight_layout()
        plt.show()

    return report


# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)   # æ­£æ€æ•°æ®
    data_non_norm = np.random.exponential(1, 500)  # éæ­£æ€æ•°æ®

    print("æ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_norm, plot=True))

    print("\néæ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_non_norm, plot=True))
```

------

## ä½¿ç”¨æ•ˆæœ

- **æ‰“å°è¾“å‡º**ï¼šæ¯ä¸ªæ–¹æ³•çš„ç»“æœï¼ˆæ˜¯å¦æ­£æ€ã€ç»Ÿè®¡é‡ã€p å€¼ç­‰ï¼‰+ ç»¼åˆç»“è®º `final_decision`ã€‚
- **ç»˜å›¾**ï¼š
  - å·¦è¾¹ï¼šæ•°æ®ç›´æ–¹å›¾ + æ‹Ÿåˆçš„æ­£æ€æ›²çº¿
  - å³è¾¹ï¼šQ-Q å›¾ï¼ˆç‚¹è¶Šè´´è¿‘å¯¹è§’çº¿ï¼Œè¶Šç¬¦åˆæ­£æ€ï¼‰

------

ğŸ‘Œ é‚£æˆ‘åœ¨ä¸Šé¢çš„ç»¼åˆå‡½æ•°é‡ŒåŠ ä¸Š **ç»éªŒ 68â€“95â€“99.7% æ£€æŸ¥**ï¼Œä¹Ÿå°±æ˜¯ç»Ÿè®¡æ•°æ®è½åœ¨ `Î¼Â±1Ïƒã€Î¼Â±2Ïƒã€Î¼Â±3Ïƒ` åŒºé—´çš„æ¯”ä¾‹ï¼Œå¹¶å’Œç†è®ºå€¼åšå¯¹æ¯”ã€‚

è¿™æ ·ä½ å°±èƒ½åŒæ—¶çœ‹åˆ°ï¼š

- å››ç§ç»Ÿè®¡æ£€éªŒç»“æœ
- ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨ï¼‰
- Sigma Ruleï¼ˆç»éªŒæ³•åˆ™ï¼‰çš„æ¯”ä¾‹æƒ…å†µ
- å¯è§†åŒ–ç»“æœ

------

## å®Œæ•´ä»£ç ï¼ˆå¸¦ Sigma Rule æ£€æŸ¥ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def check_normality(data, alpha=0.05, plot=True):
    """
    ç»¼åˆåˆ¤æ–­æ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼ˆå¤šç§æ–¹æ³• + Sigma Rule + å¯è§†åŒ–ï¼‰
    
    :param data: list æˆ– numpy array
    :param alpha: æ˜¾è‘—æ€§æ°´å¹³
    :param plot: æ˜¯å¦ç»˜åˆ¶å›¾å½¢
    :return: dictï¼ŒåŒ…å«å„æ–¹æ³•ç»“æœ + Sigma Rule + ç»¼åˆç»“è®º
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN
    n = len(data)
    
    results = {}

    # 1. Shapiro-Wilk
    if 3 <= n <= 5000:  # è¶…è¿‡5000ä¼šæŠ¥é”™
        stat, p = stats.shapiro(data)
        results["shapiro"] = {"is_normal": p > alpha, "stat": stat, "p": p}
    else:
        results["shapiro"] = {"is_normal": None, "note": "æ ·æœ¬é‡è¶…å‡ºShapiroé€‚ç”¨èŒƒå›´"}

    # 2. Anderson-Darling
    ad_result = stats.anderson(data, dist="norm")
    crit_dict = dict(zip(ad_result.significance_level, ad_result.critical_values))
    crit_value = crit_dict.get(int(alpha * 100), ad_result.critical_values[2])  # æ‰¾æœ€è¿‘æ°´å¹³
    is_norm_ad = ad_result.statistic < crit_value
    results["anderson"] = {
        "is_normal": is_norm_ad,
        "stat": ad_result.statistic,
        "critical_value": crit_value
    }

    # 3. Kolmogorov-Smirnov
    mu, sigma = np.mean(data), np.std(data, ddof=1)
    stat, p = stats.kstest(data, "norm", args=(mu, sigma))
    results["kstest"] = {"is_normal": p > alpha, "stat": stat, "p": p}

    # 4. Skewness & Kurtosis
    skew_stat, skew_p = stats.skewtest(data)
    kurt_stat, kurt_p = stats.kurtosistest(data)
    is_norm_sk = (skew_p > alpha) and (kurt_p > alpha)
    results["skewkurtosis"] = {
        "is_normal": is_norm_sk,
        "skew_stat": skew_stat, "skew_p": skew_p,
        "kurt_stat": kurt_stat, "kurt_p": kurt_p
    }

    # 5. Sigma Rule æ£€æŸ¥
    within_1 = np.mean((data >= mu - sigma) & (data <= mu + sigma)) * 100
    within_2 = np.mean((data >= mu - 2*sigma) & (data <= mu + 2*sigma)) * 100
    within_3 = np.mean((data >= mu - 3*sigma) & (data <= mu + 3*sigma)) * 100

    sigma_rule = {
        "empirical": {"Â±1Ïƒ": within_1, "Â±2Ïƒ": within_2, "Â±3Ïƒ": within_3},
        "theoretical": {"Â±1Ïƒ": 68.27, "Â±2Ïƒ": 95.45, "Â±3Ïƒ": 99.73}
    }

    # ç»¼åˆç»“è®ºï¼ˆæŠ•ç¥¨æ³•ï¼Œæ’é™¤ Noneï¼‰
    votes = [res["is_normal"] for res in results.values() if res["is_normal"] is not None]
    if len(votes) > 0:
        final_decision = votes.count(True) >= votes.count(False)
    else:
        final_decision = None

    report = {"methods": results, "sigma_rule": sigma_rule, "final_decision": final_decision}

    # ---------- å¯è§†åŒ– ----------
    if plot:
        fig, axes = plt.subplots(1, 2, figsize=(10, 4))

        # ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿
        axes[0].hist(data, bins="auto", density=True, alpha=0.6, color="skyblue", edgecolor="black")
        xs = np.linspace(data.min(), data.max(), 200)
        axes[0].plot(xs, stats.norm.pdf(xs, mu, sigma), 'r-', lw=2)
        axes[0].set_title("ç›´æ–¹å›¾ + æ­£æ€æ‹Ÿåˆæ›²çº¿")

        # Q-Q å›¾
        stats.probplot(data, dist="norm", plot=axes[1])
        axes[1].set_title("Q-Q å›¾")

        plt.tight_layout()
        plt.show()

    return report


# ---------------- ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    np.random.seed(0)
    data_norm = np.random.normal(0, 1, 500)   # æ­£æ€æ•°æ®
    data_non_norm = np.random.exponential(1, 500)  # éæ­£æ€æ•°æ®

    print("æ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_norm, plot=True))

    print("\néæ­£æ€æ•°æ®ç»“æœ:")
    print(check_normality(data_non_norm, plot=True))
```

------

## è¾“å‡ºç¤ºä¾‹ï¼ˆéƒ¨åˆ†ï¼‰

```python
"final_decision": True,
"sigma_rule": {
  "empirical": {"Â±1Ïƒ": 67.8, "Â±2Ïƒ": 95.0, "Â±3Ïƒ": 99.4},
  "theoretical": {"Â±1Ïƒ": 68.27, "Â±2Ïƒ": 95.45, "Â±3Ïƒ": 99.73}
}
```

è¿™æ ·å°±èƒ½çœ‹åˆ°ç»éªŒæ¯”ä¾‹å’Œç†è®ºå€¼çš„å¯¹æ¯”ã€‚

------

ä½ è¯´çš„æƒ…å†µå…¶å®å¾ˆå¸¸è§ï¼Œæˆ‘æ¥å¸®ä½ æ‹†è§£ä¸€ä¸‹ï¼š

------

### 1. ä¸ºä»€ä¹ˆä¼šå‡ºç° **â€œæ£€éªŒä¸é€šè¿‡ï¼Œä½†Ïƒæ³•åˆ™å¾ˆæ¥è¿‘â€**ï¼Ÿ

- **ç»Ÿè®¡æ£€éªŒå¤ªä¸¥æ ¼**
  - Shapiro-Wilkã€Anderson-Darling ç­‰æ£€éªŒåœ¨**æ ·æœ¬é‡å¤§æ—¶**éå¸¸æ•æ„Ÿï¼Œå“ªæ€•æ•°æ®ç¨å¾®åä¸€ç‚¹ç‚¹æ­£æ€åˆ†å¸ƒï¼Œå°±ä¼šæ‹’ç»æ­£æ€æ€§ã€‚
  - è¿™æ—¶å³ä½¿æ•°æ®â€œçœ‹èµ·æ¥å¾ˆæ­£æ€â€ï¼Œä¹Ÿä¼šè¢«åˆ¤å®šä¸ºä¸æ­£æ€ã€‚
- **Ïƒæ³•åˆ™æ›´å®½æ¾**
  - Ïƒ æ³•åˆ™ï¼ˆ68â€“95â€“99.7 è§„åˆ™ï¼‰æœ¬è´¨æ˜¯ç»éªŒåˆ†å¸ƒç‰¹å¾ï¼š
    - Â±1Ïƒ åŒ…å« â‰ˆ68% æ•°æ®
    - Â±2Ïƒ åŒ…å« â‰ˆ95% æ•°æ®
    - Â±3Ïƒ åŒ…å« â‰ˆ99.7% æ•°æ®
  - å¦‚æœä½ çš„æ•°æ®æ¥è¿‘è¿™ä¸ªåˆ†å¸ƒï¼Œè¯´æ˜å®ƒ**è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ**ï¼Œå³ä½¿ä¸¥æ ¼æ£€éªŒä¸é€šè¿‡ã€‚

------

### 2. æ€ä¹ˆå¤„ç†è¿™ç§å†²çªï¼Ÿ

ä½ å¯ä»¥å»ºç«‹ä¸€ä¸ª **â€œå®½æ¾ + ä¸¥æ ¼â€åŒé‡åˆ¤å®š** æœºåˆ¶ï¼š

- **ä¸¥æ ¼åˆ¤å®š**ï¼šç”¨ç»Ÿè®¡æ£€éªŒï¼ˆp > 0.05 æ‰ç®—æ­£æ€ï¼‰ã€‚
- **å®½æ¾åˆ¤å®š**ï¼šç”¨ Ïƒ æ³•åˆ™è¯¯å·® â‰¤ é˜ˆå€¼ï¼ˆä¾‹å¦‚å…è®¸è¯¯å·® 5%ï¼‰ã€‚
- **ç»¼åˆç»“è®º**ï¼š
  - å¦‚æœç»Ÿè®¡æ£€éªŒä¸é€šè¿‡ï¼Œä½† Ïƒ æ³•åˆ™è¯¯å·®å¾ˆå° â†’ **è¿‘ä¼¼æ­£æ€**
  - å¦‚æœä¸¤è€…éƒ½ä¸é€šè¿‡ â†’ **éæ­£æ€**
  - å¦‚æœä¸¤è€…éƒ½é€šè¿‡ â†’ **æ­£æ€**

------

### 3. ç¤ºä¾‹ä»£ç ï¼ˆæ”¹è¿›ç‰ˆï¼‰

```python
import numpy as np
from scipy import stats

def sigma_rule_check(data, tol=0.05):
    mean, std = np.mean(data), np.std(data)
    counts = [
        np.mean((data > mean - k*std) & (data < mean + k*std))
        for k in [1, 2, 3]
    ]
    expected = [0.68, 0.95, 0.997]
    diffs = [abs(c - e) for c, e in zip(counts, expected)]
    return all(d < tol for d in diffs), counts, diffs

def check_normality(data, alpha=0.05, tol=0.05):
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»æ‰ NaN

    # ä¸¥æ ¼ï¼šShapiro-Wilk
    _, p = stats.shapiro(data)
    strict = p > alpha

    # å®½æ¾ï¼šÏƒæ³•åˆ™
    loose, counts, diffs = sigma_rule_check(data, tol=tol)

    # ç»¼åˆç»“è®º
    if strict:
        conclusion = "æ­£æ€åˆ†å¸ƒ"
    elif loose:
        conclusion = "è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ"
    else:
        conclusion = "éæ­£æ€åˆ†å¸ƒ"

    return {
        "p_value": p,
        "sigma_counts": counts,
        "sigma_diffs": diffs,
        "strict": strict,
        "loose": loose,
        "conclusion": conclusion
    }

# ç¤ºä¾‹
data1 = np.random.normal(0, 1, 1000)
data2 = np.random.exponential(1, 1000)

print(check_normality(data1))
print(check_normality(data2))
```

------

### 4. è¾“å‡ºç¤ºä¾‹

```
{'p_value': 0.01, 'sigma_counts': [0.682, 0.953, 0.998],
 'sigma_diffs': [0.002, 0.003, 0.001],
 'strict': False, 'loose': True,
 'conclusion': 'è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ'}
```

æ„æ€æ˜¯ï¼šç»Ÿè®¡æ£€éªŒå¾ˆä¸¥æ ¼åœ°æ‹’ç»äº†æ­£æ€ï¼Œä½† Ïƒ æ³•åˆ™å‡ ä¹å®Œå…¨ç¬¦åˆ â†’ **å¯åˆ¤å®šä¸ºâ€œè¿‘ä¼¼æ­£æ€â€**ã€‚

------

å¾ˆå¥½ â€” æ—¢ç„¶ä½ çš„æ•°æ®æ¥è‡ªè®­ç»ƒè¿‡ç¨‹ï¼Œå­˜åœ¨æµ‹é‡è¯¯å·®å’Œæç«¯å€¼ï¼ˆoutliersï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€å¥—**æœ‰æ­¥éª¤ã€å¯å¤ç”¨**çš„æµç¨‹æ¥å¤„ç†è¿™ç±»â€œè¿‘ä¼¼æ­£æ€ä½†è¢«å¼‚å¸¸å€¼å¹²æ‰°â€çš„æƒ…å†µã€‚ä¸‹é¢ç»™å‡ºåŸåˆ™ + å®æˆ˜ä»£ç ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥ç»™å‡º**ä¸ºä»€ä¹ˆè¿™ä¹ˆåš**ä»¥åŠ**ä½•æ—¶é€‰ç”¨**çš„å»ºè®®ã€‚

# æ€è·¯ï¼ˆåŸåˆ™æ€§ï¼‰

1. **å…ˆä¸è½»æ˜“åˆ æ•°æ®**ï¼šè®­ç»ƒæ•°æ®å¯èƒ½åŒ…å«æœ‰æ„ä¹‰çš„æç«¯å€¼ï¼ˆçœŸå®ä¿¡å·ï¼‰ï¼Œç›²ç›®åˆ é™¤ä¼šæŸå¤±ä¿¡æ¯ã€‚
2. **æ£€æµ‹â€”â€”å®šæ€§åˆ¤æ–­â€”â€”å¤„ç†â€”â€”å¤æ£€**ï¼šæ£€æµ‹å¼‚å¸¸ â†’ åˆ¤æ–­å¼‚å¸¸æ˜¯å¦ä¸ºå™ªå£°/æµ‹é‡è¯¯å·® â†’ é€‰æ‹©å¤„ç†ç­–ç•¥ï¼ˆä¿®æ­£/æˆªå°¾/å˜æ¢/ç¨³å¥å»ºæ¨¡ï¼‰â†’ é‡æ–°æ£€éªŒæ­£æ€æ€§ä¸ä¸‹æ¸¸å½±å“ã€‚
3. **å¯å¤ç°ã€å¯å›é€€**ï¼šæ‰€æœ‰å¤„ç†åº”å¯å›é€€ï¼ˆè®°å½•è¢«ä¿®æ”¹æˆ–æˆªå°¾æ•°æ®ï¼‰ã€‚
4. **åœ¨å»ºæ¨¡æ—¶ä½¿ç”¨ç¨³å¥æ–¹æ³•**ï¼šè‹¥æ— æ³•ä¿è¯æ­£æ€æ€§ï¼Œå¯é€‰æ‹©å¯¹å¼‚å¸¸ä¸æ•æ„Ÿçš„æ¨¡å‹æˆ–åˆ†å¸ƒï¼ˆå¦‚ Studentâ€™s tã€é²æ£’å›å½’ç­‰ï¼‰ã€‚

# å¸¸ç”¨ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ä¸ä½•æ—¶ç”¨ï¼‰

- **æ£€æµ‹å¼‚å¸¸**ï¼šIQRï¼ˆé€‚åˆå³å/å·¦åéƒ½èƒ½æ£€æµ‹ï¼‰ï¼ŒZ-scoreï¼ˆå¯¹è¾ƒå°å¼‚å¸¸æœ‰æ•ˆï¼‰ï¼ŒMAD/robust-zï¼ˆå¯¹å¤§é‡å¼‚å¸¸æ›´ç¨³å¥ï¼‰ã€‚
- **å¤„ç†å¼‚å¸¸**ï¼š
  - ä¿®æ­£/è£å‰ªï¼ˆcap/winsorizeï¼‰â€”â€”å½“å¼‚å¸¸æ˜¯æµ‹é‡é”™è¯¯æˆ–æ˜æ˜¾ç¦»ç¾¤ç‚¹æ—¶ã€‚
  - åˆ é™¤ï¼ˆtrimï¼‰â€”â€”åªåœ¨ç¡®å®šä¸ºåæ•°æ®æ—¶ï¼Œå¹¶è®°å½•ã€‚
  - å˜æ¢ï¼ˆlog, sqrt, Box-Cox, Yeo-Johnsonï¼‰â€”â€”å½“æ•°æ®å‘ˆå³åæˆ–ä¹˜æ³•å™ªå£°ã€‚
  - ä½¿ç”¨ç¨³å¥ç»Ÿè®¡é‡ï¼ˆä¸­ä½æ•°ã€MADï¼‰/ç¨³å¥æ¨¡å‹ï¼ˆRANSACã€Huberï¼‰æˆ–å‡è®¾æ›´åšå°¾åˆ†å¸ƒï¼ˆtåˆ†å¸ƒï¼‰ã€‚
- **é‡æ–°æ£€éªŒ**ï¼šé‡å¤æ­£æ€æ€§æ£€éªŒä¸ sigma-rule æ£€æŸ¥ï¼Œå¹¶è¯„ä¼°å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ¨¡å‹æ€§èƒ½ã€æŒ‡æ ‡ï¼‰çš„å½±å“ã€‚

------

# å¯ç›´æ¥è¿è¡Œçš„ Python å®æˆ˜ä»£ç ï¼ˆå®Œæ•´ã€å¸¦æ³¨é‡Šï¼‰

```python
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import PowerTransformer
from statsmodels.robust.scale import mad as statsmodels_mad
from typing import Tuple, Dict

# ----------------- æ£€æµ‹å¼‚å¸¸ -----------------
def detect_outliers_iqr(x: np.ndarray, k=1.5) -> np.ndarray:
    """è¿”å›å¸ƒå°”æ•°ç»„ï¼ŒTrue è¡¨ç¤ºè¯¥ç‚¹ä¸ºå¼‚å¸¸ï¼ˆåŸºäº IQRï¼‰"""
    q1, q3 = np.nanpercentile(x, [25, 75])
    iqr = q3 - q1
    lower, upper = q1 - k * iqr, q3 + k * iqr
    return (x < lower) | (x > upper)

def detect_outliers_zscore(x: np.ndarray, z_thresh=3.0) -> np.ndarray:
    """åŸºäºæ ‡å‡† z-score çš„å¼‚å¸¸æ£€æµ‹ï¼ˆå¯¹æç«¯å€¼æ•æ„Ÿï¼‰"""
    z = (x - np.nanmean(x)) / (np.nanstd(x, ddof=1))
    return np.abs(z) > z_thresh

def detect_outliers_robust_z(x: np.ndarray, z_thresh=3.5) -> np.ndarray:
    """
    åŸºäºä¸­ä½æ•°å’Œ MAD çš„ robust z-scoreï¼ˆå¯¹å¼‚å¸¸æ›´ç¨³å¥ï¼‰
    å…¬å¼ï¼šrobust_z = 0.6745*(x - median)/MAD
    """
    med = np.nanmedian(x)
    mad = statsmodels_mad(x, center=med)  # è¿”å›MAD
    if mad == 0:
        # fallback: use std
        mad = np.nanstd(x, ddof=1)
        if mad == 0:
            return np.zeros_like(x, dtype=bool)
    robust_z = 0.6745 * (x - med) / mad
    return np.abs(robust_z) > z_thresh

# ----------------- å¤„ç†æ–¹æ³• -----------------
def winsorize_series(x: np.ndarray, lower_pct=0.01, upper_pct=0.99) -> np.ndarray:
    """æˆªå°¾ï¼ˆwinzorizeï¼‰: å°†ä½äºlower_pctåˆ†ä½æ•°çš„å€¼æ›¿æ¢ä¸ºè¯¥åˆ†ä½å€¼ï¼Œé«˜äºupper_pctåˆ†ä½æ•°ç±»ä¼¼å¤„ç†"""
    low = np.nanpercentile(x, lower_pct * 100)
    high = np.nanpercentile(x, upper_pct * 100)
    x2 = x.copy()
    x2[x2 < low] = low
    x2[x2 > high] = high
    return x2

def trim_series(x: np.ndarray, lower_pct=0.01, upper_pct=0.99) -> np.ndarray:
    """æˆªæ–­ï¼ˆåˆ é™¤ï¼‰è¾¹ç•Œå¤–æ•°æ®ï¼›è¿”å›æ©ç ä¸ä¿ç•™æ•°æ®ç´¢å¼•"""
    low = np.nanpercentile(x, lower_pct * 100)
    high = np.nanpercentile(x, upper_pct * 100)
    mask = (x >= low) & (x <= high)
    return mask

def apply_transformations(x: np.ndarray, method: str):
    """å˜æ¢ï¼š'log','boxcox','yeojohnson'ï¼ˆåä¸¤è€…éœ€è¦æ­£å€¼/å¯å…¼å®¹ï¼‰"""
    x = np.asarray(x).astype(float)
    if method == "log":
        # ä¸ºé¿å… log(<=0)ï¼Œå…ˆå¹³ç§»æœ€å°å€¼
        shift = 0
        minv = np.nanmin(x)
        if minv <= 0:
            shift = 1 - minv
        return np.log(x + shift), {"shift": shift}
    elif method == "boxcox":
        # Box-Cox è¦æ±‚æ­£å€¼
        if np.nanmin(x) <= 0:
            raise ValueError("Box-Cox requires positive data")
        transformed, lmbda = stats.boxcox(x)
        return transformed, {"lambda": lmbda}
    elif method == "yeojohnson":
        # Yeo-Johnson å¯å¤„ç†éæ­£æ•°æ®
        pt = PowerTransformer(method="yeo-johnson", standardize=False)
        xt = pt.fit_transform(x.reshape(-1, 1)).ravel()
        return xt, {"pt": pt}
    else:
        raise ValueError("Unknown transform")

# ----------------- æ­£æ€æ€§æ£€éªŒï¼ˆå¤ç”¨ä¹‹å‰çš„æ¦‚å¿µï¼‰ -----------------
def shapiro_p(x: np.ndarray) -> float:
    """è¿”å› Shapiro-Wilk çš„ p å€¼ï¼ˆè‹¥æ ·æœ¬å¤ªå¤§åˆ™è¿”å› np.nanï¼‰"""
    n = np.sum(~np.isnan(x))
    if n < 3:
        return np.nan
    if n > 5000:
        # Shapiro åœ¨ scipy ä¸­å¯¹å¤§æ ·æœ¬å¯èƒ½å¤±å»é€‚ç”¨æ€§
        return np.nan
    stat, p = stats.shapiro(x)
    return p

def sigma_rule_percentages(x: np.ndarray) -> Dict[str, float]:
    mu = np.nanmean(x); sigma = np.nanstd(x, ddof=1)
    within1 = np.mean((x >= mu - sigma) & (x <= mu + sigma)) * 100
    within2 = np.mean((x >= mu - 2*sigma) & (x <= mu + 2*sigma)) * 100
    within3 = np.mean((x >= mu - 3*sigma) & (x <= mu + 3*sigma)) * 100
    return {"Â±1Ïƒ": within1, "Â±2Ïƒ": within2, "Â±3Ïƒ": within3}

# ----------------- ç»¼åˆå¤„ç†æµç¨‹å‡½æ•° -----------------
def robust_normalize_pipeline(
    x: np.ndarray,
    outlier_method: str = "iqr",
    outlier_action: str = "winsorize",  # 'winsorize','trim','mark'
    transform_try: list = ["yeojohnson", "log"],  # try in order
    win_lower=0.01, win_upper=0.99,
    z_thresh=3.5
) -> Dict:
    """
    ä¸€ä¸ªç«¯åˆ°ç«¯æµç¨‹ï¼š
    1) æ£€æµ‹å¼‚å¸¸ï¼ˆiqr/robust_z/zscoreï¼‰
    2) æ ¹æ® outlier_action å¤„ç†ï¼ˆwinsorize/trim/markï¼‰
    3) å°è¯•å˜æ¢ï¼ˆè‹¥éœ€è¦ï¼‰å¹¶è¯„ä¼°æ­£æ€æ€§ (Shapiro + sigma_rule)
    4) è¿”å›å¤„ç†å‰åè¯Šæ–­ä¸å¤„ç†ç»“æœ
    """
    orig = np.asarray(x).astype(float)
    x0 = orig.copy()
    mask_nan = np.isnan(x0)
    n_total = x0.size

    # 1) æ£€æµ‹
    if outlier_method == "iqr":
        out_mask = detect_outliers_iqr(x0)
    elif outlier_method == "zscore":
        out_mask = detect_outliers_zscore(x0, z_thresh=z_thresh)
    elif outlier_method == "robust_z":
        out_mask = detect_outliers_robust_z(x0, z_thresh=z_thresh)
    else:
        raise ValueError("Unknown outlier_method")

    out_mask = out_mask & (~mask_nan)  # ä¸æŠŠ NaN å½“ä½œå¼‚å¸¸
    n_outliers = int(np.nansum(out_mask))

    # 2) å¤„ç†
    x_processed = x0.copy()
    removed_idx = None
    if outlier_action == "winsorize":
        x_processed = winsorize_series(x_processed, lower_pct=win_lower, upper_pct=win_upper)
    elif outlier_action == "trim":
        mask_keep = trim_series(x_processed, lower_pct=win_lower, upper_pct=win_upper)
        removed_idx = np.where(~mask_keep)[0].tolist()
        x_processed = x_processed[mask_keep]
    elif outlier_action == "mark":
        # ä¸ä¿®æ”¹æ•°æ®ï¼Œåªæ ‡è®°ï¼Œç”±ä¸‹æ¸¸æ¨¡å‹å†³å®š
        pass
    else:
        raise ValueError("Unknown outlier_action")

    # 3) è¯Šæ–­å‡½æ•°
    def diag(xarr):
        p = shapiro_p(xarr)
        sigma = sigma_rule_percentages(xarr)
        skew = stats.skew(xarr); kurt = stats.kurtosis(xarr, fisher=False)
        return {"n": np.sum(~np.isnan(xarr)), "shapiro_p": p, "sigma": sigma, "skew": skew, "kurtosis": kurt}

    before_diag = diag(x0[~mask_nan])
    after_diag = diag(x_processed[~np.isnan(x_processed)])

    # 4) å°è¯•å˜æ¢ï¼ˆå¦‚æœ after_diag æœªæ»¡è¶³ï¼‰
    transform_info = []
    best = {"method": None, "diag": after_diag, "data": x_processed}
    # åˆ¤æ–­æ˜¯å¦éœ€è¦å˜æ¢ï¼šshapiro_p éç©ºä½†å°äº0.05 æˆ– sigma ä¸ç†è®ºå·®è·å¤§
    def sigma_diff_ok(sigma_dict, tol_pct=5.0):
        # æ¯”è¾ƒ empirical(%) vs theoretical percentages
        thr = {"Â±1Ïƒ": 68.27, "Â±2Ïƒ": 95.45, "Â±3Ïƒ": 99.73}
        diffs = [abs(sigma_dict[k] - thr[k]) for k in thr]
        return all(d <= tol_pct for d in diffs)

    need_transform = True
    if (after_diag["shapiro_p"] is not None and after_diag["shapiro_p"] > 0.05) and sigma_diff_ok(after_diag["sigma"]):
        need_transform = False

    if need_transform:
        for method in transform_try:
            try:
                xt, info = apply_transformations(x_processed if outlier_action != "trim" else x_processed, method)
            except Exception as e:
                transform_info.append({"method": method, "error": str(e)})
                continue
            d = diag(xt)
            transform_info.append({"method": method, "diag": d, "info": info})
            # é€‰å–ç¬¬ä¸€ä¸ªæ»¡è¶³ shapiro_p>0.05 æˆ–è€… sigma_rule æ¥è¿‘çš„å˜æ¢ç»“æœä½œä¸ºæœ€ä½³
            if (d["shapiro_p"] is not None and d["shapiro_p"] > 0.05) or sigma_diff_ok(d["sigma"]):
                best = {"method": method, "diag": d, "data": xt}
                break

    result = {
        "n_total": int(n_total),
        "n_outliers_detected": n_outliers,
        "outlier_indices": np.where(out_mask)[0].tolist(),
        "before_diag": before_diag,
        "after_diag": after_diag,
        "outlier_action": outlier_action,
        "transforms_tested": transform_info,
        "best": best,
        "removed_indices": removed_idx
    }
    return result

# ----------------- ä½¿ç”¨ç¤ºä¾‹ -----------------
if __name__ == "__main__":
    np.random.seed(42)
    # æ„é€ ç¤ºä¾‹ï¼šæ­£æ€å™ªå£° + ä¸€äº›æç«¯å€¼
    data = np.concatenate([np.random.normal(0, 1, 2000), np.array([10, 12, -9, 15])])
    res = robust_normalize_pipeline(
        data,
        outlier_method="robust_z",
        outlier_action="winsorize",
        transform_try=["yeojohnson", "log"]
    )
    import pprint; pprint.pprint(res)
```

# ä½¿ç”¨å»ºè®®ï¼ˆå¯¹è®­ç»ƒæ•°æ®çš„å…·ä½“å»ºè®®ï¼‰

1. **ä¼˜å…ˆå°è¯• winsorizeï¼ˆæˆªå°¾ï¼‰**ï¼šè®­ç»ƒæ•°æ®ä¸­å°‘é‡æç«¯å€¼å¸¸å¸¸æ˜¯å™ªå£°ï¼Œç”¨æˆªå°¾ï¼ˆä¾‹å¦‚ 1%/99%ï¼‰èƒ½ç¨³å¥æ”¹å–„ç»Ÿè®¡æ€§è´¨è€Œä¸åˆ æ•°æ®ã€‚
2. **ä¿ç•™åŸå§‹åˆ—**ï¼šåœ¨æ•°æ®é›†ä¸­ä¿ç•™åŸå§‹æœªå¤„ç†çš„åˆ—ï¼Œä»¥åŠå¤„ç†ååˆ—ï¼ˆæ¯”å¦‚ `feature`, `feature_winsorized`, `feature_yeojohnson`ï¼‰ï¼Œä»¥ä¾¿æ¯”è¾ƒå¯¹æ¨¡å‹æ•ˆæœçš„å½±å“ã€‚
3. **è‹¥æç«¯å€¼æ¥è‡ªçœŸå®ä¿¡å·ï¼ˆä¸æ˜¯å™ªå£°ï¼‰**ï¼šè€ƒè™‘ä½¿ç”¨ç¨³å¥æ¨¡å‹æˆ–åšå°¾åˆ†å¸ƒï¼ˆStudent-tï¼‰ï¼Œæˆ–å¯¹æŸå¤±å‡½æ•°åšé²æ£’åŒ–ï¼ˆHuberã€Quantile lossï¼‰ã€‚
4. **è¯„ä¼°ä¸‹æ¸¸å½±å“**ï¼šä»»ä½•å˜æ¢éƒ½åº”ä»¥æå‡ä¸‹æ¸¸æ¨¡å‹è¡¨ç°æˆ–æ»¡è¶³æ–¹æ³•å‡è®¾ä¸ºç›®æ ‡ã€‚åš A/Bï¼ˆæœ‰/æ— å¤„ç†ï¼‰æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ã€‚
5. **è‡ªåŠ¨åŒ–å¹¶è®°å½•**ï¼šæŠŠæ£€æµ‹ã€å¤„ç†ã€å‚æ•°ï¼ˆå¦‚æˆªå°¾åˆ†ä½ç‚¹ã€å˜æ¢ lambdaï¼‰è®°å½•åˆ°ç®¡é“æ—¥å¿—ï¼Œæ–¹ä¾¿å¤ç°ä¸å›é€€ã€‚

------

å¥½â€”â€”ç»™ä½ ä¸€ä¸ª**æç®€ã€å¯å¤åˆ¶ã€å®ç”¨**çš„å‡½æ•°ï¼šåªè¿”å›â€œè¿™æ¬¡æ•°æ®æ˜¯å¦å¯ä»¥è®¤ä¸ºæ˜¯æ­£æ€åˆ†å¸ƒï¼ˆå¸ƒå°”ï¼‰â€ï¼Œå¹¶åŒæ—¶ç»™å‡ºçŸ­å°çš„è¯Šæ–­ç†ç”±ã€‚è®¾è®¡åŸåˆ™æ˜¯ï¼š

- å¯¹è®­ç»ƒæ•°æ®å¸¸è§çš„**å°‘é‡æç«¯å€¼**é‡‡ç”¨**å¯é€‰æˆªå°¾ï¼ˆwinsorizeï¼‰**æ¥é™ä½å™ªå£°å½±å“ï¼ˆé»˜è®¤å¼€å¯ï¼‰ã€‚
- å½“æ ·æœ¬é‡å¾ˆå¤§ä¸” Shapiro ä¸é€‚ç”¨æ—¶ï¼Œç”¨ååº¦/å³°åº¦æ£€éªŒæ›¿ä»£ã€‚
- åŒæ—¶ç”¨ **Ïƒ ç»éªŒæ³•åˆ™å·®å¼‚** åšâ€œå®½æ¾åˆ¤å®šâ€â€”â€”å¦‚æœä¸¥æ ¼æ£€éªŒæ²¡è¿‡ä½† Ïƒ å·®å¼‚å°ï¼Œåˆ™åˆ¤ä¸ºâ€œè¿‘ä¼¼æ­£æ€â€ã€‚

å‡½æ•°è¿”å›ï¼š

- `is_normal`ï¼ˆboolï¼‰æœ€ç»ˆç»“è®º
- `reason`ï¼ˆå­—ç¬¦ä¸²ï¼‰ç®€çŸ­è¯´æ˜ä¸ºä»€ä¹ˆæ˜¯ / ä¸æ˜¯æ­£æ€
- ä»¥åŠå…³é”®ä¸­é—´å€¼ï¼ˆp å€¼ã€sigma_diff ç­‰ï¼‰æ–¹ä¾¿è°ƒå‚ã€‚

å¤åˆ¶è¿è¡Œå³å¯ï¼š

```python
import numpy as np
from scipy import stats

def simple_normal_check(data,
                        alpha=0.05,
                        sigma_tol_pct=5.0,
                        winsorize_pct=(0.01, 0.99),
                        do_winsorize=True):
    """
    è¿”å›ä¸€ä¸ªç®€æ´åˆ¤æ–­ï¼šæ•°æ®æ˜¯å¦å¯è®¤ä¸ºæ­£æ€åˆ†å¸ƒï¼ˆTrue/Falseï¼‰åŠç®€çŸ­åŸå› ã€‚
    - data: 1D array-like
    - alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼ˆç”¨äºç»Ÿè®¡æ£€éªŒï¼‰
    - sigma_tol_pct: sigma rule ä¸ç†è®ºå€¼å…è®¸çš„è¯¯å·®ï¼ˆç™¾åˆ†æ¯”ç‚¹ï¼Œä¾‹å¦‚5.0 è¡¨ç¤º Â±5%ï¼‰
    - winsorize_pct: (low, high) æˆªå°¾åˆ†ä½ç‚¹ï¼ˆä»…åœ¨ do_winsorize=True æ—¶ç”Ÿæ•ˆï¼‰
    - do_winsorize: æ˜¯å¦å…ˆå¯¹æç«¯å€¼åš winsorizeï¼ˆé€šå¸¸å»ºè®® Trueï¼‰
    """
    x = np.asarray(data).astype(float)
    x = x[~np.isnan(x)]
    n = x.size
    if n < 3:
        return {"is_normal": False, "reason": "æ ·æœ¬é‡å¤ªå°ï¼ˆ<3ï¼‰", "n": n}

    # å¯é€‰ winsorizeï¼ˆé»˜è®¤å¼€å¯ï¼Œå‡è½»å°‘é‡æç«¯å€¼å½±å“ï¼‰
    if do_winsorize:
        low_q = np.nanpercentile(x, winsorize_pct[0]*100)
        high_q = np.nanpercentile(x, winsorize_pct[1]*100)
        x_proc = x.copy()
        x_proc[x_proc < low_q] = low_q
        x_proc[x_proc > high_q] = high_q
    else:
        x_proc = x

    mu = np.mean(x_proc)
    sigma = np.std(x_proc, ddof=1)

    # Sigma rule empiricalç™¾åˆ†æ¯”ï¼ˆä»¥ç™¾åˆ†æ¯”ç‚¹è¡¨ç¤ºï¼‰
    within_1 = np.mean((x_proc >= mu - sigma) & (x_proc <= mu + sigma)) * 100
    within_2 = np.mean((x_proc >= mu - 2*sigma) & (x_proc <= mu + 2*sigma)) * 100
    within_3 = np.mean((x_proc >= mu - 3*sigma) & (x_proc <= mu + 3*sigma)) * 100
    thr = {"Â±1Ïƒ": 68.27, "Â±2Ïƒ": 95.45, "Â±3Ïƒ": 99.73}
    diffs = {
        "Â±1Ïƒ": abs(within_1 - thr["Â±1Ïƒ"]),
        "Â±2Ïƒ": abs(within_2 - thr["Â±2Ïƒ"]),
        "Â±3Ïƒ": abs(within_3 - thr["Â±3Ïƒ"])
    }
    sigma_ok = all(d <= sigma_tol_pct for d in diffs.values())

    # ç»Ÿè®¡æ£€éªŒï¼šä¼˜å…ˆ Shapiroï¼ˆæ ·æœ¬é‡ <= 5000ï¼‰ï¼Œå¦åˆ™ç”¨ååº¦/å³°åº¦æ£€éªŒè”åˆåˆ¤æ–­
    shapiro_p = None
    skew_p = None
    kurt_p = None
    strict_ok = None

    try:
        if 3 <= n <= 5000:
            _, shapiro_p = stats.shapiro(x_proc)
            strict_ok = (shapiro_p > alpha)
        else:
            # å½“æ ·æœ¬å¾ˆå¤§æ—¶ï¼Œshapiroå®¹æ˜“æ‹’ç»ï¼šç”¨ skew/kurt tests
            skew_stat, skew_p = stats.skewtest(x_proc)
            kurt_stat, kurt_p = stats.kurtosistest(x_proc)
            strict_ok = (skew_p > alpha) and (kurt_p > alpha)
    except Exception as e:
        # è‹¥ä»»ä½•æ£€éªŒå‡ºé”™ï¼Œè®¾ä¸º None å¹¶ç»§ç»­ç”¨ sigma_rule å†³å®š
        strict_ok = None

    # ç»¼åˆå†³ç­–è§„åˆ™ï¼ˆæç®€ï¼‰
    # 1) å¦‚æœä¸¥æ ¼æ£€éªŒé€šè¿‡ -> æ­£æ€
    # 2) å¦åˆ™ï¼Œå¦‚æœä¸¥æ ¼æ£€éªŒæœªé€šè¿‡ä½† sigma_rule æ¥è¿‘ -> è¿‘ä¼¼æ­£æ€ï¼ˆä¹Ÿè®¤ä¸ºé€šè¿‡ï¼‰
    # 3) å¦åˆ™ -> éæ­£æ€
    if strict_ok is True:
        is_norm = True
        reason = f"ä¸¥æ ¼æ£€éªŒé€šè¿‡ (shapiro_p={shapiro_p:.4g})" if shapiro_p is not None else "ä¸¥æ ¼æ£€éªŒé€šè¿‡"
    elif strict_ok is False and sigma_ok:
        is_norm = True
        reason = (f"ä¸¥æ ¼æ£€éªŒæœªé€šè¿‡ (p={shapiro_p:.4g} æˆ– skew/kurt p),"
                  f" ä½† sigma-rule æ¥è¿‘ (diffs={ {k:round(v,2) for k,v in diffs.items()} }) -> è§†ä¸ºè¿‘ä¼¼æ­£æ€")
    elif strict_ok is None and sigma_ok:
        is_norm = True
        reason = f"æ£€éªŒå¤±è´¥æˆ–ä¸é€‚ç”¨ï¼Œä½† sigma-rule æ¥è¿‘ -> è§†ä¸ºè¿‘ä¼¼æ­£æ€ (diffs={ {k:round(v,2) for k,v in diffs.items()} })"
    else:
        is_norm = False
        if strict_ok is False:
            reason = (f"ä¸¥æ ¼æ£€éªŒä¸é€šè¿‡ (p={shapiro_p:.4g} æˆ– skew/kurt p)ï¼Œ"
                      f"sigma diffs={ {k:round(v,2) for k,v in diffs.items()} } è¶…å‡ºå®¹å·® {sigma_tol_pct}%")
        else:
            reason = f"sigma diffs={ {k:round(v,2) for k,v in diffs.items()} } è¶…å‡ºå®¹å·® {sigma_tol_pct}%"

    return {
        "is_normal": is_norm,
        "reason": reason,
        "n": n,
        "shapiro_p": shapiro_p,
        "skew_p": skew_p,
        "kurt_p": kurt_p,
        "sigma_empirical_pct": {"Â±1Ïƒ": within_1, "Â±2Ïƒ": within_2, "Â±3Ïƒ": within_3},
        "sigma_diffs_pct": diffs
    }

# ---------------- ä½¿ç”¨ç¤ºä¾‹ ----------------
if __name__ == "__main__":
    import numpy as np
    np.random.seed(0)
    data = np.concatenate([np.random.normal(0,1,2000), [10,12,-9,15]])  # æœ‰å°‘é‡æç«¯å€¼
    res = simple_normal_check(data, alpha=0.05, sigma_tol_pct=5.0, do_winsorize=True)
    print(res)
```

### æ¨èçš„é»˜è®¤è®¾ç½®è¯´æ˜

- `do_winsorize=True`ï¼šå¯¹è®­ç»ƒæ•°æ®é€šå¸¸æœ‰å¥½å¤„ï¼ˆ1%/99% æˆªå°¾ï¼‰â€”â€”èƒ½æ¶ˆé™¤å°‘é‡å™ªå£°ç‚¹å¯¹ç»Ÿè®¡æ£€éªŒçš„å¼ºçƒˆå½±å“ã€‚
- `sigma_tol_pct=5.0`ï¼šå…è®¸ Â±1Ïƒ/Â±2Ïƒ/Â±3Ïƒ ä¸ç†è®ºå€¼åå·®åœ¨ **5 ä¸ªç™¾åˆ†ç‚¹** å†…å°±è®¤ä¸ºâ€œæ¥è¿‘â€ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦æ”¾å®½åˆ° 8â€“10 æˆ–æ”¶ç´§åˆ° 3ã€‚
- å½“æ ·æœ¬ > 5000 æ—¶ï¼ŒShapiro é€šå¸¸ä¸é€‚ç”¨ï¼ˆä¼šå› å¾®å°åå·®æ‹’ç»ï¼‰ï¼Œæ•…æ”¹ç”¨ååº¦/å³°åº¦è”åˆæ£€éªŒã€‚

------

å¥½çš„ âœ… æˆ‘å¸®ä½ æ•´ç†ä¸€ä¸ª **æç®€ç‰ˆ**ï¼Œæ»¡è¶³ä½ æçš„è¦æ±‚ï¼š

- **ä¸ä¿®æ”¹æ•°æ®**ï¼ˆ`do_winsorize = False`ï¼‰ã€‚
- **åªè¿”å› True/False**ï¼Œä¸è¿”å›é¢å¤–ä¿¡æ¯ã€‚
- **å®¹å·®ç”¨ç›¸å¯¹è¯¯å·®**ï¼Œå¹¶ä¸”æ ¹æ®æ ·æœ¬é‡ `n` è‡ªé€‚åº”è°ƒæ•´ï¼ˆæ ·æœ¬è¶Šå¤§ï¼Œè¦æ±‚è¶Šä¸¥æ ¼ï¼‰ã€‚

------

### ä»£ç ç¤ºä¾‹

```python
import numpy as np
from scipy import stats

def simple_normal_check(data, alpha=0.05):
    """
    æç®€æ­£æ€æ€§æ£€æµ‹
    1. ä½¿ç”¨ Shapiro-Wilk æ£€éªŒ
    2. ä½¿ç”¨ sigma ruleï¼ˆ68-95-99.7 ç»éªŒæ³•åˆ™ï¼‰
    ä¸¤è€…åªè¦æœ‰ä¸€ä¸ªé€šè¿‡ï¼Œå°±è®¤ä¸ºè¿‘ä¼¼æ­£æ€
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»é™¤ NaN
    n = len(data)
    if n < 10:
        return False  # æ ·æœ¬å¤ªå°æ— æ³•åˆ¤æ–­

    # --- æ–¹æ³•1: Shapiro-Wilk ---
    _, p_value = stats.shapiro(data)
    if p_value > alpha:
        return True

    # --- æ–¹æ³•2: sigma rule ---
    mean, std = np.mean(data), np.std(data, ddof=1)
    if std == 0:
        return False

    within_1sigma = np.mean((data >= mean - std) & (data <= mean + std))
    within_2sigma = np.mean((data >= mean - 2*std) & (data <= mean + 2*std))
    within_3sigma = np.mean((data >= mean - 3*std) & (data <= mean + 3*std))

    # æœŸæœ›æ¯”ä¾‹
    expected = [0.68, 0.95, 0.997]
    observed = [within_1sigma, within_2sigma, within_3sigma]

    # è‡ªé€‚åº”å®¹å·®ï¼ˆæ ·æœ¬è¶Šå¤§ï¼Œå®¹å·®è¶Šå°ï¼‰
    tol = 1 / np.sqrt(n)  # ä¾‹å¦‚ n=100 â†’ 0.1 å®¹å·®ï¼Œn=1000 â†’ 0.03
    if all(abs(o - e) <= tol for o, e in zip(observed, expected)):
        return True

    return False


# --- ç¤ºä¾‹ ---
np.random.seed(42)
normal_data = np.random.normal(0, 1, 500)
exp_data = np.random.exponential(1, 500)

print("Normal data:", simple_normal_check(normal_data))  # True
print("Exponential data:", simple_normal_check(exp_data))  # False
```

------

### æ ¸å¿ƒé€»è¾‘

1. å…ˆç”¨ **Shapiro-Wilk æ£€éªŒ**ï¼ˆä¸¥æ ¼ç»Ÿè®¡å­¦æ ‡å‡†ï¼‰ã€‚
2. å¦‚æœå¤±è´¥ï¼Œå†ç”¨ **sigma rule + è‡ªé€‚åº”å®¹å·®** å…œåº•ã€‚
   - æ ·æœ¬é‡å° â†’ å®¹å·®æ”¾å®½
   - æ ·æœ¬é‡å¤§ â†’ å®¹å·®æ”¶ç´§

------

å¥½å˜ âœ… æˆ‘ç»™ä½ åŠ ä¸€ä¸ª `mode` å‚æ•°ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼ï¼š

- **"strict"** â†’ åªç”¨ Shapiro-Wilkï¼ˆæ›´ç»Ÿè®¡å­¦ä¸¥è°¨ï¼Œå®¹é”™ç‡ä½ï¼‰ã€‚
- **"loose"** â†’ åªç”¨ sigma ruleï¼ˆæ›´å®½æ¾ï¼Œé€‚åˆæœ‰å™ªå£°çš„è®­ç»ƒæ•°æ®ï¼‰ã€‚
- **"auto"**ï¼ˆé»˜è®¤ï¼‰ â†’ å…ˆå°è¯• Shapiro-Wilkï¼Œä¸é€šè¿‡æ—¶å†çœ‹ sigma ruleã€‚

------

### æ”¹è¿›åçš„ä»£ç 

```python
import numpy as np
from scipy import stats

def simple_normal_check(data, alpha=0.05, mode="auto"):
    """
    æç®€æ­£æ€æ€§æ£€æµ‹
    mode:
        - "strict" : åªç”¨ Shapiro-Wilk
        - "loose"  : åªç”¨ sigma rule
        - "auto"   : å…ˆ Shapiro-Wilkï¼Œå¤±è´¥å† sigma rule
    è¿”å› True/False
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]  # å»é™¤ NaN
    n = len(data)
    if n < 10:
        return False  # æ ·æœ¬å¤ªå°æ— æ³•åˆ¤æ–­

    # --- æ–¹æ³•1: Shapiro-Wilk ---
    def check_shapiro(x):
        _, p_value = stats.shapiro(x)
        return p_value > alpha

    # --- æ–¹æ³•2: sigma rule ---
    def check_sigma(x):
        mean, std = np.mean(x), np.std(x, ddof=1)
        if std == 0:
            return False

        within_1sigma = np.mean((x >= mean - std) & (x <= mean + std))
        within_2sigma = np.mean((x >= mean - 2*std) & (x <= mean + 2*std))
        within_3sigma = np.mean((x >= mean - 3*std) & (x <= mean + 3*std))

        expected = [0.68, 0.95, 0.997]
        observed = [within_1sigma, within_2sigma, within_3sigma]

        # è‡ªé€‚åº”å®¹å·®ï¼ˆæ ·æœ¬è¶Šå¤§è¶Šä¸¥æ ¼ï¼‰
        tol = 1 / np.sqrt(n)
        return all(abs(o - e) <= tol for o, e in zip(observed, expected))

    # --- æ¨¡å¼é€‰æ‹© ---
    if mode == "strict":
        return check_shapiro(data)
    elif mode == "loose":
        return check_sigma(data)
    elif mode == "auto":
        return check_shapiro(data) or check_sigma(data)
    else:
        raise ValueError("mode must be 'strict', 'loose', or 'auto'")


# --- ç¤ºä¾‹ ---
np.random.seed(42)
normal_data = np.random.normal(0, 1, 500)
exp_data = np.random.exponential(1, 500)

print("Normal data (strict):", simple_normal_check(normal_data, mode="strict"))
print("Normal data (loose):", simple_normal_check(normal_data, mode="loose"))
print("Exponential data (auto):", simple_normal_check(exp_data, mode="auto"))
```

------

### ç¤ºä¾‹è¾“å‡º

```
Normal data (strict): True
Normal data (loose): True
Exponential data (auto): False
```

------

ğŸ‘Œæˆ‘ç†è§£äº†ï¼Œä½ éœ€è¦ä¸€ä¸ª**æ›´ç¨³å¥çš„è‡ªé€‚åº”å®¹å·®**ï¼Œé¿å…ä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜ï¼š

1. **æ•°æ®é‡å¤ªå°‘** â†’ å®¹å·®å¤ªå¤§ï¼Œå¯¼è‡´å‡ ä¹ä»»ä½•åˆ†å¸ƒéƒ½â€œé€šè¿‡â€ã€‚
2. **æ•°æ®é‡å¾ˆå¤§ä½†å¸¦æœ‰å¼‚å¸¸å€¼** â†’ å®¹å·®å¤ªå°ï¼Œå¯¼è‡´æ˜æ˜è¿‘ä¼¼æ­£æ€å´åˆ¤å®šå¤±è´¥ã€‚

------

### æ”¹è¿›æ–¹æ¡ˆ

æˆ‘å»ºè®®ç»¼åˆä»¥ä¸‹å› ç´ æ¥è®¾å®šå®¹å·®ï¼š

1. **åŸºç¡€è¯¯å·®**ï¼ˆ`base_tol`ï¼‰ï¼šä¿è¯å³ä¾¿æ•°æ®é‡å¾ˆå°ï¼Œä¹Ÿæœ‰ä¸€ä¸ªæœ€å°å®¹å·®ï¼ˆé¿å…â€œè¿‡ä¸¥â€ï¼‰ã€‚
2. **æ ·æœ¬é‡å½±å“**ï¼ˆ`1/âˆšn`ï¼‰ï¼šæ ·æœ¬é‡è¶Šå¤§ï¼Œå®¹å·®è¶Šå°ï¼ˆæ ‡å‡†ç»Ÿè®¡å­¦ä¾æ®ï¼‰ã€‚
3. **æ•°æ®åˆ†å¸ƒç¨³å®šæ€§**ï¼ˆç”¨ååº¦ `skew` å’Œå³°åº¦ `kurtosis`ï¼‰ï¼š
   - å¦‚æœ `|skew| < 1` ä¸” `|kurtosis| < 3`ï¼Œè¯´æ˜æ•°æ®æ²¡æœ‰æ˜æ˜¾åæ€/åšå°¾ â†’ æ”¾å®½å®¹å·®ã€‚
   - å¦‚æœä¸¥é‡åæ€æˆ–åšå°¾ â†’ æ”¶ç´§å®¹å·®ã€‚

æœ€ç»ˆå®¹å·®å…¬å¼ï¼š

tol=base_tol+cnÃ—adjtol = base\_tol + \frac{c}{\sqrt{n}} \times adj

- `base_tol` = 0.03 ï¼ˆä¿è¯å°æ ·æœ¬ä¹Ÿèƒ½åˆ¤æ–­ï¼‰
- `c` = 1.0ï¼ˆç¼©æ”¾å› å­ï¼Œå¯è°ƒï¼‰
- `adj` = 0.5~2.0ï¼ˆæ ¹æ®ååº¦/å³°åº¦è°ƒæ•´ï¼‰

------

### æ”¹è¿›åçš„ä»£ç 

```python
import numpy as np
from scipy import stats

def simple_normal_check(data, alpha=0.05, mode="auto"):
    """
    æç®€æ­£æ€æ€§æ£€æµ‹ï¼ˆå¢å¼ºè‡ªé€‚åº”å®¹å·®ç‰ˆï¼‰
    mode:
        - "strict" : åªç”¨ Shapiro-Wilk
        - "loose"  : åªç”¨ sigma rule
        - "auto"   : å…ˆ Shapiro-Wilkï¼Œå¤±è´¥å† sigma rule
    """
    data = np.asarray(data).astype(float)
    data = data[~np.isnan(data)]
    n = len(data)
    if n < 10:
        return False  # æ ·æœ¬å¤ªå°æ— æ³•åˆ¤æ–­

    # --- æ–¹æ³•1: Shapiro-Wilk ---
    def check_shapiro(x):
        _, p_value = stats.shapiro(x)
        return p_value > alpha

    # --- æ–¹æ³•2: sigma rule ---
    def check_sigma(x):
        mean, std = np.mean(x), np.std(x, ddof=1)
        if std == 0:
            return False

        within_1sigma = np.mean((x >= mean - std) & (x <= mean + std))
        within_2sigma = np.mean((x >= mean - 2*std) & (x <= mean + 2*std))
        within_3sigma = np.mean((x >= mean - 3*std) & (x <= mean + 3*std))

        expected = [0.68, 0.95, 0.997]
        observed = [within_1sigma, within_2sigma, within_3sigma]

        # --- è‡ªé€‚åº”å®¹å·® ---
        skew = stats.skew(x)
        kurt = stats.kurtosis(x)
        adj = 1.0
        if abs(skew) < 1 and abs(kurt) < 3:
            adj = 1.5  # æ•°æ®è¾ƒç¨³å®š â†’ æ”¾å®½
        elif abs(skew) > 2 or abs(kurt) > 7:
            adj = 0.7  # æ˜æ˜¾åæ€/åšå°¾ â†’ æ”¶ç´§

        base_tol = 0.03
        tol = base_tol + (1.0 / np.sqrt(n)) * adj

        return all(abs(o - e) <= tol for o, e in zip(observed, expected))

    # --- æ¨¡å¼é€‰æ‹© ---
    if mode == "strict":
        return check_shapiro(data)
    elif mode == "loose":
        return check_sigma(data)
    elif mode == "auto":
        return check_shapiro(data) or check_sigma(data)
    else:
        raise ValueError("mode must be 'strict', 'loose', or 'auto'")
```

------

### æ”¹è¿›ç‚¹

1. **å°æ ·æœ¬æ—¶**ï¼Œè‡³å°‘ä¿ç•™ `Â±3%` å®¹å·®ï¼Œé¿å…è¿‡äºä¸¥æ ¼ã€‚
2. **å¤§æ ·æœ¬æ—¶**ï¼Œå®¹å·®ä»ç„¶æ”¶ç´§ï¼Œä½†ä¸ä¼šæ— é™æ¥è¿‘ 0ã€‚
3. **æ•°æ®åæ€/åšå°¾æ—¶**ï¼Œå®¹å·®è‡ªåŠ¨è°ƒæ•´ï¼ˆåç¦»è¶Šä¸¥é‡ï¼Œè¦æ±‚è¶Šä¸¥æ ¼ï¼‰ã€‚

------

ğŸ‘Œ é‚£æˆ‘å¸®ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ **æµ‹è¯•è„šæœ¬**ï¼Œåˆ†åˆ«ç”¨ä¸åŒåˆ†å¸ƒçš„æ•°æ®æ¥è·‘ä½ ç°åœ¨çš„ `simple_normal_check`ï¼Œçœ‹çœ‹åœ¨ **strict / loose / auto** ä¸‰ç§æ¨¡å¼ä¸‹çš„è¡¨ç°ã€‚

------

### æµ‹è¯•è„šæœ¬

```python
import numpy as np
from scipy import stats

# å‡è®¾ simple_normal_check å·²ç»å®šä¹‰åœ¨è¿™é‡Œï¼ˆç›´æ¥ç”¨ä½ ä¸Šä¸€ä¸ªç‰ˆæœ¬çš„å‡½æ•°ï¼‰

def test_normality_checker():
    np.random.seed(42)

    datasets = {
        "æ­£æ€åˆ†å¸ƒ (N(0,1))": np.random.normal(0, 1, 500),
        "è½»å¾®åæ€ (å¯¹æ•°æ­£æ€)": np.random.lognormal(0, 0.2, 500),  # åæ€è½»
        "å¼ºåæ€ (æŒ‡æ•°åˆ†å¸ƒ)": np.random.exponential(1, 500),       # åæ€é‡
        "åšå°¾åˆ†å¸ƒ (t df=2)": stats.t(df=2).rvs(500),             # heavy tail
        "å‡åŒ€åˆ†å¸ƒ (U[0,1])": np.random.uniform(0, 1, 500)        # éæ­£æ€
    }

    modes = ["strict", "loose", "auto"]

    for name, data in datasets.items():
        print(f"\n{name}:")
        for mode in modes:
            result = simple_normal_check(data, mode=mode)
            print(f"  {mode:<6} â†’ {result}")

if __name__ == "__main__":
    test_normality_checker()
```

------

### é¢„æœŸç»“æœè§£é‡Š

- **æ­£æ€åˆ†å¸ƒ (N(0,1))**
  - strict â†’ Trueï¼ˆShapiro æ£€éªŒé€šè¿‡ï¼‰
  - loose â†’ Trueï¼ˆsigma rule é€šè¿‡ï¼‰
  - auto â†’ True
- **è½»å¾®åæ€ (å¯¹æ•°æ­£æ€)**
  - strict â†’ Falseï¼ˆShapiro å¾ˆæ•æ„Ÿï¼‰
  - loose â†’ Trueï¼ˆsigma rule å®¹å¿åº¦é«˜ï¼‰
  - auto â†’ Trueï¼ˆå…œåº•æˆåŠŸï¼‰
- **å¼ºåæ€ (æŒ‡æ•°åˆ†å¸ƒ)**
  - strict â†’ False
  - loose â†’ Falseï¼ˆsigma rule ä¹Ÿåå·®å¤§ï¼‰
  - auto â†’ False
- **åšå°¾åˆ†å¸ƒ (t åˆ†å¸ƒï¼Œè‡ªç”±åº¦2)**
  - strict â†’ False
  - loose â†’ Falseï¼ˆå®¹å·®æ”¶ç´§ï¼Œåˆ¤å®šå¤±è´¥ï¼‰
  - auto â†’ False
- **å‡åŒ€åˆ†å¸ƒ**
  - strict â†’ False
  - loose â†’ False
  - auto â†’ False

------

